interactions:
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 17:45:55 GMT
      apigw-requestid:
      - fDX1AhbgIAMESrg=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: '{"country":"US","internal":"false","corrAppNum":"16123456","id":"16123456","type":"application","list":[{"appNum":"2017187096","appDate":1506470400000,"pubList":[{"pubCountry":"JP","pubNum":"2019061579","pubDate":1555545600000,"kindCode":"A","pubDateStr":"04/18/2019"},{"pubCountry":"JP","pubNum":"6768620","pubDate":1602633600000,"kindCode":"B2","pubDateStr":"10/14/2020"}],"countryCode":"JP","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/27/2017","docListMsg":null,"docNum":{"country":"JP","docNumber":"2017187096","format":null,"date":"09/27/2017","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"16123456","appDate":1536192000000,"pubList":[{"pubCountry":"US","pubNum":"20190095759","pubDate":1553731200000,"kindCode":"A1","pubDateStr":"03/28/2019"},{"pubCountry":"US","pubNum":"10902286","pubDate":1611619200000,"kindCode":"B2","pubDateStr":"01/26/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/06/2018","docListMsg":null,"docNum":{"country":"US","docNumber":"201816123456","format":null,"date":"09/06/2018","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"17130468","appDate":1608595200000,"pubList":[{"pubCountry":"US","pubNum":"20210110206","pubDate":1618444800000,"kindCode":"A1","pubDateStr":"04/15/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"US","docNumber":"16123456","kindCode":"A"},{"country":"US","docNumber":"17130468","kindCode":"A"},{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"12/22/2020","docListMsg":null,"docNum":{"country":"US","docNumber":"202017130468","format":null,"date":"12/22/2020","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '1885'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:45:55 GMT
      apigw-requestid:
      - fDX1AizWIAMES8g=
    status:
      code: 200
      message: OK
- request:
    body: '{"qf": "appEarlyPubNumber applId appLocation appType appStatus_txt appConfrNumber
      appCustNumber appGrpArtNumber appCls appSubCls appEntityStatus_txt patentNumber
      patentTitle primaryInventor firstNamedApplicant appExamName appExamPrefrdName
      appAttrDockNumber appPCTNumber appIntlPubNumber wipoEarlyPubNumber pctAppType
      firstInventorFile appClsSubCls rankAndInventorsList", "fl": "*", "fq": [], "searchText":
      "applId:(16123456)", "sort": null, "facet": "false", "start": 0}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '471'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ped.uspto.gov/api/queries
  response:
    body:
      string: '{"queryResults":{"indexLastUpdatedDate":"Fri Jan 20 04:37:18 EST 2023","searchResponse":{"responseHeader":{"zkConnected":true,"status":0,"QTime":404},"response":{"numFound":1,"start":0,"numFoundExact":true,"docs":[{"corrAddrCountryName":"UNITED
        STATES","applId":"16123456","totalPtoDays":"127","assignments":[{"reelNumber":"46816","frameNumber":"108","addressNameText":"BIRCH,
        STEWART, KOLASCH & BIRCH, LLP","addressLineOneText":"8110 GATEHOUSE ROAD,
        SUITE 100E","addressLineTwoText":"FALLS CHURCH, VA 22042","addressLineThreeText":"null","addressLineFourText":"null","mailDate":"09-10-2018","receivedDate":"09-07-2018","recordedDate":"09-07-2018","pagesCount":"3","converyanceName":"ASSIGNMENT
        OF ASSIGNORS INTEREST (SEE DOCUMENT FOR DETAILS).","sequenceNumber":"1","assignors":[{"assignorName":"KANADA,
        SHOJI","execDate":"07-11-2018"}],"assignee":[{"assigneeName":"FUJIFILM CORPORATION","streetLineOneText":"26-30,
        NISHIAZABU 2-CHOME, MINATO-KU","streetLineTwoText":"null","cityName":"TOKYO","countryCode":"JAPAN","postalCode":"106-8620"}]}],"applicants":[{"nameLineOne":"FUJIFILM
        Corporation","nameLineTwo":" ","suffix":"","streetOne":"","streetTwo":"","city":"Tokyo,
        ","geoCode":"","country":"(JP)","rankNo":"1"}],"applicantsFacet":"{||FUJIFILM
        Corporation||||Tokyo||JP|ORG|1}","appFilingDate":"2018-09-06T00:00:00Z","appExamName":"SABOURI,
        MAZDA","appExamNameFacet":"SABOURI, MAZDA","publicInd":"Y","APP_IND":"5","inventorName":"Shoji  KANADA","inventorNameFacet":"Shoji  KANADA","appEarlyPubDate":"2019-03-28T00:00:00Z","corrAddrGeoRegionCode":"VA","appLocation":"ELECTRONIC","appEarlyPubNumber":"US20190095759A1","id":"16123456","appGrpArtNumber":"2641","appGrpArtNumberFacet":"2641","applIdStr":"16123456","appl_id_txt":"16123456","appSubCls":"159000","patentNumber":"10902286","LAST_MOD_TS":"2023-01-19T05:52:43Z","transactions":[{"recordDate":"2018-09-06
        00:00:00","code":"IDSC","description":"Information Disclosure Statement considered"},{"recordDate":"2021-07-09
        00:00:00","code":"IDSC","description":"Information Disclosure Statement considered"},{"recordDate":"2021-01-26
        00:00:00","code":"PGM/","description":"Recordation of Patent Grant Mailed"},{"recordDate":"2021-01-07
        00:00:00","code":"EML_NTR","description":"Email Notification"},{"recordDate":"2021-01-06
        00:00:00","code":"WPIR","description":"Issue Notification Mailed"},{"recordDate":"2021-01-26
        00:00:00","code":"PTAC","description":"Patent Issue Date Used in PTA Calculation"},{"recordDate":"2020-12-26
        00:00:00","code":"D1935","description":"Dispatch to FDC"},{"recordDate":"2020-12-23
        00:00:00","code":"PILS","description":"Application Is Considered Ready for
        Issue"},{"recordDate":"2020-12-22 00:00:00","code":"N084","description":"Issue
        Fee Payment Verified"},{"recordDate":"2020-12-22 00:00:00","code":"IFEE","description":"Issue
        Fee Payment Received"},{"recordDate":"2020-10-20 00:00:00","code":"EML_NTR","description":"Email
        Notification"},{"recordDate":"2020-10-19 00:00:00","code":"EML_NTR","description":"Email
        Notification"},{"recordDate":"2020-10-20 00:00:00","code":"TCPB","description":"Printer
        Rush- No mailing"},{"recordDate":"2020-10-20 00:00:00","code":"MCNOA","description":"Mailing
        Corrected Notice of Allowability"},{"recordDate":"2020-10-15 00:00:00","code":"CNOA","description":"Corrected
        Notice of Allowability"},{"recordDate":"2020-10-19 00:00:00","code":"TCPB","description":"Printer
        Rush- No mailing"},{"recordDate":"2020-10-19 00:00:00","code":"MM327","description":"Mail
        Miscellaneous Communication to Applicant"},{"recordDate":"2020-10-13 00:00:00","code":"M327","description":"Miscellaneous
        Communication to Applicant - No Action Count"},{"recordDate":"2020-10-07 00:00:00","code":"PUBTC","description":"Pubs
        Case Remand to TC"},{"recordDate":"2020-10-05 00:00:00","code":"M844","description":"Information
        Disclosure Statement (IDS) Filed"},{"recordDate":"2020-10-05 00:00:00","code":"WIDS","description":"Information
        Disclosure Statement (IDS) Filed"},{"recordDate":"2020-09-23 00:00:00","code":"ELC_RVW","description":"Electronic
        Review"},{"recordDate":"2020-09-23 00:00:00","code":"EML_NTF","description":"Email
        Notification"},{"recordDate":"2020-09-23 00:00:00","code":"MN/=.","description":"Mail
        Notice of Allowance"},{"recordDate":"2020-09-20 00:00:00","code":"N/=.","description":"Notice
        of Allowance Data Verification Completed"},{"recordDate":"2020-09-18 00:00:00","code":"EX.A","description":"Examiner''s
        Amendment Communication"},{"recordDate":"2020-07-02 00:00:00","code":"FWDX","description":"Date
        Forwarded to Examiner"},{"recordDate":"2020-06-26 00:00:00","code":"A...","description":"Response
        after Non-Final Action"},{"recordDate":"2020-06-09 00:00:00","code":"EML_NTR","description":"Email
        Notification"},{"recordDate":"2020-06-09 00:00:00","code":"MEXIA","description":"Mail
        Applicant Initiated Interview Summary"},{"recordDate":"2020-06-03 00:00:00","code":"EXAT","description":"Interview
        Summary - Applicant Initiated - Telephonic"},{"recordDate":"2020-06-03 00:00:00","code":"EXIA","description":"Interview
        Summary- Applicant Initiated"},{"recordDate":"2020-05-22 00:00:00","code":"DOCK","description":"Case
        Docketed to Examiner in GAU"},{"recordDate":"2020-05-21 00:00:00","code":"DOCK","description":"Case
        Docketed to Examiner in GAU"},{"recordDate":"2020-03-27 00:00:00","code":"ELC_RVW","description":"Electronic
        Review"},{"recordDate":"2020-03-27 00:00:00","code":"EML_NTF","description":"Email
        Notification"},{"recordDate":"2020-03-27 00:00:00","code":"MCTNF","description":"Mail
        Non-Final Rejection"},{"recordDate":"2020-03-23 00:00:00","code":"CTNF","description":"Non-Final
        Rejection"},{"recordDate":"2020-01-08 00:00:00","code":"DOCK","description":"Case
        Docketed to Examiner in GAU"},{"recordDate":"2019-12-09 00:00:00","code":"DOCK","description":"Case
        Docketed to Examiner in GAU"},{"recordDate":"2019-03-29 00:00:00","code":"EML_NTR","description":"Email
        Notification"},{"recordDate":"2019-03-28 00:00:00","code":"CCRDY","description":"Application
        ready for PDX access by participating foreign offices"},{"recordDate":"2019-03-28
        00:00:00","code":"PG-ISSUE","description":"PG-Pub Issue Notification"},{"recordDate":"2018-10-23
        00:00:00","code":"PD.RECVD","description":"Priority document has successfully
        retrieved via PDX/DAS"},{"recordDate":"2018-09-06 00:00:00","code":"M844","description":"Information
        Disclosure Statement (IDS) Filed"},{"recordDate":"2018-10-19 00:00:00","code":"DOCK","description":"Case
        Docketed to Examiner in GAU"},{"recordDate":"2018-10-18 00:00:00","code":"OIPE","description":"Application
        Dispatched from OIPE"},{"recordDate":"2018-10-12 00:00:00","code":"EML_NTR","description":"Email
        Notification"},{"recordDate":"2018-10-11 00:00:00","code":"PGPC","description":"Sent
        to Classification Contractor"},{"recordDate":"2018-10-11 00:00:00","code":"FTFS","description":"FITF
        set to YES - revise initial setting"},{"recordDate":"2018-09-06 00:00:00","code":"PTA.RFE","description":"Patent
        Term Adjustment - Ready for Examination"},{"recordDate":"2018-10-12 00:00:00","code":"COMP","description":"Application
        Is Now Complete"},{"recordDate":"2018-10-12 00:00:00","code":"FLRCPT.O","description":"Filing
        Receipt"},{"recordDate":"2018-09-06 00:00:00","code":"APPERMS","description":"Applicants
        have given acceptable permission for participating foreign "},{"recordDate":"2018-09-06
        00:00:00","code":"PDREQUST","description":"Request from applicant for the
        USPTO to retrieve the Priority Document"},{"recordDate":"2018-09-26 00:00:00","code":"L128","description":"Cleared
        by L&R (LARS)"},{"recordDate":"2018-09-14 00:00:00","code":"L198","description":"Referred
        to Level 2 (LARS) by OIPE CSR"},{"recordDate":"2018-09-06 00:00:00","code":"WIDS","description":"Information
        Disclosure Statement (IDS) Filed"},{"recordDate":"2018-09-06 00:00:00","code":"SCAN","description":"IFW
        Scan & PACR Auto Security Review"},{"recordDate":"2018-09-06 00:00:00","code":"BIG.","description":"Entity
        Status Set To Undiscounted (Initial Default Setting or Status Change)"},{"recordDate":"2018-09-06
        00:00:00","code":"IEXX","description":"Initial Exam Team nn"}],"LAST_INSERT_TIME":"2023-01-19T14:10:02Z","appCls":"382","appStatus":"Patented
        Case","appStatusFacet":"Patented Case","appStatus_txt":"Patented Case","corrAddrStreetLineTwo":"Suite
        100 East","ptaPteInd":"PTA","ptaPteTranHistory":[{"number":"74.5","ptaOrPteDate":"01-26-2021","contentsDescription":"PTA
        36 Months","ptoDays":"","applDays":"","start":"0.5"},{"number":"74","ptaOrPteDate":"01-26-2021","contentsDescription":"Patent
        Issue Date Used in PTA Calculation","ptoDays":"","applDays":"","start":"0"},{"number":"72","ptaOrPteDate":"12-26-2020","contentsDescription":"Dispatch
        to FDC","ptoDays":"","applDays":"","start":"0"},{"number":"71","ptaOrPteDate":"12-23-2020","contentsDescription":"Application
        Is Considered Ready for Issue","ptoDays":"","applDays":"","start":"0"},{"number":"70","ptaOrPteDate":"12-22-2020","contentsDescription":"Issue
        Fee Payment Verified","ptoDays":"","applDays":"","start":"0"},{"number":"69","ptaOrPteDate":"12-22-2020","contentsDescription":"Issue
        Fee Payment Received","ptoDays":"","applDays":"","start":"0"},{"number":"67","ptaOrPteDate":"10-20-2020","contentsDescription":"Email
        Notification","ptoDays":"","applDays":"","start":"0"},{"number":"66","ptaOrPteDate":"10-19-2020","contentsDescription":"Email
        Notification","ptoDays":"","applDays":"","start":"0"},{"number":"65","ptaOrPteDate":"10-20-2020","contentsDescription":"Printer
        Rush- No mailing","ptoDays":"","applDays":"","start":"0"},{"number":"64","ptaOrPteDate":"10-20-2020","contentsDescription":"Mailing
        Corrected Notice of Allowability","ptoDays":"","applDays":"","start":"0"},{"number":"62","ptaOrPteDate":"10-15-2020","contentsDescription":"Corrected
        Notice of Allowability","ptoDays":"","applDays":"","start":"0"},{"number":"61","ptaOrPteDate":"10-19-2020","contentsDescription":"Printer
        Rush- No mailing","ptoDays":"","applDays":"","start":"0"},{"number":"60","ptaOrPteDate":"10-19-2020","contentsDescription":"Mail
        Miscellaneous Communication to Applicant","ptoDays":"","applDays":"15","start":"55"},{"number":"59","ptaOrPteDate":"10-13-2020","contentsDescription":"Miscellaneous
        Communication to Applicant - No Action Count","ptoDays":"","applDays":"","start":"0"},{"number":"58","ptaOrPteDate":"10-13-2020","contentsDescription":"Information
        Disclosure Statement considered","ptoDays":"","applDays":"","start":"0"},{"number":"57","ptaOrPteDate":"10-07-2020","contentsDescription":"Pubs
        Case Remand to TC","ptoDays":"","applDays":"","start":"0"},{"number":"56","ptaOrPteDate":"10-05-2020","contentsDescription":"Information
        Disclosure Statement (IDS) Filed","ptoDays":"","applDays":"","start":"0"},{"number":"55","ptaOrPteDate":"10-05-2020","contentsDescription":"Information
        Disclosure Statement (IDS) Filed","ptoDays":"","applDays":"","start":"0"},{"number":"53","ptaOrPteDate":"09-23-2020","contentsDescription":"Electronic
        Review","ptoDays":"","applDays":"","start":"0"},{"number":"52","ptaOrPteDate":"09-23-2020","contentsDescription":"Email
        Notification","ptoDays":"","applDays":"","start":"0"},{"number":"51","ptaOrPteDate":"09-23-2020","contentsDescription":"Mail
        Notice of Allowance","ptoDays":"","applDays":"","start":"0"},{"number":"47","ptaOrPteDate":"09-20-2020","contentsDescription":"Notice
        of Allowance Data Verification Completed","ptoDays":"","applDays":"","start":"0"},{"number":"45","ptaOrPteDate":"09-18-2020","contentsDescription":"Examiner''s
        Amendment Communication","ptoDays":"","applDays":"","start":"0"},{"number":"43","ptaOrPteDate":"07-02-2020","contentsDescription":"Date
        Forwarded to Examiner","ptoDays":"","applDays":"","start":"0"},{"number":"42","ptaOrPteDate":"06-26-2020","contentsDescription":"Response
        after Non-Final Action","ptoDays":"","applDays":"","start":"0"},{"number":"40","ptaOrPteDate":"06-09-2020","contentsDescription":"Email
        Notification","ptoDays":"","applDays":"","start":"0"},{"number":"39","ptaOrPteDate":"06-09-2020","contentsDescription":"Mail
        Applicant Initiated Interview Summary","ptoDays":"","applDays":"","start":"0"},{"number":"38","ptaOrPteDate":"06-03-2020","contentsDescription":"Interview
        Summary - Applicant Initiated - Telephonic","ptoDays":"","applDays":"","start":"0"},{"number":"37","ptaOrPteDate":"06-03-2020","contentsDescription":"Interview
        Summary- Applicant Initiated","ptoDays":"","applDays":"","start":"0"},{"number":"35","ptaOrPteDate":"05-22-2020","contentsDescription":"Case
        Docketed to Examiner in GAU","ptoDays":"","applDays":"","start":"0"},{"number":"33","ptaOrPteDate":"05-21-2020","contentsDescription":"Case
        Docketed to Examiner in GAU","ptoDays":"","applDays":"","start":"0"},{"number":"32","ptaOrPteDate":"03-27-2020","contentsDescription":"Electronic
        Review","ptoDays":"","applDays":"","start":"0"},{"number":"31","ptaOrPteDate":"03-27-2020","contentsDescription":"Email
        Notification","ptoDays":"","applDays":"","start":"0"},{"number":"29","ptaOrPteDate":"03-27-2020","contentsDescription":"Mail
        Non-Final Rejection","ptoDays":"142","applDays":"","start":"0.5"},{"number":"28","ptaOrPteDate":"03-23-2020","contentsDescription":"Non-Final
        Rejection","ptoDays":"","applDays":"","start":"0"},{"number":"27","ptaOrPteDate":"03-23-2020","contentsDescription":"Information
        Disclosure Statement considered","ptoDays":"","applDays":"","start":"0"},{"number":"26","ptaOrPteDate":"01-08-2020","contentsDescription":"Case
        Docketed to Examiner in GAU","ptoDays":"","applDays":"","start":"0"},{"number":"25","ptaOrPteDate":"12-09-2019","contentsDescription":"Case
        Docketed to Examiner in GAU","ptoDays":"","applDays":"","start":"0"},{"number":"21","ptaOrPteDate":"03-29-2019","contentsDescription":"Email
        Notification","ptoDays":"","applDays":"","start":"0"},{"number":"20","ptaOrPteDate":"03-28-2019","contentsDescription":"Application
        ready for PDX access by participating foreign offices","ptoDays":"","applDays":"","start":"0"},{"number":"19","ptaOrPteDate":"03-28-2019","contentsDescription":"PG-Pub
        Issue Notification","ptoDays":"","applDays":"","start":"0"},{"number":"18","ptaOrPteDate":"10-23-2018","contentsDescription":"Priority
        document has successfully retrieved via PDX/DAS","ptoDays":"","applDays":"","start":"0"},{"number":"17","ptaOrPteDate":"09-06-2018","contentsDescription":"Information
        Disclosure Statement (IDS) Filed","ptoDays":"","applDays":"","start":"0"},{"number":"16","ptaOrPteDate":"10-19-2018","contentsDescription":"Case
        Docketed to Examiner in GAU","ptoDays":"","applDays":"","start":"0"},{"number":"15","ptaOrPteDate":"10-18-2018","contentsDescription":"Application
        Dispatched from OIPE","ptoDays":"","applDays":"","start":"0"},{"number":"14","ptaOrPteDate":"10-12-2018","contentsDescription":"Email
        Notification","ptoDays":"","applDays":"","start":"0"},{"number":"13","ptaOrPteDate":"10-11-2018","contentsDescription":"Sent
        to Classification Contractor","ptoDays":"","applDays":"","start":"0"},{"number":"12","ptaOrPteDate":"10-11-2018","contentsDescription":"FITF
        set to YES - revise initial setting","ptoDays":"","applDays":"","start":"0"},{"number":"11","ptaOrPteDate":"09-06-2018","contentsDescription":"Patent
        Term Adjustment - Ready for Examination","ptoDays":"","applDays":"","start":"0"},{"number":"10","ptaOrPteDate":"10-12-2018","contentsDescription":"Application
        Is Now Complete","ptoDays":"","applDays":"","start":"0"},{"number":"9","ptaOrPteDate":"10-12-2018","contentsDescription":"Filing
        Receipt","ptoDays":"","applDays":"","start":"0"},{"number":"8","ptaOrPteDate":"09-06-2018","contentsDescription":"Applicants
        have given acceptable permission for participating foreign ","ptoDays":"","applDays":"","start":"0"},{"number":"7","ptaOrPteDate":"09-06-2018","contentsDescription":"Request
        from applicant for the USPTO to retrieve the Priority Document","ptoDays":"","applDays":"","start":"0"},{"number":"6","ptaOrPteDate":"09-26-2018","contentsDescription":"Cleared
        by L&R (LARS)","ptoDays":"","applDays":"","start":"0"},{"number":"5","ptaOrPteDate":"09-14-2018","contentsDescription":"Referred
        to Level 2 (LARS) by OIPE CSR","ptoDays":"","applDays":"","start":"0"},{"number":"4","ptaOrPteDate":"09-06-2018","contentsDescription":"Information
        Disclosure Statement (IDS) Filed","ptoDays":"","applDays":"","start":"0"},{"number":"3","ptaOrPteDate":"09-06-2018","contentsDescription":"IFW
        Scan & PACR Auto Security Review","ptoDays":"","applDays":"","start":"0"},{"number":"2","ptaOrPteDate":"09-06-2018","contentsDescription":"Entity
        Status Set To Undiscounted (Initial Default Setting or Status Change)","ptoDays":"","applDays":"","start":"0"},{"number":"1","ptaOrPteDate":"09-06-2018","contentsDescription":"Initial
        Exam Team nn","ptoDays":"","applDays":"","start":"0"},{"number":"0.5","ptaOrPteDate":"09-06-2018","contentsDescription":"Filing
        date","ptoDays":"","applDays":"","start":"0"}],"foreignPriority":[{"applicationNumberText":"16123456","priorityClaim":"2017-187096","countryName":"JAPAN","filingDate":"09-27-2017"}],"patentTitle":"LEARNING
        ASSISTANCE DEVICE, METHOD OF OPERATING LEARNING ASSISTANCE DEVICE, LEARNING
        ASSISTANCE PROGRAM, LEARNING ASSISTANCE SYSTEM, AND TERMINAL DEVICE","applDelay":"15","cDelay":"0","appStatusDate":"2021-01-06T00:00:00Z","appAttrDockNumber":"1982-1042PUS1","inventors":[{"nameLineOne":"KANADA","nameLineTwo":"Shoji
        ","suffix":"","streetOne":"","streetTwo":"","city":"Tokyo, ","geoCode":"","country":"(JP)","rankNo":"1"}],"inventorsFacet":"{Shoji||KANADA||||Tokyo||JP|IND|1}","ptoAdjustments":"0","corrAddrStreetLineOne":"8110
        Gatehouse Road","firstInventorFile":"Yes","childContinuity":[{"claimApplicationNumberText":"17130468","applicationNumberText":"16123456","filingDate":"12-22-2020","aiaIndicator":"Y","patentNumberText":"","applicationStatus":"Pending","applicationStatusDescription":"claims
        the benefit of"}],"overlapDelay":"0","aDelay":"142","appType":"Utility","appTypeFacet":"Utility","corrAddrPostalCode":"22042-1248","appCustNumber":"2292","appClsSubCls":"382/159000","appClsSubClsFacet":"382/159000","patentIssueDate":"2021-01-26T00:00:00Z","bDelay":"0","corrAddrNameLineOne":"BIRCH
        STEWART KOLASCH & BIRCH, LLP","firstNamedApplicant":["  FUJIFILM Corporation"],"firstNamedApplicantFacet":"  FUJIFILM
        Corporation","attrnyAddr":[{"applicationId":null,"registrationNo":"19382","fullName":"Birch,
        Terrell  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"22463","fullName":"Kolasch,
        Joseph  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"28380","fullName":"Slattery,
        James  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"28977","fullName":"Murphy,
        Gerald  Jr","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"29271","fullName":"Gorenstein,
        Charles  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"29680","fullName":"Mutter,
        Michael  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"30330","fullName":"Svensson,
        Leonard  ","phoneNum":"858-792-8855","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"32181","fullName":"Weiner,
        Marc  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"32868","fullName":"Meikle,
        Andrew  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"32881","fullName":"Bailey,
        Johnny  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"39538","fullName":"Eller,
        James  Jr","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"40069","fullName":"Armstrong,
        Mary Anne  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"40417","fullName":"Hatsumi,
        Maki  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"40439","fullName":"Anderson,
        David  Jr","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"40953","fullName":"Chong,
        Esther  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"41533","fullName":"McDonald,
        Christopher  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"42325","fullName":"Bilodeau,
        David  ","phoneNum":"703-205-8072","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"42874","fullName":"McRobbie,
        Craig  ","phoneNum":"703-208-4002","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"43368","fullName":"Lewis,
        Paul  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"43451","fullName":"Lutz,
        Laura  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"47099","fullName":"Smith,
        Michael  ","phoneNum":"304-624-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"46607","fullName":"Caudle,
        Penny  ","phoneNum":"--","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"47305","fullName":"Rhodes,
        Jason  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"48501","fullName":"Perez,
        Eugene  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"48917","fullName":"Billings,
        Chad  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"50875","fullName":"Wells,
        Chad  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"52327","fullName":"Voisinet,
        Catherine  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"54577","fullName":"Kim,
        Seth  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"58258","fullName":"Rink,
        Chad  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"58565","fullName":"Larsen,
        James  ","phoneNum":"425-298-6846","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"59981","fullName":"Remily,
        Whitney  ","phoneNum":"703-205-8083","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"61007","fullName":"Hsu,
        Cheng Kang  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"61158","fullName":"Perez-Ramos,
        Vanessa  ","phoneNum":"703-205-8053","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"63406","fullName":"Fritz,
        Bradford  ","phoneNum":"703-836-6505","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"67246","fullName":"Liou,
        Eric  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"68859","fullName":"Konno,
        Toyohiko  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"75257","fullName":"Taousakis,
        Alexander  ","phoneNum":"703-205-8090","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"76455","fullName":"Lin,
        Qing  ","phoneNum":"703-205-8085","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"77520","fullName":"Bureau,
        Hailey  ","phoneNum":"703-205-8038","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"78391","fullName":"Kwon,
        Hyuk Jung  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"78681","fullName":"Nakamura,
        Atsushi  ","phoneNum":"703-205-8000","regStatus":"ACTIVE"},{"applicationId":null,"registrationNo":"79417","fullName":"Gustilo,
        Estella  ","phoneNum":"202-739-5727","regStatus":"ACTIVE"}],"corrAddrCustNo":"2292","corrAddrCountryCd":"US","ptoDelay":"142","corrAddrCity":"Falls
        Church","appEntityStatus":"UNDISCOUNTED","appConfrNumber":"7051","lastUpdatedTimestamp":"2023-01-20T08:40:49.686Z","cmsData":["{16123456|2022-08-26
        04:51:27|PROSECUTION|Examiner''s search strategy and results|SRNT|1|L7AH385RGREENX5},{16123456|2021-01-06
        13:47:08|PROSECUTION|Issue Notification|ISSUE.NTF|1|KJM2MWIEDFLYX10},{16123456|2020-12-22
        09:15:20|PROSECUTION|Electronic Filing System(EFS) Acknowledgment Receipt|N417|2|KJ0DBK1TLDFLYX5},{16123456|2020-12-22
        09:15:20|PROSECUTION|Fee Worksheet (SB06)|WFEE|2|KJ0DBK1SLDFLYX5},{16123456|2020-12-22
        09:15:20|PROSECUTION|Issue Fee Payment (PTO-85B)|IFEE|1|KJ0DBK1RLDFLYX5},{16123456|2020-10-20
        03:19:47|PROSECUTION|Notice of Allowance and Fees Due (PTOL-85)|NOA|2|KGC5TXULLDFLYX9},{16123456|2020-10-20
        03:19:47|PROSECUTION|List of References cited by applicant and considered
        by examiner|1449|2|KGC5TXUNLDFLYX9},{16123456|2020-10-19 07:19:45|PROSECUTION|Miscellaneous
        Communication to Applicant - No Action Count|M327|2|KGAYYNY3LDFLYX5},{16123456|2020-10-19
        07:19:45|PROSECUTION|List of References cited by applicant and considered
        by examiner|1449|2|KGAYYNY5LDFLYX5},{16123456|2020-10-05 13:19:17|PROSECUTION|Non
        Patent Literature|NPL|5|KFX1ELATLDFLYX7},{16123456|2020-10-05 13:19:17|PROSECUTION|Information
        Disclosure Statement (IDS) Form (SB08)|IDS|2|KFX1ELAPLDFLYX7},{16123456|2020-10-05
        13:19:17|PROSECUTION|Transmittal Letter|TRAN.LET|5|KFX1ELAVLDFLYX7},{16123456|2020-10-05
        13:19:17|PROSECUTION|Foreign Reference|FOR|6|KFX1ELASLDFLYX7},{16123456|2020-10-05
        13:19:17|PROSECUTION|Fee Worksheet (SB06)|WFEE|2|KFX1ELAQLDFLYX7},{16123456|2020-10-05
        13:19:17|PROSECUTION|Electronic Filing System(EFS) Acknowledgment Receipt|N417|3|KFX1ELARLDFLYX7},{16123456|2020-10-05
        13:19:17|PROSECUTION|Transmittal Letter|TRAN.LET|1|KFX1ELAULDFLYX7},{16123456|2020-09-23
        06:47:08|PROSECUTION|Issue Information including classification, examiner,
        name, claim, renumbering, etc.|IIFW|3|KFCN8CLCRXEAPX3},{16123456|2020-09-23
        06:47:08|PROSECUTION|Examiner''s search strategy and results|SRNT|1|KFCN8CLFRXEAPX3},{16123456|2020-09-23
        06:47:08|PROSECUTION|Search information including classification, databases
        and other search related notes|SRFW|1|KFCN8CLERXEAPX3},{16123456|2020-09-23
        06:47:08|PROSECUTION|Notice of Allowance and Fees Due (PTOL-85)|NOA|7|KFCN8CLBRXEAPX3},{16123456|2020-06-26
        11:13:35|PROSECUTION|Applicant Arguments/Remarks Made in an Amendment|REM|6|KBWLFU3VRXEAPX2},{16123456|2020-06-26
        11:13:35|PROSECUTION|Electronic Filing System(EFS) Acknowledgment Receipt|N417|2|KBWLFU3WRXEAPX2},{16123456|2020-06-26
        11:13:35|PROSECUTION|Claims|CLM|7|KBWLFU3URXEAPX2},{16123456|2020-06-26 11:13:35|PROSECUTION|Amendment/Request
        for Reconsideration-After Non-Final Rejection|A...|1|KBWLFU3TRXEAPX2},{16123456|2020-06-26
        11:13:35|PROSECUTION|Transmittal Letter|TRAN.LET|1|KBWLFU3XRXEAPX2},{16123456|2020-06-26
        03:39:52|PROSECUTION|Fee Worksheet (SB06)|WFEE|1|KC4PVGZURXEAPX2},{16123456|2020-06-08
        21:28:00|PROSECUTION|Applicant Initiated Interview Summary (PTOL-413)|INTV.SUM.APP|3|KB3753VDRXEAPX1},{16123456|2020-03-27
        04:15:37|PROSECUTION|List of References cited by applicant and considered
        by examiner|1449|1|K87AK41MRXEAPX5},{16123456|2020-03-27 04:15:37|PROSECUTION|Search
        information including classification, databases and other search related notes|SRFW|1|K87AK41IRXEAPX5},{16123456|2020-03-27
        04:15:37|PROSECUTION|Index of Claims|FWCLM|1|K87AK41JRXEAPX5},{16123456|2020-03-27
        04:15:37|PROSECUTION|List of references cited by examiner|892|1|K87AK41HRXEAPX5},{16123456|2020-03-27
        04:15:37|PROSECUTION|Examiner''s search strategy and results|SRNT|2|K87AK41KRXEAPX5},{16123456|2020-03-27
        04:15:37|PROSECUTION|Bibliographic Data Sheet|BIB|1|K87AK41LRXEAPX5},{16123456|2020-03-27
        04:15:37|PROSECUTION|Non-Final Rejection|CTNF|9|K87AK41FRXEAPX5},{16123456|2019-03-28
        05:59:47|PROSECUTION|Notice of Publication|NTC.PUB|1|JTSPFUAFRXEAPX4},{16123456|2018-10-23
        03:43:21|PROSECUTION|Priority Documents electronically retrieved by USPTO
        from a participating IP Office|PD.FILED.E|30|JNLNVIHARXEAPX1},{16123456|2018-10-12
        13:34:10|PROSECUTION|Fee Worksheet (SB06)|WFEE|1|JN53P2IURXEAPX0},{16123456|2018-10-12
        03:10:24|PROSECUTION|Filing Receipt|APP.FILE.REC|4|JN4HEWH3RXEAPX5},{16123456|2018-09-06
        11:08:26|PROSECUTION|Electronic Filing System(EFS) Acknowledgment Receipt|N417|3|JLQY2WZMRXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Fee Worksheet (SB06)|WFEE|2|JLQY2WZLRXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Drawings-other than black and white line drawings|DRW.NONBW|9|JLQY2WZSRXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Oath or Declaration filed|OATH|2|JLQY2WZRRXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Specification|SPEC|22|JLQY2WZORXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Abstract|ABST|1|JLQY2WZQRXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Information Disclosure Statement (IDS) Form (SB08)|IDS|1|JLQY2WZKRXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Foreign Reference|FOR|13|JLQY2WZTRXEAPX4},{16123456|2018-09-06
        11:08:26|PROSECUTION|Claims|CLM|5|JLQY2WZPRXEAPX4},{16123456|2018-09-06 11:08:26|PROSECUTION|Application
        Data Sheet|ADS|8|JLQY2WZNRXEAPX4},{16123456|2018-09-06 11:08:25|PROSECUTION|Power
        of Attorney|PA..|2|JLQY2TUNRXEAPX0},{16123456|2018-09-06 11:08:25|PROSECUTION|Transmittal
        Letter|TRAN.LET|5|JLQY2TUORXEAPX0},{16123456|2018-09-06 11:08:25|PROSECUTION|Transmittal
        of New Application|TRNA|2|JLXS0Y2CLXEAPX3},{16123456|2018-09-06 03:55:43|PROSECUTION|Placeholder
        sheet indicating presence of supplemental content in Supplemental Complex
        Repository for Examiners(SCORE)|SCORE|1|JMT3ID42RXEAPX1}"],"appAttrDockNumberFacet":"1982-1042PUS1","appEntityStatusFacet":"UNDISCOUNTED","appCustNumberFacet":"2292","_version_":1755531053401899008,"firstInventorFileFacet":"Yes","appLocationFacet":"ELECTRONIC","appEarlyPubNumberFacet":"US20190095759A1","patentNumberFacet":"10902286"}]}},"queryId":"5711b9de-3409-4298-bcf1-4a4f5dbf2252"},"jobStatus":null,"queryId":"5711b9de-3409-4298-bcf1-4a4f5dbf2252","page":1,"count":0,"createQueryRequest":{"searchText":"applId:(16123456)","facet":"false","facetField":null,"facetLimit":null,"facetMissing":null,"facetDate":null,"facetDateGap":null,"facetDateStart":null,"facetDateEnd":null,"facetDateOther":null,"facetDateInclude":null,"mm":null,"sort":null,"qf":"appEarlyPubNumber
        applId appLocation appType appStatus_txt appConfrNumber appCustNumber appGrpArtNumber
        appCls appSubCls appEntityStatus_txt patentNumber patentTitle primaryInventor
        firstNamedApplicant appExamName appExamPrefrdName appAttrDockNumber appPCTNumber
        appIntlPubNumber wipoEarlyPubNumber pctAppType firstInventorFile appClsSubCls
        rankAndInventorsList","wt":null,"df":null,"fl":"*","start":"0","fq":[],"rows":null,"parameters":{"qf":"appEarlyPubNumber
        applId appLocation appType appStatus_txt appConfrNumber appCustNumber appGrpArtNumber
        appCls appSubCls appEntityStatus_txt patentNumber patentTitle primaryInventor
        firstNamedApplicant appExamName appExamPrefrdName appAttrDockNumber appPCTNumber
        appIntlPubNumber wipoEarlyPubNumber pctAppType firstInventorFile appClsSubCls
        rankAndInventorsList","fl":["*"],"start":"0","fq":[],"rows":25,"facet":"false"}},"links":[{"rel":"self","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252?format=XML"},{"rel":"results","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/results?page=1"},{"rel":"package","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/package?format=XML"},{"rel":"first","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/results?page=1"},{"rel":"next","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/results?page=2"},{"rel":"self","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252?format=JSON"},{"rel":"results","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/results?page=1"},{"rel":"package","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/package?format=JSON"},{"rel":"first","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/results?page=1"},{"rel":"next","href":"https://ped.uspto.gov/api/queries/5711b9de-3409-4298-bcf1-4a4f5dbf2252/results?page=2"}]}'
    headers:
      Cache-Control:
      - no-cache, no-store, max-age=0, must-revalidate
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:46:19 GMT
      Expires:
      - '0'
      Pragma:
      - no-cache
      Server-Timing:
      - intid;desc=7abaf2d041a8a4c5
      Strict-Transport-Security:
      - max-age=31536000;includeSubDomains
      Via:
      - 1.1 125207e6ab84815ebe7dac5799ad56c2.cloudfront.net (CloudFront)
      X-Amz-Cf-Id:
      - _2jtRn8mSVTjaRdejEWbVeQ8-7y0iMWCXMbPc1HwgV0op0VMzq88iw==
      X-Amz-Cf-Pop:
      - DFW50-C1
      X-Cache:
      - Miss from cloudfront
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - DENY
      X-XSS-Protection:
      - 1; mode=block
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 17:50:32 GMT
      apigw-requestid:
      - fDYgbjy7oAMEMiA=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: '{"country":"US","internal":"false","corrAppNum":"16123456","id":"16123456","type":"application","list":[{"appNum":"2017187096","appDate":1506470400000,"pubList":[{"pubCountry":"JP","pubNum":"2019061579","pubDate":1555545600000,"kindCode":"A","pubDateStr":"04/18/2019"},{"pubCountry":"JP","pubNum":"6768620","pubDate":1602633600000,"kindCode":"B2","pubDateStr":"10/14/2020"}],"countryCode":"JP","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/27/2017","docListMsg":null,"docNum":{"country":"JP","docNumber":"2017187096","format":null,"date":"09/27/2017","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"16123456","appDate":1536192000000,"pubList":[{"pubCountry":"US","pubNum":"20190095759","pubDate":1553731200000,"kindCode":"A1","pubDateStr":"03/28/2019"},{"pubCountry":"US","pubNum":"10902286","pubDate":1611619200000,"kindCode":"B2","pubDateStr":"01/26/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/06/2018","docListMsg":null,"docNum":{"country":"US","docNumber":"201816123456","format":null,"date":"09/06/2018","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"17130468","appDate":1608595200000,"pubList":[{"pubCountry":"US","pubNum":"20210110206","pubDate":1618444800000,"kindCode":"A1","pubDateStr":"04/15/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"US","docNumber":"16123456","kindCode":"A"},{"country":"US","docNumber":"17130468","kindCode":"A"},{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"12/22/2020","docListMsg":null,"docNum":{"country":"US","docNumber":"202017130468","format":null,"date":"12/22/2020","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '1885'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:50:33 GMT
      apigw-requestid:
      - fDYgbhp1IAMEM-Q=
    status:
      code: 200
      message: OK
- request:
    body: '"27611"'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '7'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/users/me/session
  response:
    body:
      string: '{"userSessionId":2168601,"ipAddress":null,"displayName":"anonymous","deleteIn":false,"userCase":{"caseId":2168779,"userId":2168601,"caseName":"Untitled
        Case","applicationNumber":null},"userPreferences":{},"hostURLs":{"image_server_host":"https://pasr-img.pvt.uspto.gov"},"versions":[{"product":"API","releaseNumber":"2.0.0.1","date":"13.12.2022
        @ 15:09:41 UTC","commitId":"801fde86","collectionName":""},{"product":"SOLR","releaseNumber":"7.6.0","date":"5-4-0","commitId":"","collectionName":"us_patent_grant"}],"documentsLimitInQuery":1000,"sessionTimeOutTime":1800}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:52:36 GMT
      Server-Timing:
      - intid;desc=2771a77e3c83fe71
    status:
      code: 200
      message: ''
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 2168779, "hl_snippets": "2", "op": "OR", "q": "\"20190095759\".PN.",
      "queryName": "\"20190095759\".PN.", "highlights": "1", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "US-PGPUB", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "\"20190095759\".PN."}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '727'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":2168779,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"US-PGPUB","countryCodes":[]}],"q":"\"20190095759\".PN.","queryName":"\"20190095759\".PN.","userEnteredQuery":"\"20190095759\".PN.","viewName":"tile","op":"OR","highlights":"1","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"\"20190095759\".PN.","error":null,"terms":["\"20190095759\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":34,"highlightingTime":0,"cursorMarker":"AoJwwJTm4OkCOjY1ODA3Njk2IVBHLVVTLTIwMTkwMDk1NzU5","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-20190095759-A1","publicationReferenceDocumentNumber":"20190095759","compositeId":"65807696!PG-US-20190095759","publicationReferenceDocumentNumber1":"20190095759","datePublishedKwicHits":null,"datePublished":"2019-03-28T00:00:00Z","inventionTitle":"LEARNING
        ASSISTANCE DEVICE, METHOD OF OPERATING LEARNING ASSISTANCE DEVICE, LEARNING
        ASSISTANCE PROGRAM, LEARNING ASSISTANCE SYSTEM, AND TERMINAL DEVICE","type":"US-PGPUB","mainClassificationCode":"1/1","applicantName":["FUJIFILM
        Corporation"],"assigneeName":["FUJIFILM Corporation"],"uspcFullClassificationFlattened":null,"ipcCodeFlattened":"G06K9/62;G16H30/20","cpcInventiveFlattened":"G06F18/2178;G06N3/042;G06V10/774;G06N3/045;G06V10/7784;G16H50/70;G06N3/08;G16H30/20;G06F18/214;G06F18/217;G16H30/40;G06N20/00","cpcAdditionalFlattened":null,"applicationFilingDate":["2018-09-06T00:00:00Z"],"applicationFilingDateKwicHits":null,"relatedApplFilingDate":null,"primaryExaminer":null,"assistantExaminer":null,"applicationNumber":"16/123456","frontPageStart":1,"frontPageEnd":1,"drawingsStart":2,"drawingsEnd":10,"specificationStart":11,"specificationEnd":19,"claimsStart":19,"claimsEnd":21,"abstractStart":1,"abstractEnd":1,"bibStart":1,"bibEnd":1,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":21,"pageCountDisplay":"21","previouslyViewed":false,"unused":false,"imageLocation":"us-pgpub/US/2019/0095/759","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":"KANADA;
        Shoji","familyIdentifierCur":65807696,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"PGPB","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":["2017-09-27T00:00:00Z"],"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        20190095759 A1","derwentAccessionNumber":null,"documentSize":84795,"score":13.732954,"governmentInterest":null,"kindCode":["A1"],"urpn":null,"urpnCode":null,"publicationReferenceDocumentNumberOne":"20190095759","descriptionStart":11,"descriptionEnd":19}],"qtime":30}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:52:36 GMT
      Server-Timing:
      - intid;desc=4ef9fce079a0761f
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://ppubs.uspto.gov/dirsearch-public/patents/US-20190095759-A1/highlight?queryId=1&source=US-PGPUB&includeSections=True
  response:
    body:
      string: "{\"guid\":\"US-20190095759-A1\",\"publicationReferenceDocumentNumber\":\"20190095759\",\"compositeId\":\"65807696!PG-US-20190095759\",\"publicationReferenceDocumentNumber1\":\"20190095759\",\"datePublishedKwicHits\":null,\"datePublished\":\"2019-03-28T00:00:00Z\",\"inventionTitle\":\"LEARNING
        ASSISTANCE DEVICE, METHOD OF OPERATING LEARNING ASSISTANCE DEVICE, LEARNING
        ASSISTANCE PROGRAM, LEARNING ASSISTANCE SYSTEM, AND TERMINAL DEVICE\",\"type\":\"US-PGPUB\",\"mainClassificationCode\":\"1/1\",\"applicantName\":[\"FUJIFILM
        Corporation\"],\"assigneeName\":[\"FUJIFILM Corporation\"],\"uspcFullClassificationFlattened\":null,\"ipcCodeFlattened\":\"G06K9/62;G16H30/20\",\"cpcInventiveFlattened\":\"G06F18/2178;G06N3/042;G06V10/774;G06N3/045;G06V10/7784;G16H50/70;G06N3/08;G16H30/20;G06F18/214;G06F18/217;G16H30/40;G06N20/00\",\"cpcAdditionalFlattened\":null,\"applicationFilingDate\":[\"2018-09-06T00:00:00Z\"],\"applicationFilingDateKwicHits\":null,\"relatedApplFilingDate\":null,\"primaryExaminer\":null,\"assistantExaminer\":null,\"applicationNumber\":\"16/123456\",\"frontPageStart\":1,\"frontPageEnd\":1,\"drawingsStart\":2,\"drawingsEnd\":10,\"specificationStart\":11,\"specificationEnd\":19,\"claimsStart\":19,\"claimsEnd\":21,\"abstractStart\":1,\"abstractEnd\":1,\"bibStart\":1,\"bibEnd\":1,\"certCorrectionStart\":0,\"certCorrectionEnd\":0,\"certReexaminationStart\":0,\"certReexaminationEnd\":0,\"supplementalStart\":0,\"supplementalEnd\":0,\"ptabStart\":0,\"ptabEnd\":0,\"amendStart\":0,\"amendEnd\":0,\"searchReportStart\":0,\"searchReportEnd\":0,\"pageCount\":21,\"pageCountDisplay\":\"21\",\"previouslyViewed\":false,\"unused\":false,\"imageLocation\":\"us-pgpub/US/2019/0095/759\",\"imageFileName\":\"00000001.tif\",\"cpcCodes\":null,\"queryId\":1,\"tags\":null,\"inventorsShort\":\"KANADA;
        Shoji\",\"familyIdentifierCur\":65807696,\"familyIdentifierCurStr\":\"65807696\",\"languageIndicator\":\"EN\",\"databaseName\":\"PGPB\",\"dwImageDoctypeList\":null,\"dwImageLocList\":null,\"dwPageCountList\":null,\"dwImageDocidList\":null,\"patentFamilyMembers\":null,\"patentFamilyCountry\":null,\"patentFamilySerialNumber\":null,\"documentIdWithDashesDw\":null,\"pfPublDate\":null,\"pfPublDateKwicHits\":null,\"priorityClaimsDate\":[\"2017-09-27T00:00:00Z\"],\"priorityClaimsDateKwicHits\":null,\"pfApplicationSerialNumber\":null,\"pfApplicationDescriptor\":null,\"pfLanguage\":null,\"pfApplicationDate\":null,\"pfApplicationDateKwicHits\":null,\"clippedUri\":null,\"source\":null,\"documentId\":\"<span
        term=\\\"us20190095759a1\\\" class=\\\"highlight18\\\">US 20190095759 A1</span>\",\"derwentAccessionNumber\":null,\"documentSize\":84795,\"score\":0.0,\"governmentInterest\":null,\"kindCode\":[\"A1\"],\"urpn\":null,\"urpnCode\":null,\"abstractedPatentNumber\":null,\"assigneeCity\":[\"Tokyo\"],\"assigneePostalCode\":[\"N/A\"],\"assigneeState\":[\"N/A\"],\"assigneeTypeCode\":[\"03\"],\"curIntlPatentClassificationPrimary\":[\"G06K9/62
        20060101\"],\"curIntlPatentClassificationPrimaryDateKwicHits\":null,\"designatedStates\":null,\"examinerGroup\":null,\"issuedUsCrossRefClassification\":null,\"jpoFtermCurrent\":null,\"languageOfSpecification\":null,\"chosenDrawingsReference\":null,\"derwentClass\":null,\"inventionTitleHighlights\":null,\"cpcOrigInventiveClassificationHighlights\":[\"G06K9/6262
        20130101\",\"G06N99/005 20130101\",\"G16H30/20 20180101\"],\"cpcInventiveDateKwicHits\":null,\"cpcOrigAdditionalClassification\":null,\"cpcAdditionalDateKwicHits\":null,\"curIntlPatentClssficationSecHighlights\":null,\"fieldOfSearchClassSubclassHighlights\":null,\"cpcCombinationSetsCurHighlights\":null,\"applicantCountry\":[\"JP\"],\"applicantCity\":[\"Tokyo\"],\"applicantState\":[\"N/A\"],\"applicantZipCode\":[\"N/A\"],\"applicantAuthorityType\":[\"assignee\"],\"applicantDescriptiveText\":null,\"applicationSerialNumber\":[\"123456\"],\"inventorCity\":[\"Tokyo\"],\"inventorState\":[\"N/A\"],\"inventorPostalCode\":[\"N/A\"],\"standardTitleTermsHighlights\":null,\"primaryExaminerHighlights\":null,\"continuityData\":null,\"inventors\":null,\"uspcFullClassification\":null,\"uspcCodeFmtFlattened\":null,\"ipcCode\":null,\"applicationNumberHighlights\":[\"16/123456\"],\"dateProduced\":\"2019-03-12T00:00:00Z\",\"auxFamilyMembersGroupTempPlaceHolder\":null,\"priorityCountryCode\":null,\"cpcCurAdditionalClassification\":null,\"internationalClassificationMain\":null,\"internationalClassificationSecondary\":null,\"internationalClassificationInformational\":null,\"europeanClassification\":null,\"europeanClassificationMain\":null,\"europeanClassificationSecondary\":null,\"lanuageIndicator\":null,\"intlPubClassificationPrimary\":[\"G06K9/62
        20060101 G06K009/62\"],\"intlPubClassificationPrimaryDateKwicHits\":null,\"intlPubClassificationSecondary\":[\"G16H30/20
        20060101 G16H030/20\",\"G06N99/00 20060101 G06N099/00\"],\"intlPubClassificationSecondaryDateKwicHits\":null,\"publicationDate\":null,\"derwentWeekInt\":0,\"derwentWeek\":null,\"currentUsOriginalClassification\":\"1/1\",\"currentUsCrossReferenceClassification\":null,\"locarnoClassification\":null,\"equivalentAbstractText\":null,\"hagueIntlRegistrationNumber\":null,\"hagueIntlFilingDate\":null,\"hagueIntlFilingDateKwicHits\":null,\"hagueIntlRegistrationDate\":null,\"hagueIntlRegistrationDateKwicHits\":null,\"hagueIntlRegistrationPubDate\":null,\"hagueIntlRegistrationPubDateKwicHits\":null,\"curIntlPatentClassificationNoninvention\":null,\"curIntlPatentClassificationNoninventionDateKwicHits\":null,\"curIntlPatentClassificationSecondary\":[\"G16H30/20
        20060101\",\"G06N99/00 20060101\"],\"curIntlPatentClassificationSecondaryDateKwicHits\":null,\"abstractHtml\":\"A
        learning assistance device acquires a plurality of learned discriminators
        obtained by causing learning discriminators provided in a plurality of respective
        terminal devices to perform learning using image correct answer data, acquires
        a plurality of discrimination results obtained by causing a plurality of learned
        discriminators to discriminate the same input image, determines the correct
        answer data of the input image on the basis of the plurality of discrimination
        results, causes the discriminator to perform learning the input image and
        the correct answer data, and outputs a result thereof as a new learning discriminator
        to each terminal device.\",\"descriptionHtml\":\"BRIEF DESCRIPTION OF THE
        DRAWINGS<br />[0026] <figref idref=\\\"DRAWINGS\\\">FIG. 1</figref> is a diagram
        illustrating a schematic configuration of a learning assistance system of
        the present invention.<br />[0027] <figref idref=\\\"DRAWINGS\\\">FIG. 2</figref>
        is a diagram illustrating a schematic configuration of a medical information
        system.<br />[0028] <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref> illustrates
        an example of a multilayered neural network.<br />[0029] <figref idref=\\\"DRAWINGS\\\">FIG.
        4</figref> is a block diagram illustrating a schematic configuration of a
        terminal device and a learning assistance device according to a first embodiment.<br
        />[0030] <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref> is a diagram illustrating
        learning of a discriminator.<br />[0031] <figref idref=\\\"DRAWINGS\\\">FIG.
        6</figref> is a flowchart showing a flow of a process of causing the discriminator
        to perform learning.<br />[0032] <figref idref=\\\"DRAWINGS\\\">FIG. 7</figref>
        is a block diagram illustrating a schematic configuration of a terminal device
        and a learning assistance device according to a second embodiment.<br />[0033]
        <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref> is a block diagram illustrating
        a schematic configuration of a terminal device and a learning assistance device
        according to a third embodiment.<br />[0034] <figref idref=\\\"DRAWINGS\\\">FIG.
        9</figref> is a block diagram illustrating a schematic configuration of a
        terminal device and a learning assistance device according to a fourth embodiment.<br
        />DETAILED DESCRIPTION<br />[0035] <figref idref=\\\"DRAWINGS\\\">FIG. 1</figref>
        illustrates a schematic configuration of a learning assistance system <b>1</b>
        according to a first embodiment of the present invention. The learning assistance
        system <b>1</b> is configured by connecting a plurality of terminal devices
        <b>10</b> installed in a plurality of medical institutions A, B, . . ., X
        and a learning assistance device <b>20</b> placed on a cloud side over a network
        <b>30</b>.<br />[0036] The learning assistance device <b>20</b> includes a
        well-known hardware configuration such as a central processing unit (CPU),
        a memory, a storage, an input and output interface, a communication interface,
        an input device, a display device, and a data bus, and is a high-performance
        computer in which a well-known operation system or the like is installed and
        which has a server function. Further, a graphics processing unit (GPU) may
        be provided, as necessary. Alternatively, the learning assistance device <b>20</b>
        may be a virtualized virtual server provided using one or a plurality of computers.
        The learning assistance program of the present invention is installed in a
        server, and functions as a learning assistance device by a program instruction
        being executed by the CPU of the computer.<br />[0037] The terminal device
        <b>10</b> is a computer for image processing provided in the respective medical
        institutions A, B, . . ., X, and includes a well-known hardware configuration
        such as a CPU, a memory, a storage, an input and output interface, a communication
        interface, an input device, a display device, and a data bus. A well-known
        operation system or the like is installed in the terminal device <b>10</b>.
        The terminal device <b>10</b> includes a display as a display device. Further,
        a GPU may be provided, as necessary.<br />[0038] The network <b>30</b> is
        a wide area network (WAN) that widely connects the terminal devices <b>10</b>
        placed at the plurality of medical institutions A, B, . . ., X to the learning
        assistance device <b>20</b> via a public network or a private network.<br
        />[0039] Further, as illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 2</figref>,
        the terminal device <b>10</b> is connected to respective medical information
        systems <b>50</b> of the respective medical institutions A, B, . . ., X over
        a local area network (LAN) <b>51</b>. The medical information system <b>50</b>
        includes a modality (an imaging device) <b>52</b>, an image database <b>53</b>,
        and an image interpretation medical workstation <b>54</b>, and is configured
        so that transmission and reception of image data to and from each other are
        performed over the network <b>51</b>. It should be noted that in the network
        <b>51</b>, it is desirable to use a communication cable such as an optical
        fiber so that image data can be transferred at a high speed.<br />[0040] The
        modality <b>52</b> includes a device that images an examination target part
        of a subject to generate an examination image representing the part, adds
        accessory information defined in a DICOM standard to the image, and outputs
        the resultant image. Specific examples of the device include a computed tomography
        (CT) device, a magnetic resonance imaging (MRI) device, a positron emission
        tomography (PET) device, an ultrasonic device, and a computed radiography
        (CR) device using a planar X-ray detector (FPD: flat panel detector).<br />[0041]
        In the image database <b>53</b>, a software program for providing a function
        of a database management system (DBMS) is incorporated in a general-purpose
        computer, and a large capacity storage is included. This storage may be a
        large capacity hard disk device, or may be a disk device connected to a network
        attached storage (NAS) or a storage area network (SAN) connected to the network
        <b>51</b>. Further, the image data captured by the modality <b>52</b> is transmitted
        to and stored in the image database <b>53</b> over the network <b>51</b> according
        to a storage format and a communication standard conforming to a DICOM standard.<br
        />[0042] The image interpretation medical workstation <b>54</b> is a computer
        that is used for an image interpretation doctor of a radiology department
        to interpret an image and create an interpretation report. The image interpretation
        medical workstation <b>54</b> performs a display of the image data received
        from the image database <b>53</b> and performs automatic detection of a portion
        likely to be a lesion in the image.<br />[0043] In the embodiment, a case
        where an image processing program in which a discriminator functioning as
        an actually operated discriminator is incorporated in each terminal device
        <b>10</b> is provided from the learning assistance device <b>20</b>, and a
        learning program in which a discriminator functioning as a learning program
        separately from the image processing program is incorporated is provided will
        be described. The image processing program and the learning program distributed
        to each terminal device <b>10</b> are installed in the terminal device <b>10</b>
        to function as an image processing device in which the actually operated discriminator
        is incorporated, and a learning discriminator.<br />[0044] Further, a case
        where the actually operated discriminator and the learning discriminator are
        multilayered neural networks subjected to deep learning to be able to discriminate
        a plurality of types of organ areas and/or lesion areas will be described.
        In the multilayered neural network, a calculation process is performed on
        a plurality of pieces of different calculation result data obtained by a preceding
        layer for input data, that is, extraction result data of a feature amount
        using various kernels in each layer, data of the feature amount obtained by
        the calculation process is acquired, and a further calculation process is
        performed on the data of the feature amount in the next and subsequent processing
        layers. Thus, it is possible to improve a recognition rate of the feature
        amount and to discriminate which of a plurality of types of areas the input
        image data is.<br />[0045] <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref>
        is a diagram illustrating an example of the multilayered neural network. As
        illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref>, the multilayered
        neural network <b>40</b> includes a plurality of layers including an input
        layer <b>41</b> and an output layer <b>42</b>. In <figref idref=\\\"DRAWINGS\\\">FIG.
        3</figref>, a layer before the output layer <b>42</b> is denoted by reference
        numeral <b>43</b>.<br />[0046] In the multilayered neural network <b>40</b>,
        the image data is input to the input layer <b>41</b> and a discrimination
        result of an area is output. In a case where learning is performed, the output
        discrimination result is compared with correct answer data, and a weight of
        coupling between the respective layers of units (indicated by circles in <figref
        idref=\\\"DRAWINGS\\\">FIG. 3</figref>) included in the respective layers
        of the multilayered neural network <b>40</b> is corrected from the output
        side (the output layer <b>42</b>) to the input side (the input layer <b>41</b>)
        according to whether an answer is a correct answer or an incorrect answer.
        The correction of the weight of coupling is repeatedly performed a predetermined
        number of times, or is repeatedly performed until a correct answer rate of
        the output discrimination result is 100% or is equal to or greater than a
        predetermined threshold value using a large number of pieces of image data
        with correct answer data, and the learning ends.<br />[0047] <figref idref=\\\"DRAWINGS\\\">FIG.
        4</figref> is a block diagram illustrating a schematic configuration of the
        terminal device <b>10</b> and the learning assistance device <b>20</b>. Functions
        of the terminal device <b>10</b> and the learning assistance device <b>20</b>
        will be described in detail with reference to <figref idref=\\\"DRAWINGS\\\">FIG.
        4</figref>. First, the terminal device <b>10</b> will be described.<br />[0048]
        The terminal device <b>10</b> includes a discriminator acquisition unit <b>12</b>,
        a discrimination result acquisition unit <b>13</b>, a learning unit <b>14</b>,
        and a learned discriminator output unit <b>15</b>.<br />[0049] The discriminator
        acquisition unit <b>12</b> acquires a learning discriminator and an actually
        operated discriminator. For example, the image processing program and the
        learning program are received from the learning assistance device <b>20</b>
        over the network <b>30</b>, and the received image processing program is installed.
        Accordingly, image processing in which the actually operated discriminator
        is incorporated becomes executable in the terminal device <b>10</b> and functions
        as the discrimination result acquisition unit <b>13</b>. Similarly, the learning
        program is installed, and the learning discriminator becomes executable and
        functions as the learning unit <b>14</b>. It should be noted that the learning
        discriminator is a discriminator that has learned the same image correct answer
        data as the actually operated discriminator received from the learning assistance
        device <b>20</b>. In the following description, the image processing in which
        the actually operated discriminator is incorporated is simply referred to
        as an actually operated discriminator. It should be noted that the image correct
        answer data refers to a combination of the image data and correct answer data
        thereof. Details of the image correct answer data will be described below.<br
        />[0050] The discrimination result acquisition unit <b>13</b> inputs a discrimination
        target image data to the actually operated discriminator and acquires a discrimination
        result. The actually operated discriminator is a discriminator of which discrimination
        performance has been guaranteed in the learning assistance device <b>20</b>,
        and in each of the medical institutions A, B, X, the discrimination is performed
        on the image data that is a diagnosis target using the actually operated discriminator.
        Further, the discrimination result acquisition unit <b>13</b> may perform
        discrimination of the image data that is a diagnosis target sent from the
        image interpretation medical workstation <b>54</b> to the terminal device
        <b>10</b> over the network <b>51</b>, and transmit a discrimination result
        from the terminal device <b>10</b> to the image interpretation medical workstation
        <b>54</b>.<br />[0051] The learning unit <b>14</b> causes the learning discriminator
        to perform learning using the image data and the correct answer data thereof.
        The correct answer data includes a mask image showing an area such as an organ
        or abnormal shadow of the image data, and information indicating what the
        area of the mask image is (for example, an area of an organ such as a liver,
        a kidney, or a lung or an area of an abnormal shadow such as a liver cancer,
        a kidney cancer, or a pulmonary nodule).<br />[0052] The correct answer data
        may be created by an image interpretation doctor or the like of each of the
        medical institutions A, B, . . ., X observing the image data. For example,
        the image data is extracted from the image database <b>53</b>, the discrimination
        result acquisition unit <b>13</b> inputs the image data to the actually operated
        discriminator and acquires a discrimination result, and a user such as the
        image interpretation doctor determines whether the discrimination result is
        a correct answer or an incorrect answer, and stores a discrimination result
        together with the input image data and correct answer data in the image database
        <b>53</b> as image correct answer data in the case of the correct answer.
        In the case of an incorrect answer, the user generates a mask image of the
        correct answer data, assigns the correct answer data to the image data, and
        stores the resultant data in the image database <b>53</b> as image correct
        answer data.<br />[0053] Therefore, the learning unit <b>14</b> causes the
        multilayered neural network <b>40</b> of the learning discriminator to perform
        learning using a large number of pieces of image correct answer data stored
        in the image database <b>53</b>. First, the image data of the image correct
        answer data is input to the multilayered neural network <b>40</b>, and a discrimination
        result is output. Then, the output discrimination result is compared with
        the correct answer data, and a weight of coupling between the respective layers
        of the units included in the respective layers of the multilayered neural
        network <b>40</b> from the output side to the input side is corrected according
        to whether the answer is a correct answer or an incorrect answer. The correction
        of the weight of the coupling is repeatedly performed using a large number
        of pieces of correct answer data a predetermined number of times or until
        the correct answer rate of the output discrimination result becomes 100%,
        and the learning is ended.<br />[0054] The learned discriminator output unit
        <b>15</b> outputs the learning discriminator of which the learning has ended
        in the learning unit <b>14</b> as a learned discriminator. Specifically, the
        weight (hereinafter referred to as a parameter) of coupling between the layers
        of the units constituting the neural network constituting the learned discriminator
        is periodically transmitted to the learning assistance device <b>20</b> over
        the network <b>30</b>.<br />[0055] Next, the learning assistance device <b>20</b>
        will be described. As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 4</figref>,
        the learning assistance device <b>20</b> includes a learned discriminator
        acquisition unit <b>22</b>, a discriminator storage unit <b>23</b>, a correct
        answer data acquisition unit <b>24</b>, a correct answer data storage unit
        <b>25</b>, a learning unit <b>26</b>, and a discriminator output unit <b>27</b>.<br
        />[0056] The learned discriminator acquisition unit <b>22</b> receives a parameter
        of the multilayered neural network <b>40</b> constituting the learned discriminators
        transmitted from the plurality of terminal devices <b>10</b> over the network
        <b>30</b>. The received parameter is temporarily stored in the discriminator
        storage unit <b>23</b>. The multilayered neural network <b>40</b> is provided
        in the learning assistance device <b>20</b> in advance and the parameter received
        from each terminal device <b>10</b> is set as the weight of the coupling between
        the respective layers of the units of the multilayered neural network <b>40</b>
        provided in the learning assistance device <b>20</b>. By re-setting the parameter
        received from each terminal device in this weight, the same learned discriminator
        as each terminal device <b>10</b> can be acquired.<br />[0057] The correct
        answer data acquisition unit <b>24</b> causes each of the learned discriminators
        collected from each of the terminal devices <b>10</b> to discriminate the
        same input image data to acquire a plurality of discrimination results, and
        determines the correct answer data of the input image data from the plurality
        of discrimination results.<br />[0058] A large number of pieces of image data
        are often stored in a database without correct answer data attached thereto.
        In order to attach the correct answer data to the image data, for example,
        the image data is input to the discriminator so as to acquire a discrimination
        result, and a user such as an image interpretation doctor performs a determination
        that the answer is a correct answer or an incorrect answer with respect to
        the discrimination result, and registers the discrimination result as the
        image correct answer data in association with the correct answer data and
        the input image data in the case of the correct answer. In case of the incorrect
        answer, the user creates a mask image of the correct answer data and registers
        the mask image as image correct answer data in association with input image
        data. Work of creating the correct answer data in this way is laborious and
        it is difficult to manually generate a large number of pieces of correct answer
        data.<br />[0059] Therefore, the correct answer data acquisition unit <b>24</b>
        determines the largest number of same discrimination results as correct answer
        data of the input image data from the discrimination results obtained by inputting
        the same input image data to the learned discriminators collected from the
        respective terminal devices <b>10</b>. Thus, in a case where the correct answer
        data is determined from a plurality of discrimination results obtained by
        using the learned discriminators collected from the respective terminal devices
        <b>10</b>, accurate correct answer data can be automatically generated, and
        therefore, it is possible to easily obtain a large number of pieces of correct
        answer data. The obtained correct answer data is accumulated in the storage
        (the correct answer data storage unit <b>25</b>) as image correct answer data
        in association with the input image data.<br />[0060] The learning unit <b>26</b>
        is provided with a deep learning discriminator configured of a multilayered
        neural network <b>40</b>. The image correct answer data accumulated in the
        correct answer data storage unit <b>25</b> is sequentially input to the deep
        learning discriminator, so that learning is performed.<br />[0061] In a stage
        the learning had progressed to a certain extent and accuracy of the discrimination
        of the deep learning discriminator has improved, the discriminator output
        unit <b>27</b> generates an image processing program (actually operated discriminator)
        incorporating the deep learning discriminator learned by the learning unit
        <b>26</b> and a learning program (the learning discriminator), and distributes
        the programs to each terminal device <b>10</b> over the network <b>30</b>.
        Since software intended for medical purposes is a target of the Pharmaceutical
        and Medical Device Act (the revised Pharmaceutical Affairs Act), the software
        is required to meet a criterion prescribed in the Pharmaceutical and Medical
        Device Act. Therefore, it is preferable to confirm that a deep learning discriminator
        before distribution exceeds an evaluation standard using an evaluation image
        set formed through a combination of a plurality of images in which the criterion
        prescribed in the Pharmaceutical and Medical Device Act can be evaluated,
        and then, distribute deep learning discriminator.<br />[0062] Even after the
        discriminator output unit <b>27</b> distributes the image processing program
        and the learning program to the respective terminal devices <b>10</b>, the
        learning unit <b>26</b> sequentially inputs the image correct answer data
        accumulated in the correct answer data storage unit <b>25</b> to the deep
        learning discriminator as it is and causes the deep learning discriminator
        to perform learning. That is, the learning unit <b>26</b> causes the learning
        discriminator output by the discriminator output unit <b>27</b> to perform
        additional learning, and the discriminator output unit <b>27</b> outputs the
        additionally learned learning discriminator as a new learning discriminator.<br
        />[0063] Next, a flow of a deep learning process of the embodiment will be
        described with reference to a transition diagram of <figref idref=\\\"DRAWINGS\\\">FIG.
        5</figref> and a flowchart of <figref idref=\\\"DRAWINGS\\\">FIG. 6</figref>.<br
        />[0064] First, in the learning assistance device <b>20</b>, the discriminator
        output unit <b>27</b> distributes the actually operated discriminator NNo
        and the learning discriminator NNt to the plurality of terminal devices <b>10</b>,
        and to the medical institution A, . . ., the medical institution X over the
        network <b>30</b> (S<b>1</b>).<br />[0065] In the terminal device <b>10</b>,
        the discriminator acquisition unit <b>12</b> acquires the actually operated
        discriminator NNo and the learning discriminator NNt (S<b>2</b>). The actually
        operated discriminator NNo is used for diagnosis by an image interpretation
        doctor, and the discrimination result acquisition unit <b>13</b> discriminates
        image data (input) that is a diagnosis target and obtains a discrimination
        result (output) (see <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>). Further,
        in the medical institution A, the learning unit <b>14</b> of the terminal
        device <b>10</b> causes the learning discriminator NNt to perform learning
        using the image correct answer data T stored in the image database <b>53</b>
        (S<b>3</b>) and generates the learned discrimination unit NNt-A (S<b>4</b>).
        Similarly, in the medical institution X, the learning unit <b>14</b> of the
        terminal device <b>10</b> causes the learning discriminator NNt to perform
        learning using the image correct answer data T and generates the learned discriminator
        NNt-X (S<b>4</b>).<br />[0066] Periodically, in each terminal device <b>10</b>,
        the learned discriminator output unit <b>15</b> transmits the learned discriminator
        to the learning assistance device <b>20</b> (S<b>5</b>). The parameter of
        the learned discriminator NNt-A is transmitted from the terminal device <b>10</b>
        of the medical institution A to the learning assistance device <b>20</b>,
        and the parameter of the learned discriminator NNt-X is transmitted from the
        terminal device <b>10</b> of the medical institution X to the learning assistance
        device <b>20</b> (see a solid arrow (1) in <figref idref=\\\"DRAWINGS\\\">FIG.
        5</figref>).<br />[0067] In the learning assistance device <b>20</b>, the
        learned discriminator acquisition unit <b>22</b> temporarily stores the parameters
        of the learned discriminators received from the plurality of terminal devices
        <b>10</b> in the discriminator storage unit <b>23</b>. By setting this parameter
        in the multilayered neural network <b>40</b> provided in the learning assistance
        device <b>20</b>, the learned discriminator learned by each terminal device
        <b>10</b> is acquired (S<b>6</b>).<br />[0068] The correct answer data acquisition
        unit <b>24</b> inputs the input image data P to the learned discriminator
        of each terminal device <b>10</b> to obtain the discrimination result. In
        the example of <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>, a result a,
        a result b, a result c, . . ., a result g are obtained, and a largest number
        of results b are determined to be correct answer data of the input image data
        P (S<b>7</b>). The input image data P and the correct answer data are accumulated
        in the storage <b>25</b> in association with each other.<br />[0069] The learning
        unit <b>26</b> causes the deep learning discriminator NNl to perform learning
        using the input image data P and the result b (correct answer data) (S<b>8</b>).
        Periodically, a new version of the actually operated discriminator NNo and
        the learning discriminator NNt are generated on the basis of the deep learning
        discriminator NNl (S<b>9</b>). The discriminator output unit <b>27</b> distributes
        the new version of the actually operated discriminator NNo and learning discriminator
        NNt to each of the terminal devices <b>10</b> (S<b>10</b>; see an arrow (2)
        of a broken line in <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>).<br />[0070]
        In the terminal device <b>10</b> of the medical institution A, the discriminator
        acquisition unit <b>12</b> acquires the new version of the actually operated
        discriminator NNo and learning discriminator NNt again (S<b>2</b>). The learning
        unit <b>14</b> of the terminal device <b>10</b> causes the new version of
        learning discriminator NNt to perform learning using the image correct answer
        data T stored in the image database <b>53</b> (S<b>3</b>) and generate the
        learned discriminator NNt-A again (S<b>4</b>).<br />[0071] In the terminal
        device <b>10</b> of the medical institution X, the discriminator acquisition
        unit <b>12</b> acquires the new version of the actually operated discriminator
        NNo and learning discriminator NNt again (S<b>2</b>). The learning unit <b>14</b>
        of the terminal device <b>10</b> causes the new version of learning discriminator
        NNt to perform learning using the image correct answer data T stored in the
        image database <b>53</b> (S<b>3</b>) and generates a learned discriminator
        NNt-X (S<b>4</b>). Consequently, the process from S<b>5</b> to S<b>10</b>
        is performed, as in the same manner as described above.<br />[0072] The processes
        of S<b>2</b> to S<b>10</b> are repeated, and the learning assistance device
        <b>20</b> generates an actually operated discriminator and a learning discriminator
        of which the performance has been improved while generating the correct answer
        data, and distributes the actually operated discriminator and the learning
        discriminator to the terminal device <b>10</b>.<br />[0073] As described above,
        by the terminal device <b>10</b> placed in each medical institution performing
        learning using the image data stored in the medical institution, a discriminator
        improving discrimination performance is generated in each medical institution.
        By the learning assistance device <b>20</b> generating the correct answer
        data using the discriminators of which the performance have been improved
        in each medical institution, a large amount of accurate correct answer data
        can be generated, and deep Learning can be performed using this correct answer
        data.<br />[0074] Although the case where the mask image of the correct answer
        data for the image data has the information indicating what the area on the
        image is has been described above, the discriminator may be configured to
        (1) determine an organ area and an organ name by determining what organ each
        pixel position of the image data is, (2) determine a lesion area and a type
        of lesion by determining a type of lesion of each pixel in units of pixels
        of the image data. Alternatively, correct answer data for one image may be
        specified as a disease name or an image diagnostic name, and (3) the disease
        name may be specified from the image data.<br />[0075] Next, a second embodiment
        will be described. The second embodiment is different from the first embodiment
        in the method of determining the correct answer data. Since a schematic configuration
        of the learning assistance system <b>1</b> is the same as that of the first
        embodiment, detailed description thereof will be omitted. <figref idref=\\\"DRAWINGS\\\">FIG.
        7</figref> is a block diagram illustrating a schematic configuration of a
        terminal device <b>10</b> and a learning assistance device <b>20</b> according
        to the second embodiment. The same configurations as those of the first embodiment
        are denoted by the same reference numerals as those of the first embodiment,
        detailed description thereof will be omitted, and only different configurations
        will be described.<br />[0076] As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        7</figref>, the terminal device <b>10</b> includes a discriminator acquisition
        unit <b>12</b>, a discrimination result acquisition unit <b>13</b>, a learning
        unit <b>14</b>, and a learned discriminator output unit <b>15</b>. The learning
        assistance device <b>20</b> includes a learned discriminator acquisition unit
        <b>22</b>, a discriminator storage unit <b>23</b>, a correct answer data acquisition
        unit <b>24</b><i>a, </i>a correct answer data storage unit <b>25</b>, a learning
        unit <b>26</b>, and a discriminator output unit <b>27</b>. A configuration
        of the terminal device <b>10</b> is the same as that of the first embodiment
        except that the correct answer data acquisition unit <b>24</b><i>a </i>of
        the learning assistance device <b>20</b> includes an evaluation unit <b>28</b>
        and an evaluation image storage unit <b>29</b>.<br />[0077] In the case where
        the correct answer data is determined by majority vote, sufficient learning
        may be performed in each terminal device <b>10</b> as in the first embodiment,
        but for example, in a case where a discriminator not additionally learned
        in the terminal device <b>10</b> is received as the learned discriminator
        or in a case where a learned discriminator with a small number of additional
        learnings is used, a determination result is likely to be the same for the
        same input image data P, and a determination result of the discriminator in
        which such learning is not sufficiently performed is highly likely to be the
        correct answer data.<br />[0078] Therefore, the learning assistance device
        <b>20</b> evaluates the learned discriminators collected from the respective
        terminal device <b>10</b> in advance. The learning assistance device <b>20</b>
        sets a plurality of cases of disease covering a representative case pattern
        and also having correct answer data for image data as an evaluation image
        set SET, and the evaluation unit <b>28</b> evaluates the learned discriminators
        sent from the respective terminal devices <b>10</b> using the image set SET,
        and determines the weight of the discriminator according to a height of the
        correct answer rate. Further, since software intended for medical purposes
        is a target of the Pharmaceutical and Medical Device Act (the revised Pharmaceutical
        Affairs Act), the software is required to meet a criterion prescribed in the
        Pharmaceutical and Medical Device Act. Therefore, it is preferable for an
        evaluation image set SET formed through a combination of a plurality of images
        in which the criterion prescribed in the Pharmaceutical and Medical Device
        Act can be evaluated to be stored in the storage (the evaluation image storage
        unit <b>29</b>) in advance. However, this evaluation image set SET is not
        sufficient for use in deep learning.<br />[0079] The weights are determined
        for the respective learned discriminators collected from the terminal device
        <b>10</b> according to the correct answer rate of the evaluation image set,
        the weights of the learned discriminators having the same result among the
        discrimination results are added, and the discrimination result having the
        largest added weight is set as correct answer data of the input image.<br
        />[0080] Since a flow of the deep learning process is the same as that of
        the first embodiment, the flow will be omitted.<br />[0081] Next, a third
        embodiment will be described. The third embodiment is different from the first
        and second embodiments in the method of determining the correct answer data.
        Since a schematic configuration of the learning assistance system <b>1</b>
        is the same as that of the first embodiment, detailed description thereof
        will be omitted. <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref> is a block
        diagram illustrating a schematic configuration of a terminal device <b>10</b>
        and a learning assistance device <b>20</b> according to the third embodiment.
        The same configurations as those of the first embodiment are denoted by the
        same reference numerals as those of the first embodiment, detailed description
        thereof will be omitted, and only different configurations will be described.<br
        />[0082] As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref>,
        the terminal device <b>10</b> includes a discriminator acquisition unit <b>12</b>,
        a discrimination result acquisition unit <b>13</b>, a learning unit <b>14</b>,
        and a learned discriminator output unit <b>15</b><i>a. </i>The learning assistance
        device <b>20</b> includes a learned discriminator acquisition unit <b>22</b><i>a,
        </i>a discriminator storage unit <b>23</b>, a correct answer data acquisition
        unit <b>24</b><i>b, </i>a correct answer data storage unit <b>25</b>, a learning
        unit <b>26</b>, and a discriminator output unit <b>27</b>. The learned discriminator
        output unit <b>15</b><i>a </i>of the terminal device <b>10</b>, and the learned
        discriminator acquisition unit <b>22</b><i>a </i>and the correct answer data
        acquisition unit <b>24</b><i>b </i>of the learning assistance device <b>20</b>
        are different from those of the first embodiment.<br />[0083] As in the second
        embodiment, even in a case where the performance of the learned discriminator
        is evaluated, evaluation results using the evaluation image set may be insufficient
        in a case where there is a problem with the number of cases of disease for
        evaluation of the learning assistance device <b>20</b> or coverage of the
        cases of disease. However, in a case where the learned discrimination learns
        a certain number of pieces of image correct answer data, it can be presumed
        that the performance is improving as appropriate.<br />[0084] Therefore, in
        a case where the learned discriminator output unit <b>15</b><i>a </i>of the
        terminal device <b>10</b> transmits the parameter of the learned discriminator,
        the learned discriminator output unit <b>15</b><i>a </i>of the terminal device
        <b>10</b> transmits the number of pieces of image correct answer data learned
        by the learned discriminator to the learning assistance device <b>20</b>.<br
        />[0085] The learned discriminator acquisition unit <b>22</b><i>a </i>of the
        learning assistance device <b>20</b> receives the number of pieces of image
        correct answer data learned by the learned discriminator at each terminal
        device <b>10</b> together in a case where the learned discriminator acquisition
        unit <b>22</b><i>a </i>of the learning assistance device <b>20</b> receives
        the parameter. The weight is determined so that the weight increases as the
        number of pieces of image correct answer data learned by the correct answer
        data acquisition unit <b>24</b><i>b </i>and the learned discriminators of
        each terminal device <b>10</b> increases. The weights of the learned discriminators
        having the same discrimination result are added, and the discrimination result
        with the largest added weight is set as the correct answer data of the input
        image.<br />[0086] Since a flow of deep learning process is the same as that
        of the first embodiment, description thereof is omitted.<br />[0087] Next,
        a fourth embodiment will be described. The fourth embodiment is different
        from the first, second, and third embodiments in the method of determining
        the correct answer data. Since a schematic configuration of the learning assistance
        system <b>1</b> is the same as that of the first embodiment, detailed description
        thereof will be omitted. <figref idref=\\\"DRAWINGS\\\">FIG. 9</figref> is
        a block diagram illustrating a schematic configuration of a terminal device
        <b>10</b> and a learning assistance device <b>20</b> according to the fourth
        embodiment. The same configurations as those of the first embodiment are denoted
        by the same reference numerals as those of the first embodiment, detailed
        description thereof will be omitted, and only different configurations will
        be described.<br />[0088] As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        9</figref>, the terminal device <b>10</b> includes a discriminator acquisition
        unit <b>12</b>, a discrimination result acquisition unit <b>13</b>, a learning
        unit <b>14</b>, and a learned discriminator output unit <b>15</b><i>b. </i>The
        learning assistance device <b>20</b> includes a learned discriminator acquisition
        unit <b>22</b><i>b, </i>a discriminator storage unit <b>23</b>, a correct
        answer data acquisition unit <b>24</b><i>c, </i>a correct answer data storage
        unit <b>25</b>, a learning unit <b>26</b>, and a discriminator output unit
        <b>27</b>. The learned discriminator output unit <b>15</b><i>b </i>of the
        terminal device <b>10</b> and the learned discriminator acquisition unit <b>22</b><i>b
        </i>and the correct answer data acquisition unit <b>24</b><i>c </i>of the
        learning assistance device <b>20</b> are different from those of the first
        embodiment.<br />[0089] The number of pieces of image correct answer data
        for each type of cases of disease is biased due to the characteristics of
        the medical facility, the regional nature, or the like, and the number of
        pieces of image correct answer data is small in any kind of diseases even
        though the number of all pieces of image correct answer data is large, the
        performance is likely not to be improved in the disease. Therefore, the number
        of pieces of image correct answer data learned by the learned discriminator
        of each medical facility is received from each terminal device <b>10</b> for
        each type of cases of disease.<br />[0090] Therefore, in a case where the
        learned discriminator output unit <b>15</b><i>b </i>of the terminal device
        <b>10</b> transmits the parameter of the learned discriminator, the learned
        discriminator output unit <b>15</b><i>b </i>of the terminal device <b>10</b>
        transmits the number of pieces of image correct answer data for each type
        of cases of disease learned by the learned discriminator to the learning assistance
        device <b>20</b>. Specifically, it is determined which case of disease the
        image correct answer data learned by the learning unit <b>14</b> of the terminal
        device <b>10</b> relates to, for example, on the basis of a DICOM tag attached
        to the image of image correct answer data, and the number of learned image
        correct answer data is changed for each type of cases of disease.<br />[0091]
        The type of cases of disease is classified by disease name (which may be an
        image diagnostic name in case of image inspection) or a type of disease name.
        In a case where a plurality of organs are collectively processed by one discriminator,
        an organ name may be used.<br />[0092] The learned discriminator acquisition
        unit <b>22</b><i>b </i>of the learning assistance device <b>20</b> receives
        the number of pieces of image correct answer data for each type of cases of
        disease learned by the learned discriminator at each terminal device <b>10</b>
        in a case where the learned discriminator acquisition unit <b>22</b><i>b </i>of
        the learning assistance device <b>20</b> receives the parameter.<br />[0093]
        In the correct answer data acquisition unit <b>24</b><i>c, </i>it is estimated
        that the performance of the learned discriminator is higher as the number
        of learned image correct answer data is larger. To reflect this, the number
        of pieces of image correct answer data learned by each facility for each type
        of cases of disease is counted for the learned discriminator of each terminal
        device <b>10</b>, and, the weight for each type of cases of disease is determined
        for each learned discriminator so that weight is increased as the number increases.
        In addition, weights of the learned discriminators having the same discrimination
        result are added in correspondence to the type of cases of disease of the
        input image, and the discrimination result having the largest added weight
        is set as the correct answer data of the input image.<br />[0094] Since a
        flow of deep learning process is the same as that of the first embodiment,
        description thereof will be omitted.<br />[0095] With a scheme according to
        the embodiment, it is possible to evaluate the learned discriminator in consideration
        of, the number of pieces of image correct answer data of each medical facility,
        the type of cases of disease, and the like.<br />[0096] Further, the evaluation
        image set according to the second embodiment may be set as an evaluation image
        set capable of evaluating a discriminator for each type of disease, and the
        weight of the learned discriminator of each terminal device may be determined
        according to the correct answer rate for each type of disease. The weights
        of the learned discriminators having the same discrimination result may be
        added according to the type of cases of disease of the input image, and the
        discrimination result having the largest added weight may be set as the correct
        answer data of the input image.<br />[0097] Further, although the weight is
        automatically determined in the fourth embodiment, the weight may be determined
        manually and stored in the learning assistance device <b>20</b> in advance
        in consideration of importance of the facility, a confident disease of each
        facility, or the like.<br />[0098] Although the embodiment in which the learning
        assistance device <b>20</b> and the terminal device <b>10</b> are connected
        via the network has been described in the above description, an image processing
        program incorporating a discriminator functioning as an actually operated
        discriminator and a learning program incorporating a discriminator functioning
        as a learning discriminator may be stored in a recording medium such as a
        DVD-ROM and distributed to each medical institution, instead of over the network.<br
        />[0099] In this embodiment, the discriminator acquisition unit <b>12</b>
        of the terminal device <b>10</b> reads the image processing program and the
        learning program from the DVD-ROM to the terminal device <b>10</b> and installs
        the image processing program and the learning program, and reads the identification
        information ID of the image correct answer data used for learning of the learning
        discriminator from the recording medium. Further, the learned discriminator
        output unit <b>15</b> of the terminal device <b>10</b> records the parameter
        of the learned discriminator in the DVD-ROM, and distributes the parameter
        to an operator of the learning assistance device <b>20</b> by mailing or the
        like.<br />[0100] Further, the learned discriminator acquisition unit <b>22</b>
        of the learning assistance device <b>20</b> reads the parameters of the learned
        discriminator recorded on the DVD-ROM. Furthermore, the discriminator output
        unit <b>27</b> of the learning assistance device <b>20</b> records the image
        processing program and the learning program on a DVD-ROM and sends the image
        processing program and the learning program to an operator of the terminal
        device <b>10</b> by mailing or the like.<br />[0101] As described in detail
        above, in the present invention, accurate correct answer data of an image
        is automatically generated using a discriminator of which the performance
        has been improved using the medical images stored in each medical institution.
        Thus, it is possible to use a large amount of medical images for deep learning.<br
        />[0102] Although the case where the learning assistance device and the terminal
        device function on a general-purpose computer has been described above, a
        dedicated circuit such as an application specific integrated circuit (ASIC)
        or field programmable gate arrays (FPGA) that permanently stores a program
        for executing some of functions may be provided. Alternatively, a program
        instruction stored in a dedicated circuit and a program instruction executed
        by a general-purpose CPU programmed to use a program of a dedicated circuit
        may be combined. As described above, the program instructions may be executed
        through any combination of hardware configurations of the computer.\",\"claimsHtml\":\"<b>1</b>.
        A learning assistance device comprising: a learned discriminator acquisition
        unit that acquires a plurality of learned discriminators obtained by causing
        each of learning discriminators provided in a plurality of respective terminal
        devices to perform learning using an image and correct answer data thereof;
        and a discriminator output unit that acquires a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, determines correct answer data of the
        input image on the basis of the plurality of discrimination results, and outputs
        a new learning discriminator obtained by causing the learning discriminator
        to perform learning again using the input image and the determined correct
        answer data, wherein the learning assistance device repeatedly performs a
        process in which the learning discriminator acquisition unit acquires the
        plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, and the discriminator output unit determines a new correct answer
        data of a new same input image different from the input image on the basis
        of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired by the learned discriminator acquisition
        unit to discriminate the new same input image, and outputs a new learning
        discriminator obtained by causing the learning discriminator output by the
        discriminator output unit to perform learning again using the new input image
        and the determined new correct answer data.  <br /> <b>2</b>. The learning
        assistance device according to claim 1, wherein the discriminator output unit
        outputs an actually operated discriminator learning the image and the correct
        answer data thereof used for learning by the new learning discriminator. <br
        /> <b>3</b>. The learning assistance device according to claim 1, wherein
        the learned discriminator acquisition unit acquires the learned discriminator
        from the plurality of terminal devices over a network, and the discriminator
        output unit outputs the new learning discriminator to the plurality of terminal
        devices over the network.  <br /> <b>4</b>. The learning assistance device
        according to claim 1, wherein the discriminator output unit determines a discrimination
        result having the largest number of same results among the plurality of discrimination
        results, as correct answer data of the input image. <br /> <b>5</b>. The learning
        assistance device according to claim 1, wherein the discriminator output unit
        determines a weight of each of the plurality of learned discriminators according
        to the terminal device learned by the learned discriminator, adds the weights
        of the learned discriminators having the same result among the discrimination
        results, and sets a discrimination result having the largest added weight
        as correct answer data of the input image. <br /> <b>6</b>. The learning assistance
        device according to claim 5, wherein the discriminator output unit determines
        the weight of each of the learned discriminators learned at each of the terminal
        devices according to the number of pieces of correct answer data learned by
        the learned discriminator at each terminal device, adds the weights of the
        learned discriminators having the same discrimination result, and sets a discrimination
        result having the largest added weight as correct answer data of the input
        image. <br /> <b>7</b>. The learning assistance device according to claim
        5, wherein the discriminator output unit determines weights for types of cases
        of disease of the image learned by the respective learned discriminators with
        respect to the respective learned discriminators, adds the weights corresponding
        to the types of cases of disease of the input image in the learned discriminator
        having the same discrimination result, and sets a discrimination result having
        the largest added weight as correct answer data of the input image. <br />
        <b>8</b>. The learning assistance device according to claim 5, further comprising:
        an evaluation unit that evaluates a correct answer rate using an image set
        including a plurality of images serving as a reference with respect to the
        plurality of learned discriminators, wherein the discriminator output unit
        determines the weight of each of the plurality of learned discriminators according
        to the correct answer rate obtained by the evaluation unit, adds the weights
        of the respective learned discriminators having the same discrimination results,
        and sets the discrimination result having the largest added weight as correct
        answer data of the input image.  <br /> <b>9</b>. A method of operating a
        learning assistance device including a learned discriminator acquisition unit
        and a discriminator output unit, the method comprising: acquiring a plurality
        of learned discriminators obtained by causing each of learning discriminators
        provided in a plurality of respective terminal devices to perform learning
        using an image and correct answer data thereof by the learned discriminator
        acquisition unit; acquiring a plurality of discrimination results obtained
        by causing each of the plurality of learned discriminators to discriminate
        the same input image, determines correct answer data of the input image on
        the basis of the plurality of discrimination results, and outputs a new learning
        discriminator obtained by causing the learning discriminator to perform learning
        again using the input image and the determined correct answer data by the
        discriminator output unit; and repeatedly performing a process in which the
        learned discriminator acquisition unit acquires the plurality of learned discriminators
        obtained by causing the each of learning discriminators, the each of learning
        discriminators being output from the discriminator output unit and being provided
        for each of the plurality of terminal devices, to perform learning using an
        image and correct answer data thereof, and the discriminator output unit determines
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired by the learned discriminator
        acquisition unit to discriminate the new same input image, and outputs a new
        learning discriminator obtained by causing the learning discriminator output
        by the discriminator output unit to perform learning again using the new input
        image and the determined new correct answer data.  <br /> <b>10</b>. A non-transitory
        computer-readable recording medium storing therein a learning assistance program
        causing a computer to function as: a learned discriminator acquisition unit
        that acquires a plurality of learned discriminators obtained by causing each
        of learning discriminators provided in a plurality of respective terminal
        devices to perform learning using an image and correct answer data thereof;
        and a discriminator output unit that acquires a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, determines correct answer data of the
        input image on the basis of the plurality of discrimination results, and outputs
        a new learning discriminator obtained by causing the learning discriminator
        to perform learning again using the input image and the determined correct
        answer data, wherein the program causes a process in which the learned discriminator
        acquisition unit acquires the plurality of learned discriminators obtained
        by causing the each of learning discriminators, the each of learning discriminators
        being output from the discriminator output unit and being provided for each
        of the plurality of terminal devices, to perform learning using an image and
        correct answer data thereof, and the discriminator output unit determines
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired by the learned discriminator
        acquisition unit to discriminate the new same input image, and outputs a new
        learning discriminator obtained by causing the learning discriminator output
        by the discriminator output unit to perform learning again using the new input
        image and the determined new correct answer data, to be repeatedly performed.
        \ <br /> <b>11</b>. A learning assistance system in which a learning assistance
        device and a plurality of terminal devices are connected over a network, wherein
        the terminal device includes a learned discriminator output unit that outputs
        a learned discriminator obtained by causing a learning discriminator to perform
        learning using an image and correct answer data thereof over the network,
        the learning assistance device includes a learned discriminator acquisition
        unit that acquires a plurality of learned discriminators from the plurality
        of terminal devices over the network, and a discriminator output unit that
        acquires a plurality of discrimination results obtained by causing the plurality
        of learned discriminators to discriminate the same input image, determines
        correct answer data of the input image on the basis of the plurality of discrimination
        results, and outputs a new learning discriminator obtained by causing the
        learning discriminator to perform learning again using the input image and
        the determined correct answer data to the plurality of terminal devices over
        the network, and the terminal device includes a discriminator acquisition
        unit that receives the learning discriminator output from the learning assistance
        device over the network.  <br /> <b>12</b>. The learning assistance system
        according to claim 11, wherein the discriminator acquisition unit further
        acquires an actually operated discriminator learning the same image and correct
        answer data of the image as those of the learning discriminator output from
        the learning assistance device over a network, and the terminal device further
        includes a discrimination result acquisition unit that acquires a discrimination
        result of discriminating an image that is a discrimination target using the
        actually operated discriminator.  <br /> <b>13</b>. A terminal device comprising:
        a discriminator acquisition unit that acquires a learning discriminator, and
        a learned actually operated discriminator learned using the same image and
        correct answer data of the image as those of the learning discriminator; a
        discrimination result acquisition unit that acquires a discrimination result
        of discriminating an image that is a discrimination target using the actually
        operated discriminator; and a learned discriminator output unit that outputs
        a learned discriminator obtained by causing the learning discriminator to
        perform learning using an image and correct answer data thereof.  <br /> <b>14</b>.
        The terminal device according to claim 13, wherein the discriminator acquisition
        unit acquires the learning discriminator and the actually operated discriminator
        from a learning assistance device over a network, and the learned discriminator
        output unit sends and outputs the learned discriminator over the network.\",\"briefHtml\":\"CROSS-REFERENCE
        TO RELATED APPLICATION<br />[0001] This application claims priority from Japanese
        Patent Application No. 2017-187096, filed on Sep. 27, 2017, the disclosure
        of which is incorporated by reference herein in its entirety.<br />BACKGROUND<br
        />Field of the Invention<br />[0002] The present invention relates to a learning
        assistance device that assists in generation of a discriminator using machine
        learning, a method of operating the learning assistance device, a learning
        assistance program, a learning assistance system, and a terminal device.<br
        />Related Art<br />[0003] In the related art, machine learning has been used
        to learn features of data and perform recognition or classification of images
        or the like. In recent years, various learning schemes have been developed,
        a processing time has been shortened due to an improved processing capability
        of a computer, and deep learning in which a system learns features of image
        data or the like at a deeper level can be performed. By performing the deep
        learning, features of images or the like can be recognized with very high
        accuracy, and improvement of performance of discrimination is expected.<br
        />[0004] In the medical field, artificial intelligence (AI) that recognizes
        features of images with high accuracy by performing learning using deep learning
        is desired. For deep learning, it is indispensable to perform learning using
        a large amount of high quality data according to purposes. Therefore, it is
        important to prepare learning data efficiently. Image data of a large number
        of cases of disease is accumulated in each medical institution with the spread
        of a picture archiving and communication system (PACS). Therefore, learning
        using image data of various cases of disease accumulated in each medical institution
        is considered.<br />[0005] Further, in recent years, a technical level of
        artificial intelligence has been improving in a variety of fields, and the
        artificial intelligence is incorporated into a variety of services and started
        to be used and utilized. In particular, services provided to various edge
        terminals over a network are increasing. For example, JP2008-046729A discloses
        a device in which a learning model is incorporated into a moving image topic
        division device that automatically divides a moving image at a switching point
        of a topic. JP2008-046729A discloses that the learning model has been distributed
        to a client terminal, the topic division is automatically executed using the
        distributed learning model at each client terminal, and content corrected
        by a user for a result of the automatic topic division is fed back for updating
        of the learning model. After the feedback corrected by the user is accumulated
        in an integration module over a network, a learning model reconstructed using
        the accumulated feedback is distributed to each client terminal over the network
        again.<br />[0006] However, in the medical field, since data to be learned
        is medical data of a patient, confidentiality is very high, and it is necessary
        to handle data carefully for use as learning data. Further, correct answer
        data is not attached to image data. Alternatively, even in a case where there
        is the correct answer data, the correct answer data is often managed without
        being associated with original image data. Therefore, it is difficult and
        costly to efficiently collect learning data to which the correct answer data
        is added.<br />SUMMARY<br />[0007] Therefore, in order to solve the above-described
        problems, an object of the present invention is to provide a learning assistance
        device which enables learning of a large amount and a variety of learning
        data necessary for deep learning in a medical field, a method of operating
        the learning assistance device, a learning assistance program, a learning
        assistance system, and a terminal device.<br />[0008] A learning assistance
        device of the present invention comprises: a learned discriminator acquisition
        unit that acquires a plurality of learned discriminators obtained by causing
        each of learning discriminators provided in a plurality of respective terminal
        devices to perform learning using an image and correct answer data thereof;
        and a discriminator output unit that acquires a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, determines correct answer data of the
        input image on the basis of the plurality of discrimination results, and outputs
        a new learning discriminator obtained by causing the learning discriminator
        to perform learning again using the input image and the determined correct
        answer data, wherein the learning assistance device repeatedly performs a
        process in which the learning discriminator acquisition unit acquires the
        plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, and the discriminator output unit determines a new correct answer
        data of a new same input image different from the input image on the basis
        of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired by the learned discriminator acquisition
        unit to discriminate the new same input image, and outputs a new learning
        discriminator obtained by causing the learning discriminator output by the
        discriminator output unit to perform learning again using the new input image
        and the determined new correct answer data.<br />[0009] A method of operating
        a learning assistance device of the present invention is a method of operating
        a learning assistance device including a learned discriminator acquisition
        unit and a discriminator output unit, the method comprising: acquiring a plurality
        of learned discriminators obtained by causing each of learning discriminators
        provided in a plurality of respective terminal devices to perform learning
        using an image and correct answer data thereof by the learned discriminator
        acquisition unit; acquiring a plurality of discrimination results obtained
        by causing each of the plurality of learned discriminators to discriminate
        the same input image, determines correct answer data of the input image on
        the basis of the plurality of discrimination results, and outputs a new learning
        discriminator obtained by causing the learning discriminator to perform learning
        again using the input image and the determined correct answer data by the
        discriminator output unit; and repeatedly performing a process in which the
        learned discriminator acquisition unit acquires the plurality of learned discriminators
        obtained by causing the each of learning discriminators, the each of learning
        discriminators being output from the discriminator output unit and being provided
        for each of the plurality of terminal devices, to perform learning using an
        image and correct answer data thereof, and the discriminator output unit determines
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired by the learned discriminator
        acquisition unit to discriminate the new same input image, and outputs a new
        learning discriminator obtained by causing the learning discriminator output
        by the discriminator output unit to perform learning again using the new input
        image and the determined new correct answer data.<br />[0010] A learning assistance
        program according to the present invention causes a computer to function as:
        a learned discriminator acquisition unit that acquires a plurality of learned
        discriminators obtained by causing each of learning discriminators provided
        in a plurality of respective terminal devices to perform learning using an
        image and correct answer data thereof; and a discriminator output unit that
        acquires a plurality of discrimination results obtained by causing each of
        the plurality of learned discriminators to discriminate the same input image,
        determines correct answer data of the input image on the basis of the plurality
        of discrimination results, and outputs a new learning discriminator obtained
        by causing the learning discriminator to perform learning again using the
        input image and the determined correct answer data, wherein the program causes
        a process in which the learned discriminator acquisition unit acquires the
        plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, and the discriminator output unit determines a new correct answer
        data of a new same input image different from the input image on the basis
        of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired by the learned discriminator acquisition
        unit to discriminate the new same input image, and outputs a new learning
        discriminator obtained by causing the learning discriminator output by the
        discriminator output unit to perform learning again using the new input image
        and the determined new correct answer data, to be repeatedly performed.<br
        />[0011] \u201CAcquire a learned discriminator\u201D may be acquiring the
        learned discriminator or may be receiving a parameter of the discriminator
        and setting the parameter in a prepared discriminator to acquire the learned
        discriminator. For example, in a case where the discriminator is a multilayered
        neural network, a program in which a learned multilayered neural network is
        incorporated may be acquired or a weight of coupling between layers of units
        of the multilayered neural network may be acquired as a parameter and the
        parameter may be set in the prepared multilayered neural network, so that
        the learned multilayered neural network can be acquired.<br />[0012] Further,
        the discriminator output unit further outputs an actually operated discriminator
        learning the image and the correct answer data thereof used for learning by
        the new learning discriminator.<br />[0013] The \u201Cactually operated discriminator\u201D
        is a discriminator capable of acquiring a discrimination result of input image
        data, which cannot perform additional learning, and the \u201Clearning discriminator\u201D
        is a discriminator that can perform additional learning using an image and
        correct answer data of the image. Further, the actually operated discriminator
        to be output is a discriminator caused to perform learning using an image
        and correct answer data of the image that are all the same as the image and
        the correct answer data of the image learned by the output learning discriminator.<br
        />[0014] Further, the learned discriminator acquisition unit may acquire the
        learned discriminator from the plurality of terminal devices over a network,
        and the discriminator output unit may output the new learning discriminator
        to the plurality of terminal devices over the network.<br />[0015] Further,
        the discriminator output unit may determine a discrimination result having
        the largest number of same results among the plurality of discrimination results,
        as correct answer data of the input image.<br />[0016] Further, the discriminator
        output unit may determine a weight of each of the plurality of learned discriminators
        according to the terminal device learned by the learned discriminator, adds
        the weights of the learned discriminators having the same result among the
        discrimination results, and sets a discrimination result having the largest
        added weight as correct answer data of the input image.<br />[0017] Further,
        the discriminator output unit may determine the weight of each of the learned
        discriminators learned at each of the terminal devices according to the number
        of pieces of correct answer data learned by the learned discriminator at each
        terminal device, add the weights of the learned discriminators having the
        same discrimination result, and set a discrimination result having the largest
        added weight as correct answer data of the input image.<br />[0018] Further,
        the discriminator output unit may determine weights for types of cases of
        disease of the image learned by the respective learned discriminators with
        respect to the respective learned discriminators, add the weights corresponding
        to the types of cases of disease of the input image in the learned discriminator
        having the same discrimination result, and set a discrimination result having
        the largest added weight as correct answer data of the input image.<br />[0019]
        Further, the learning assistance device may further comprise an evaluation
        unit that evaluates a correct answer rate using an image set including a plurality
        of images serving as a reference with respect to the plurality of learned
        discriminators, wherein the discriminator output unit may determine the weight
        of each of the plurality of learned discriminators according to the correct
        answer rate obtained by the evaluation unit, add the weights of the respective
        learned discriminators having the same discrimination results, and set the
        discrimination result having the largest added weight as correct answer data
        of the input image.<br />[0020] A learning assistance system according to
        the present invention is a learning assistance system in which a learning
        assistance device and a plurality of terminal devices are connected over a
        network, wherein the terminal device includes a learned discriminator output
        unit that outputs a learned discriminator obtained by causing a learning discriminator
        to perform learning using an image and correct answer data thereof over the
        network, the learning assistance device includes a learned discriminator acquisition
        unit that acquires a plurality of learned discriminators from the plurality
        of terminal devices over the network, and a discriminator output unit that
        acquires a plurality of discrimination results obtained by causing the plurality
        of learned discriminators to discriminate the same input image, determines
        correct answer data of the input image on the basis of the plurality of discrimination
        results, and outputs a new learning discriminator obtained by causing the
        learning discriminator to perform learning again using the input image and
        the determined correct answer data to the plurality of terminal devices over
        the network, and the terminal device includes a discriminator acquisition
        unit that receives the learning discriminator output from the learning assistance
        device over the network.<br />[0021] Further, in the learning assistance system,
        the discriminator acquisition unit may further acquire an actually operated
        discriminator learning the same image and correct answer data of the image
        as those of the learning discriminator output from the learning assistance
        device over a network, and the terminal device may further include a discrimination
        result acquisition unit that acquires a discrimination result of discriminating
        an image that is a discrimination target using the actually operated discriminator.<br
        />[0022] A terminal device according to the present invention comprises a
        discriminator acquisition unit that acquires a learning discriminator, and
        a learned actually operated discriminator learned using the same image and
        correct answer data of the image as those of the learning discriminator; a
        discrimination result acquisition unit that acquires a discrimination result
        of discriminating an image that is a discrimination target using the actually
        operated discriminator; and a learned discriminator output unit that outputs
        a learned discriminator obtained by causing the learning discriminator to
        perform learning using an image and correct answer data thereof.<br />[0023]
        Further, in the terminal device, the discriminator acquisition unit may acquire
        the learning discriminator and the actually operated discriminator from a
        learning assistance device over a network, and the learned discriminator output
        unit may send and output the learned discriminator over the network.<br />[0024]
        Another learning assistance device of the present invention comprises a memory
        that stores instructions to be executed by a computer, and a processor configured
        to execute the stored instructions, wherein the processor executes an acquisition
        process of acquiring a plurality of learned discriminators obtained by causing
        each of learning discriminators provided in a plurality of respective terminal
        devices to perform learning using an image and correct answer data thereof,
        and an output process of acquiring a plurality of discrimination results obtained
        by causing each of the plurality of learned discriminators to discriminate
        the same input image, determining correct answer data of the input image on
        the basis of the plurality of discrimination results, and outputting a new
        learning discriminator obtained by causing the learning discriminator to perform
        learning again using the input image and the determined correct answer data,
        and the learning assistance device repeatedly performs a process of acquiring
        the plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, determining a new correct answer data of a new same input image different
        from the input image on the basis of a plurality of discrimination results
        obtained by causing the plurality of learned discriminators acquired by the
        learned discriminator acquisition unit to discriminate the new same input
        image, and outputting a new learning discriminator obtained by causing the
        learning discriminator output by the discriminator output unit to perform
        learning again using the new input image and the determined new correct answer
        data.<br />[0025] According to the present invention, the learning assistance
        device acquires the plurality of learned discriminators obtained by causing
        learning discriminators provided in a plurality of respective terminal devices
        to perform learning using image correct answer data, acquires the plurality
        of discrimination results of discriminating the same input image, determines
        the correct answer data of the input image on the basis of the plurality of
        discrimination results, and causes the discriminator to perform learning using
        the input image and the determined correct answer data. Therefore, it is possible
        to automatically generate the data for learning from the image to which the
        correct answer data is not attached, perform deep learning using a large amount
        of images, and improve performance of discrimination.\",\"backgroundTextHtml\":null,\"subHeadingM0Html\":null,\"subHeadingM1Html\":null,\"subHeadingM2Html\":null,\"subHeadingM3Html\":null,\"subHeadingM4Html\":null,\"subHeadingM5Html\":null,\"subHeadingM6Html\":null,\"usClassIssued\":null,\"issuedUsDigestRefClassifi\":null,\"datePublYear\":\"2019\",\"applicationYear\":\"2018\",\"pfDerwentWeekYear\":null,\"pfApplicationYear\":null,\"pfPublYear\":null,\"reissueApplNumber\":null,\"abstractHeader\":null,\"abstractedPublicationDerwent\":null,\"affidavit130BFlag\":null,\"affidavit130BText\":null,\"applicantGroup\":[\"FUJIFILM
        Corporation Tokyo JP\"],\"applicantHeader\":null,\"applicationFilingDateInt\":20180906,\"applicationFilingDateIntKwicHits\":null,\"applicationRefFilingType\":null,\"applicationReferenceGroup\":null,\"applicationSeriesAndNumber\":\"16123456\",\"applicationSeriesCode\":\"16\",\"assignee1\":\"[]\",\"assigneeDescriptiveText\":null,\"patentAssigneeTerms\":null,\"associateAttorneyName\":null,\"attorneyName\":null,\"biologicalDepositInformation\":null,\"applicationType\":null,\"unlinkedDerwentRegistryNumber\":null,\"unlinkedRingIndexNumbersRarerFragments\":null,\"claimStatement\":\"What
        is claimed is:\",\"claimsTextAmended\":null,\"continuedProsecutionAppl\":null,\"cpcAdditionalLong\":null,\"cpcCisClassificationOrig\":null,\"cpcCombinationClassificationOrig\":null,\"cpcInventive\":[\"G06F18/2178
        20230101\",\"G06N3/042 20230101\",\"G06V10/774 20220101\",\"G06N3/045 20230101\",\"G06V10/7784
        20220101\",\"G16H50/70 20180101\",\"G06N3/08 20130101\",\"G16H30/20 20180101\",\"G06F18/214
        20230101\",\"G06F18/217 20230101\",\"G16H30/40 20180101\",\"G06N20/00 20190101\"],\"cpcInventiveCurrentDateKwicHits\":null,\"cpcAdditional\":null,\"cpcAdditionalCurrentDateKwicHits\":null,\"cpcOrigClassificationGroup\":[\"G
        G06K G06K9/6262 20130101 F I 20190328 US\",\"G G06N G06N99/005 20130101 L
        I 20190328 US\",\"G G16H G16H30/20 20180101 L I 20190328 US\"],\"curIntlPatentClassificationGroup\":null,\"curUsClassificationUsPrimaryClass\":\"1\",\"curUsClassificationUsSecondaryClass\":null,\"customerNumber\":null,\"depositAccessionNumber\":null,\"depositDescription\":null,\"derwentClassAlpha\":null,\"designatedstatesRouteGroup\":null,\"docAccessionNumber\":null,\"drawingDescription\":null,\"editionField\":null,\"exchangeWeek\":null,\"exemplaryClaimNumber\":null,\"familyIdentifierOrig\":null,\"fieldOfSearchCpcClassification\":null,\"fieldOfSearchCpcMainClass\":null,\"fieldOfSearchIpcMainClass\":null,\"fieldOfSearchIpcMainClassSubclass\":null,\"fieldOfSearchSubclasses\":null,\"foreignRefGroup\":null,\"foreignRefPubDate\":null,\"foreignRefPubDateKwicHits\":null,\"foreignRefCitationClassification\":null,\"foreignRefPatentNumber\":null,\"foreignRefCitationCpc\":null,\"foreignRefCountryCode\":null,\"iceXmlIndicator\":\"Y\",\"internationalClassificationHeader\":null,\"internationalClassificationInformationalGroup\":null,\"intlPubClassificationGroup\":[\"20060101
        A G06K G06K9/62 F I B US H 20190328\",\"20060101 A G16H G16H30/20 L I B US
        H 20190328\",\"20060101 A G06N G06N99/00 L I B US H 20190328\"],\"intlPubClassificationNonInvention\":null,\"inventorCitizenship\":null,\"inventorCorrection\":null,\"inventorDeceased\":null,\"inventorStreetAddress\":null,\"inventorText\":null,\"jpoFiClassification\":null,\"legalRepresentativeCity\":null,\"legalRepresentativeCountry\":null,\"legalRepresentativeName\":null,\"legalRepresentativePostcode\":null,\"legalRepresentativeState\":null,\"legalRepresentativeStreetAddress\":null,\"legalRepresentativeText\":null,\"messengerDocsFlag\":null,\"newRecordPatentDerwent\":null,\"numberOfClaims\":null,\"numberOfDrawingSheets\":null,\"numberOfFigures\":null,\"numberOfPagesInSpecification\":null,\"numberOfPagesOfSpecification\":null,\"objectContents\":\"[]\",\"objectDescription\":\"[]\",\"parentDocCountry\":null,\"parentGrantDocCountry\":null,\"patentBibliographicHeader\":null,\"pctOrRegionalPublishingSerial\":null,\"pfDerwentWeekNum\":null,\"principalAttorneyName\":null,\"priorityApplicationCountry\":null,\"priorityClaimsCountry\":[\"JP\"],\"priorityNumberDerived\":[\"2017JP-2017-187096\"],\"publicationIssueNumber\":null,\"refCitedPatentDocNumber\":null,\"refCitedPatentDocDate\":null,\"refCitedPatentDocKindCode\":null,\"referenceCitedCode\":null,\"referenceCitedGroup\":null,\"referenceCitedSearchPhase\":null,\"referenceCitedText\":null,\"registrationNumber\":null,\"reissueApplCountry\":null,\"reissueParentKind\":null,\"reissueParentNumber\":null,\"reissueParentPubCountry\":null,\"reissuePatentGroup\":null,\"reissuePatentParentStatus\":null,\"reissuedPatentApplCountry\":null,\"reissuedPatentApplKind\":null,\"reissuedPatentApplNumber\":null,\"relatedApplChildPatentCountry\":null,\"relatedApplChildPatentName\":null,\"relatedApplChildPatentNumber\":null,\"relatedApplCountryCode\":null,\"relatedApplParentGrantPatentKind\":null,\"relatedApplParentGrantPatentName\":null,\"relatedApplParentPatentKind\":null,\"relatedApplParentPatentName\":null,\"relatedApplParentPctDoc\":null,\"relatedApplParentStatusCode\":null,\"relatedApplPatentNumber\":null,\"relatedApplRelatedPub\":null,\"relatedApplTypeOfCorrection\":null,\"rule47Flag\":null,\"selectedDrawingCharacter\":null,\"selectedDrawingFigure\":null,\"statutoryInventionText\":null,\"termOfExtension\":null,\"termOfPatentGrant\":null,\"titleTermsData\":null,\"additionalIndexingTerm\":null,\"applicationYearSearch\":\"2018\",\"pfApplicationYearSearch\":null,\"assigneeCountry\":[\"JP\"],\"certOfCorrectionFlag\":null,\"citedPatentLiteratureAddressInformation\":null,\"citedPatentLiteratureClassificationIpc\":null,\"citedPatentLiteratureOrganizationName\":null,\"citedPatentLiteratureRefNumber\":null,\"crossReferenceNumber\":null,\"country\":\"US\",\"cpiManualCodes\":null,\"cpiSecondaryAccessionNumber\":null,\"curIntlPatentAllClassificationLong\":null,\"currentUsOriginalClassificationLong\":null,\"datePublSearch\":\"2019-03-28T00:00:00.000+00:00\",\"datePublYearSearch\":\"2019\",\"epiManualCodes\":null,\"fieldOfSearchMainClassNational\":null,\"inventorCountry\":[\"JP\"],\"ipcAllMainClassification\":[\"G06N\",\"G16H\",\"G06K\"],\"issuedUsClassificationFull\":null,\"issuedUsDigestRefClassification\":null,\"jpoFiCurrentAdditionalClassification\":null,\"jpoFiCurrentInventiveClassification\":null,\"legalFirmName\":null,\"locarnoMainClassification\":null,\"nonCpiSecondaryAccessionNumber\":null,\"objectId\":null,\"otherRefPub\":null,\"pageNumber\":null,\"patentAssigneeCode\":null,\"patentAssigneeNameTotal\":null,\"patentFamilyDate\":null,\"patentFamilyDocNumber\":null,\"patentFamilyKind\":null,\"patentFamilyKindCode\":null,\"patentFamilyLanguage\":null,\"patentFamilyName\":null,\"patentNumberOfLocalApplication\":null,\"pct102eDate\":null,\"pct371c124Date\":null,\"pct371c124DateKwicHits\":null,\"pctFilingDate\":null,\"pctFilingDateKwicHits\":null,\"pctFilingDocCountryCode\":null,\"pctFilingKind\":null,\"pctFilingNumber\":null,\"pctName\":null,\"pctOrRegionalPublishingCountry\":null,\"pctOrRegionalPublishingKind\":null,\"pctOrRegionalPublishingName\":null,\"pctOrRegionalPublishingText\":null,\"pctPubDate\":null,\"pctPubDateKwicHits\":null,\"pctPubDocIdentifier\":null,\"pctPubNumber\":null,\"pfApplicationDateSearch\":null,\"pfApplicationType\":null,\"pfDerwentWeekDate\":null,\"pfPublDateSearch\":null,\"pfPublDateSearchKwicHits\":null,\"pfPublYearSearch\":null,\"polymerIndexingCodes\":null,\"polymerMultipunchCodeRecordNumber\":null,\"polymerMultipunchCodes\":null,\"priorPublishedDocCountryCode\":null,\"priorPublishedDocDate\":null,\"priorPublishedDocDateKwicHits\":null,\"priorPublishedDocIdentifier\":null,\"priorPublishedDocKindCode\":null,\"priorPublishedDocNumber\":null,\"priorityApplYear\":[\"2017\"],\"priorityApplicationDate\":null,\"priorityClaimsDateSearch\":null,\"priorityClaimsDocNumber\":[\"2017-187096\"],\"priorityPatentDid\":null,\"priorityPatentNumber\":null,\"ptabCertFlag\":null,\"pubRefCountryCode\":\"US\",\"pubRefDocNumber\":\"20190095759\",\"pubRefDocNumber1\":\"20190095759\",\"publicationData\":null,\"recordPatentNumber\":null,\"reexaminationFlag\":null,\"refCitedOthers\":null,\"refCitedPatentDocCountryCode\":null,\"refCitedPatentDocName\":null,\"refCitedPatentRelevantPassage\":null,\"reissueParentIssueDate\":null,\"reissuedPatentApplFilingDate\":null,\"relatedAccessionNumbers\":null,\"relatedApplChildPatentDate\":null,\"relatedApplFilingDateKwicHits\":null,\"relatedApplNumber\":null,\"relatedApplPatentIssueDate\":null,\"relatedApplPatentIssueDateKwicHits\":null,\"relatedDocumentKindCode\":null,\"securityLegend\":null,\"sequenceCwu\":null,\"sequenceListNewRules\":null,\"sequenceListOldRules\":null,\"sequencesListText\":null,\"standardTitleTerms\":null,\"supplementalExaminationFlag\":null,\"usBotanicLatinName\":null,\"usBotanicVariety\":null,\"usRefClassification\":null,\"usRefCpcClassification\":null,\"usRefGroup\":null,\"usRefIssueDate\":null,\"usRefIssueDateKwicHits\":null,\"usRefPatenteeName\":null,\"volumeNumber\":null,\"correspondenceNameAddress\":null,\"correspondenceAddressCustomerNumber\":null,\"ibmtdbAccessionNumber\":null,\"inventorsName\":[\"KANADA;
        Shoji\"],\"applicationKindCode\":\"A1\",\"inventorNameDerived\":null,\"intlPubClassificationClass\":[\"G06K\",\"G16H\",\"G06N\"],\"issuedUsOrigClassification\":null,\"curCpcSubclassFull\":[\"G06N\",\"G06F\",\"G16H\",\"G06V\"],\"cpcCurAdditionalClass\":null,\"cpcCurInventiveClass\":[\"G06F\",\"G06N\",\"G06V\",\"G06N\",\"G06V\",\"G16H\",\"G06N\",\"G16H\",\"G06F\",\"G06F\",\"G16H\",\"G06N\"],\"cpcCurClassificationGroup\":[\"G
        G06F G06F18/2178 20230101 L I R C 20230101 US\",\"G G06N G06N3/042 20230101
        L I R C 20230101 US\",\"G G06V G06V10/774 20220101 L I R H 20230111 US\",\"G
        G06N G06N3/045 20230101 L I R C 20230101 US\",\"G G06V G06V10/7784 20220101
        L I R H 20230111 US\",\"G G16H G16H50/70 20180101 L I B H 20191011 US\",\"G
        G06N G06N3/08 20130101 F I B H 20191011 US\",\"G G16H G16H30/20 20180101 L
        I B H 20210127 US\",\"G G06F G06F18/214 20230101 L I R C 20230101 US\",\"G
        G06F G06F18/217 20230101 L I R C 20230101 US\",\"G G16H G16H30/40 20180101
        L I B H 20191011 US\",\"G G06N G06N20/00 20190101 L I B C 20210127 US\"],\"curCpcClassificationFull\":[\"G06F18/214
        20230101\",\"G06N20/00 20190101\",\"G06V10/774 20220101\",\"G16H30/20 20180101\",\"G06F18/217
        20230101\",\"G06N3/08 20130101\",\"G16H50/70 20180101\",\"G16H30/40 20180101\",\"G06N3/045
        20230101\",\"G06V10/7784 20220101\",\"G06N3/042 20230101\",\"G06F18/2178 20230101\"],\"cpcCombinationClassificationCur\":null,\"cpcCombinationTallyCur\":null,\"intlFurtherClassification\":null,\"currentUsPatentClass\":[\"1\"],\"internationalClassificationInfom\":null,\"cpcOrigInventvClssifHlghts\":[\"G06K9/6262
        20130101\",\"G06N99/005 20130101\",\"G16H30/20 20180101\"],\"idWithoutSolrPartition\":\"PG-US-20190095759\",\"curIntlPatentClassifictionPrimaryDateKwicHits\":null,\"curIntlPatentClssifSecHlights\":null,\"publicationReferenceDocumentNumberOne\":\"20190095759\",\"descriptionStart\":11,\"descriptionEnd\":19}"
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:52:36 GMT
      Server-Timing:
      - intid;desc=a5628724be41bfa5
      Transfer-Encoding:
      - chunked
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 17:52:36 GMT
      apigw-requestid:
      - fDYzti-6oAMEPRg=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: '{"country":"US","internal":"false","corrAppNum":"16123456","id":"16123456","type":"application","list":[{"appNum":"2017187096","appDate":1506470400000,"pubList":[{"pubCountry":"JP","pubNum":"2019061579","pubDate":1555545600000,"kindCode":"A","pubDateStr":"04/18/2019"},{"pubCountry":"JP","pubNum":"6768620","pubDate":1602633600000,"kindCode":"B2","pubDateStr":"10/14/2020"}],"countryCode":"JP","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/27/2017","docListMsg":null,"docNum":{"country":"JP","docNumber":"2017187096","format":null,"date":"09/27/2017","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"16123456","appDate":1536192000000,"pubList":[{"pubCountry":"US","pubNum":"20190095759","pubDate":1553731200000,"kindCode":"A1","pubDateStr":"03/28/2019"},{"pubCountry":"US","pubNum":"10902286","pubDate":1611619200000,"kindCode":"B2","pubDateStr":"01/26/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/06/2018","docListMsg":null,"docNum":{"country":"US","docNumber":"201816123456","format":null,"date":"09/06/2018","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"17130468","appDate":1608595200000,"pubList":[{"pubCountry":"US","pubNum":"20210110206","pubDate":1618444800000,"kindCode":"A1","pubDateStr":"04/15/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"US","docNumber":"16123456","kindCode":"A"},{"country":"US","docNumber":"17130468","kindCode":"A"},{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"12/22/2020","docListMsg":null,"docNum":{"country":"US","docNumber":"202017130468","format":null,"date":"12/22/2020","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '1885'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:52:36 GMT
      apigw-requestid:
      - fDYzuhOhIAMEPTg=
    status:
      code: 200
      message: OK
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 2168779, "hl_snippets": "2", "op": "OR", "q": "\"10902286\".PN.",
      "queryName": "\"10902286\".PN.", "highlights": "1", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "USPAT", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "\"10902286\".PN."}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '715'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":2168779,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"USPAT","countryCodes":[]}],"q":"\"10902286\".PN.","queryName":"\"10902286\".PN.","userEnteredQuery":"\"10902286\".PN.","viewName":"tile","op":"OR","highlights":"1","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"\"10902286\".PN.","error":null,"terms":["\"10902286\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":27,"highlightingTime":0,"cursorMarker":"AoJwwK3/nfcCNzY1ODA3Njk2IVVTLVVTLTEwOTAyMjg2","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-10902286-B2","publicationReferenceDocumentNumber":"10902286","compositeId":"65807696!US-US-10902286","publicationReferenceDocumentNumber1":"10902286","datePublishedKwicHits":null,"datePublished":"2021-01-26T00:00:00Z","inventionTitle":"Learning
        assistance device, method of operating learning assistance device, learning
        assistance program, learning assistance system, and terminal device","type":"USPAT","mainClassificationCode":"1/1","applicantName":["FUJIFILM
        Corporation"],"assigneeName":["FUJIFILM Corporation"],"uspcFullClassificationFlattened":null,"ipcCodeFlattened":"G06K9/62;G06N3/04","cpcInventiveFlattened":"G16H30/40;G06N3/08;G06V10/7784;G06K9/6256;G06N20/00;G16H30/20;G06V10/774;G06K9/6263;G16H50/70;G06N3/0454;G06N3/0427;G06K9/6262","cpcAdditionalFlattened":null,"applicationFilingDate":["2018-09-06T00:00:00Z"],"applicationFilingDateKwicHits":null,"relatedApplFilingDate":null,"primaryExaminer":"Sabouri;
        Mazda","assistantExaminer":null,"applicationNumber":"16/123456","frontPageStart":1,"frontPageEnd":1,"drawingsStart":2,"drawingsEnd":10,"specificationStart":11,"specificationEnd":18,"claimsStart":18,"claimsEnd":20,"abstractStart":1,"abstractEnd":1,"bibStart":1,"bibEnd":1,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":20,"pageCountDisplay":"20","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/G86/022/109","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":"Kanada;
        Shoji","familyIdentifierCur":65807696,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":["2017-09-27T00:00:00Z"],"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        10902286 B2","derwentAccessionNumber":null,"documentSize":88828,"score":14.152454,"governmentInterest":null,"kindCode":["B2"],"urpn":["2007/0217688","2015/0242760"],"urpnCode":["2007/0217688","2015/0242760"],"descriptionEnd":18,"publicationReferenceDocumentNumberOne":"10902286","descriptionStart":11}],"qtime":24}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:52:36 GMT
      Server-Timing:
      - intid;desc=662942d907168925
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://ppubs.uspto.gov/dirsearch-public/patents/US-10902286-B2/highlight?queryId=1&source=USPAT&includeSections=True
  response:
    body:
      string: "{\"guid\":\"US-10902286-B2\",\"publicationReferenceDocumentNumber\":\"10902286\",\"compositeId\":\"65807696!US-US-10902286\",\"publicationReferenceDocumentNumber1\":\"10902286\",\"datePublishedKwicHits\":null,\"datePublished\":\"2021-01-26T00:00:00Z\",\"inventionTitle\":\"Learning
        assistance device, method of operating learning assistance device, learning
        assistance program, learning assistance system, and terminal device\",\"type\":\"USPAT\",\"mainClassificationCode\":\"1/1\",\"applicantName\":[\"FUJIFILM
        Corporation\"],\"assigneeName\":[\"FUJIFILM Corporation\"],\"uspcFullClassificationFlattened\":null,\"ipcCodeFlattened\":\"G06K9/62;G06N3/04\",\"cpcInventiveFlattened\":\"G16H30/40;G06N3/08;G06V10/7784;G06K9/6256;G06N20/00;G16H30/20;G06V10/774;G06K9/6263;G16H50/70;G06N3/0454;G06N3/0427;G06K9/6262\",\"cpcAdditionalFlattened\":null,\"applicationFilingDate\":[\"2018-09-06T00:00:00Z\"],\"applicationFilingDateKwicHits\":null,\"relatedApplFilingDate\":null,\"primaryExaminer\":\"Sabouri;
        Mazda\",\"assistantExaminer\":null,\"applicationNumber\":\"16/123456\",\"frontPageStart\":1,\"frontPageEnd\":1,\"drawingsStart\":2,\"drawingsEnd\":10,\"specificationStart\":11,\"specificationEnd\":18,\"claimsStart\":18,\"claimsEnd\":20,\"abstractStart\":1,\"abstractEnd\":1,\"bibStart\":1,\"bibEnd\":1,\"certCorrectionStart\":0,\"certCorrectionEnd\":0,\"certReexaminationStart\":0,\"certReexaminationEnd\":0,\"supplementalStart\":0,\"supplementalEnd\":0,\"ptabStart\":0,\"ptabEnd\":0,\"amendStart\":0,\"amendEnd\":0,\"searchReportStart\":0,\"searchReportEnd\":0,\"pageCount\":20,\"pageCountDisplay\":\"20\",\"previouslyViewed\":false,\"unused\":false,\"imageLocation\":\"uspat/US/G86/022/109\",\"imageFileName\":\"00000001.tif\",\"cpcCodes\":null,\"queryId\":1,\"tags\":null,\"inventorsShort\":\"Kanada;
        Shoji\",\"familyIdentifierCur\":65807696,\"familyIdentifierCurStr\":\"65807696\",\"languageIndicator\":\"EN\",\"databaseName\":\"USPT\",\"dwImageDoctypeList\":null,\"dwImageLocList\":null,\"dwPageCountList\":null,\"dwImageDocidList\":null,\"patentFamilyMembers\":null,\"patentFamilyCountry\":null,\"patentFamilySerialNumber\":null,\"documentIdWithDashesDw\":null,\"pfPublDate\":null,\"pfPublDateKwicHits\":null,\"priorityClaimsDate\":[\"2017-09-27T00:00:00Z\"],\"priorityClaimsDateKwicHits\":null,\"pfApplicationSerialNumber\":null,\"pfApplicationDescriptor\":null,\"pfLanguage\":null,\"pfApplicationDate\":null,\"pfApplicationDateKwicHits\":null,\"clippedUri\":null,\"source\":null,\"documentId\":\"<span
        term=\\\"us10902286b2\\\" class=\\\"highlight18\\\">US 10902286 B2</span>\",\"derwentAccessionNumber\":null,\"documentSize\":88828,\"score\":0.0,\"governmentInterest\":null,\"kindCode\":[\"B2\"],\"urpn\":[\"2007/0217688\",\"2015/0242760\"],\"urpnCode\":[\"2007/0217688\",\"2015/0242760\"],\"abstractedPatentNumber\":null,\"assigneeCity\":[\"Tokyo\"],\"assigneePostalCode\":[\"N/A\"],\"assigneeState\":[\"N/A\"],\"assigneeTypeCode\":[\"03\"],\"curIntlPatentClassificationPrimary\":[\"G06K9/62
        20060101\"],\"curIntlPatentClassificationPrimaryDateKwicHits\":null,\"designatedStates\":null,\"examinerGroup\":\"2641\",\"issuedUsCrossRefClassification\":null,\"jpoFtermCurrent\":null,\"languageOfSpecification\":null,\"chosenDrawingsReference\":null,\"derwentClass\":null,\"inventionTitleHighlights\":null,\"cpcOrigInventiveClassificationHighlights\":[\"G06K9/6256
        20130101\",\"G06K9/6263 20130101\",\"G06N3/0427 20130101\",\"G06N3/0454 20130101\",\"G06N3/08
        20130101\",\"G16H30/40 20180101\",\"G16H50/70 20180101\"],\"cpcInventiveDateKwicHits\":null,\"cpcOrigAdditionalClassification\":null,\"cpcAdditionalDateKwicHits\":null,\"curIntlPatentClssficationSecHighlights\":null,\"fieldOfSearchClassSubclassHighlights\":[\"382/159\"],\"cpcCombinationSetsCurHighlights\":null,\"applicantCountry\":[\"JP\"],\"applicantCity\":[\"Tokyo\"],\"applicantState\":[\"N/A\"],\"applicantZipCode\":[\"N/A\"],\"applicantAuthorityType\":[\"assignee\"],\"applicantDescriptiveText\":null,\"applicationSerialNumber\":[\"123456\"],\"inventorCity\":[\"Tokyo\"],\"inventorState\":[\"N/A\"],\"inventorPostalCode\":[\"N/A\"],\"standardTitleTermsHighlights\":null,\"primaryExaminerHighlights\":\"Sabouri;
        Mazda\",\"continuityData\":null,\"inventors\":null,\"uspcFullClassification\":null,\"uspcCodeFmtFlattened\":null,\"ipcCode\":null,\"applicationNumberHighlights\":[\"16/123456\"],\"dateProduced\":\"2021-01-06T00:00:00Z\",\"auxFamilyMembersGroupTempPlaceHolder\":null,\"priorityCountryCode\":null,\"cpcCurAdditionalClassification\":null,\"internationalClassificationMain\":null,\"internationalClassificationSecondary\":null,\"internationalClassificationInformational\":null,\"europeanClassification\":null,\"europeanClassificationMain\":null,\"europeanClassificationSecondary\":null,\"lanuageIndicator\":null,\"intlPubClassificationPrimary\":[\"G06K9/62
        20060101 G06K009/62\"],\"intlPubClassificationPrimaryDateKwicHits\":null,\"intlPubClassificationSecondary\":[\"G06N3/04
        20060101 G06N003/04\",\"G16H50/70 20180101 G16H050/70\",\"G06N3/08 20060101
        G06N003/08\",\"G16H30/40 20180101 G16H030/40\"],\"intlPubClassificationSecondaryDateKwicHits\":null,\"publicationDate\":null,\"derwentWeekInt\":0,\"derwentWeek\":null,\"currentUsOriginalClassification\":\"1/1\",\"currentUsCrossReferenceClassification\":null,\"locarnoClassification\":null,\"equivalentAbstractText\":null,\"hagueIntlRegistrationNumber\":null,\"hagueIntlFilingDate\":null,\"hagueIntlFilingDateKwicHits\":null,\"hagueIntlRegistrationDate\":null,\"hagueIntlRegistrationDateKwicHits\":null,\"hagueIntlRegistrationPubDate\":null,\"hagueIntlRegistrationPubDateKwicHits\":null,\"curIntlPatentClassificationNoninvention\":null,\"curIntlPatentClassificationNoninventionDateKwicHits\":null,\"curIntlPatentClassificationSecondary\":[\"G06N3/04
        20060101\",\"G16H50/70 20180101\",\"G06N3/08 20060101\",\"G16H30/40 20180101\"],\"curIntlPatentClassificationSecondaryDateKwicHits\":null,\"abstractHtml\":\"A
        learning assistance device acquires a plurality of learned discriminators
        obtained by causing learning discriminators provided in a plurality of respective
        terminal devices to perform learning using image correct answer data, acquires
        a plurality of discrimination results obtained by causing a plurality of learned
        discriminators to discriminate the same input image, determines the correct
        answer data of the input image on the basis of the plurality of discrimination
        results, causes the discriminator to perform learning the input image and
        the correct answer data, and outputs a result thereof as a new learning discriminator
        to each terminal device.\",\"descriptionHtml\":\"BRIEF DESCRIPTION OF THE
        DRAWINGS<br />(1) <figref idref=\\\"DRAWINGS\\\">FIG. 1</figref> is a diagram
        illustrating a schematic configuration of a learning assistance system of
        the present invention.<br />(2) <figref idref=\\\"DRAWINGS\\\">FIG. 2</figref>
        is a diagram illustrating a schematic configuration of a medical information
        system.<br />(3) <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref> illustrates
        an example of a multilayered neural network.<br />(4) <figref idref=\\\"DRAWINGS\\\">FIG.
        4</figref> is a block diagram illustrating a schematic configuration of a
        terminal device and a learning assistance device according to a first embodiment.<br
        />(5) <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref> is a diagram illustrating
        learning of a discriminator.<br />(6) <figref idref=\\\"DRAWINGS\\\">FIG.
        6</figref> is a flowchart showing a flow of a process of causing the discriminator
        to perform learning.<br />(7) <figref idref=\\\"DRAWINGS\\\">FIG. 7</figref>
        is a block diagram illustrating a schematic configuration of a terminal device
        and a learning assistance device according to a second embodiment.<br />(8)
        <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref> is a block diagram illustrating
        a schematic configuration of a terminal device and a learning assistance device
        according to a third embodiment.<br />(9) <figref idref=\\\"DRAWINGS\\\">FIG.
        9</figref> is a block diagram illustrating a schematic configuration of a
        terminal device and a learning assistance device according to a fourth embodiment.<br
        />DETAILED DESCRIPTION<br />(10) <figref idref=\\\"DRAWINGS\\\">FIG. 1</figref>
        illustrates a schematic configuration of a learning assistance system <b>1</b>
        according to a first embodiment of the present invention. The learning assistance
        system <b>1</b> is configured by connecting a plurality of terminal devices
        <b>10</b> installed in a plurality of medical institutions A, B, . . . , X
        and a learning assistance device <b>20</b> placed on a cloud side over a network
        <b>30</b>.<br />(11) The learning assistance device <b>20</b> includes a well-known
        hardware configuration such as a central processing unit (CPU), a memory,
        a storage, an input and output interface, a communication interface, an input
        device, a display device, and a data bus, and is a high-performance computer
        in which a well-known operation system or the like is installed and which
        has a server function. Further, a graphics processing unit (GPU) may be provided,
        as necessary. Alternatively, the learning assistance device <b>20</b> may
        be a virtualized virtual server provided using one or a plurality of computers.
        The learning assistance program of the present invention is installed in a
        server, and functions as a learning assistance device by a program instruction
        being executed by the CPU of the computer.<br />(12) The terminal device <b>10</b>
        is a computer for image processing provided in the respective medical institutions
        A, B, . . . , X, and includes a well-known hardware configuration such as
        a CPU, a memory, a storage, an input and output interface, a communication
        interface, an input device, a display device, and a data bus. A well-known
        operation system or the like is installed in the terminal device <b>10</b>.
        The terminal device <b>10</b> includes a display as a display device. Further,
        a GPU may be provided, as necessary.<br />(13) The network <b>30</b> is a
        wide area network (WAN) that widely connects the terminal devices <b>10</b>
        placed at the plurality of medical institutions A, B, . . . , X to the learning
        assistance device <b>20</b> via a public network or a private network.<br
        />(14) Further, as illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 2</figref>,
        the terminal device <b>10</b> is connected to respective medical information
        systems <b>50</b> of the respective medical institutions A, B, . . . , X over
        a local area network (LAN) <b>51</b>. The medical information system <b>50</b>
        includes a modality (an imaging device) <b>52</b>, an image database <b>53</b>,
        and an image interpretation medical workstation <b>54</b>, and is configured
        so that transmission and reception of image data to and from each other are
        performed over the network <b>51</b>. It should be noted that in the network
        <b>51</b>, it is desirable to use a communication cable such as an optical
        fiber so that image data can be transferred at a high speed.<br />(15) The
        modality <b>52</b> includes a device that images an examination target part
        of a subject to generate an examination image representing the part, adds
        accessory information defined in a DICOM standard to the image, and outputs
        the resultant image. Specific examples of the device include a computed tomography
        (CT) device, a magnetic resonance imaging (MRI) device, a positron emission
        tomography (PET) device, an ultrasonic device, and a computed radiography
        (CR) device using a planar X-ray detector (FPD: flat panel detector).<br />(16)
        In the image database <b>53</b>, a software program for providing a function
        of a database management system (DBMS) is incorporated in a general-purpose
        computer, and a large capacity storage is included. This storage may be a
        large capacity hard disk device, or may be a disk device connected to a network
        attached storage (NAS) or a storage area network (SAN) connected to the network
        <b>51</b>. Further, the image data captured by the modality <b>52</b> is transmitted
        to and stored in the image database <b>53</b> over the network <b>51</b> according
        to a storage format and a communication standard conforming to a DICOM standard.<br
        />(17) The image interpretation medical workstation <b>54</b> is a computer
        that is used for an image interpretation doctor of a radiology department
        to interpret an image and create an interpretation report. The image interpretation
        medical workstation <b>54</b> performs a display of the image data received
        from the image database <b>53</b> and performs automatic detection of a portion
        likely to be a lesion in the image.<br />(18) In the embodiment, a case where
        an image processing program in which a discriminator functioning as an actually
        operated discriminator is incorporated in each terminal device <b>10</b> is
        provided from the learning assistance device <b>20</b>, and a learning program
        in which a discriminator functioning as a learning program separately from
        the image processing program is incorporated is provided will be described.
        The image processing program and the learning program distributed to each
        terminal device <b>10</b> are installed in the terminal device <b>10</b> to
        function as an image processing device in which the actually operated discriminator
        is incorporated, and a learning discriminator.<br />(19) Further, a case where
        the actually operated discriminator and the learning discriminator are multilayered
        neural networks subjected to deep learning to be able to discriminate a plurality
        of types of organ areas and/or lesion areas will be described. In the multilayered
        neural network, a calculation process is performed on a plurality of pieces
        of different calculation result data obtained by a preceding layer for input
        data, that is, extraction result data of a feature amount using various kernels
        in each layer, data of the feature amount obtained by the calculation process
        is acquired, and a further calculation process is performed on the data of
        the feature amount in the next and subsequent processing layers. Thus, it
        is possible to improve a recognition rate of the feature amount and to discriminate
        which of a plurality of types of areas the input image data is.<br />(20)
        <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref> is a diagram illustrating an
        example of the multilayered neural network. As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        3</figref>, the multilayered neural network <b>40</b> includes a plurality
        of layers including an input layer <b>41</b> and an output layer <b>42</b>.
        In <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref>, a layer before the output
        layer <b>42</b> is denoted by reference numeral <b>43</b>.<br />(21) In the
        multilayered neural network <b>40</b>, the image data is input to the input
        layer <b>41</b> and a discrimination result of an area is output. In a case
        where learning is performed, the output discrimination result is compared
        with correct answer data, and a weight of coupling between the respective
        layers of units (indicated by circles in <figref idref=\\\"DRAWINGS\\\">FIG.
        3</figref>) included in the respective layers of the multilayered neural network
        <b>40</b> is corrected from the output side (the output layer <b>42</b>) to
        the input side (the input layer <b>41</b>) according to whether an answer
        is a correct answer or an incorrect answer. The correction of the weight of
        coupling is repeatedly performed a predetermined number of times, or is repeatedly
        performed until a correct answer rate of the output discrimination result
        is 100% or is equal to or greater than a predetermined threshold value using
        a large number of pieces of image data with correct answer data, and the learning
        ends.<br />(22) <figref idref=\\\"DRAWINGS\\\">FIG. 4</figref> is a block
        diagram illustrating a schematic configuration of the terminal device <b>10</b>
        and the learning assistance device <b>20</b>. Functions of the terminal device
        <b>10</b> and the learning assistance device <b>20</b> will be described in
        detail with reference to <figref idref=\\\"DRAWINGS\\\">FIG. 4</figref>. First,
        the terminal device <b>10</b> will be described.<br />(23) The terminal device
        <b>10</b> includes a discriminator acquisition unit <b>12</b>, a discrimination
        result acquisition unit <b>13</b>, a learning unit <b>14</b>, and a learned
        discriminator output unit <b>15</b>.<br />(24) The discriminator acquisition
        unit <b>12</b> acquires a learning discriminator and an actually operated
        discriminator. For example, the image processing program and the learning
        program are received from the learning assistance device <b>20</b> over the
        network <b>30</b>, and the received image processing program is installed.
        Accordingly, image processing in which the actually operated discriminator
        is incorporated becomes executable in the terminal device <b>10</b> and functions
        as the discrimination result acquisition unit <b>13</b>. Similarly, the learning
        program is installed, and the learning discriminator becomes executable and
        functions as the learning unit <b>14</b>. It should be noted that the learning
        discriminator is a discriminator that has learned the same image correct answer
        data as the actually operated discriminator received from the learning assistance
        device <b>20</b>. In the following description, the image processing in which
        the actually operated discriminator is incorporated is simply referred to
        as an actually operated discriminator. It should be noted that the image correct
        answer data refers to a combination of the image data and correct answer data
        thereof. Details of the image correct answer data will be described below.<br
        />(25) The discrimination result acquisition unit <b>13</b> inputs a discrimination
        target image data to the actually operated discriminator and acquires a discrimination
        result. The actually operated discriminator is a discriminator of which discrimination
        performance has been guaranteed in the learning assistance device <b>20</b>,
        and in each of the medical institutions A, B, X, the discrimination is performed
        on the image data that is a diagnosis target using the actually operated discriminator.
        Further, the discrimination result acquisition unit <b>13</b> may perform
        discrimination of the image data that is a diagnosis target sent from the
        image interpretation medical workstation <b>54</b> to the terminal device
        <b>10</b> over the network <b>51</b>, and transmit a discrimination result
        from the terminal device <b>10</b> to the image interpretation medical workstation
        <b>54</b>.<br />(26) The learning unit <b>14</b> causes the learning discriminator
        to perform learning using the image data and the correct answer data thereof.
        The correct answer data includes a mask image showing an area such as an organ
        or abnormal shadow of the image data, and information indicating what the
        area of the mask image is (for example, an area of an organ such as a liver,
        a kidney, or a lung or an area of an abnormal shadow such as a liver cancer,
        a kidney cancer, or a pulmonary nodule).<br />(27) The correct answer data
        may be created by an image interpretation doctor or the like of each of the
        medical institutions A, B, . . . , X observing the image data. For example,
        the image data is extracted from the image database <b>53</b>, the discrimination
        result acquisition unit <b>13</b> inputs the image data to the actually operated
        discriminator and acquires a discrimination result, and a user such as the
        image interpretation doctor determines whether the discrimination result is
        a correct answer or an incorrect answer, and stores a discrimination result
        together with the input image data and correct answer data in the image database
        <b>53</b> as image correct answer data in the case of the correct answer.
        In the case of an incorrect answer, the user generates a mask image of the
        correct answer data, assigns the correct answer data to the image data, and
        stores the resultant data in the image database <b>53</b> as image correct
        answer data.<br />(28) Therefore, the learning unit <b>14</b> causes the multilayered
        neural network <b>40</b> of the learning discriminator to perform learning
        using a large number of pieces of image correct answer data stored in the
        image database <b>53</b>. First, the image data of the image correct answer
        data is input to the multilayered neural network <b>40</b>, and a discrimination
        result is output. Then, the output discrimination result is compared with
        the correct answer data, and a weight of coupling between the respective layers
        of the units included in the respective layers of the multilayered neural
        network <b>40</b> from the output side to the input side is corrected according
        to whether the answer is a correct answer or an incorrect answer. The correction
        of the weight of the coupling is repeatedly performed using a large number
        of pieces of correct answer data a predetermined number of times or until
        the correct answer rate of the output discrimination result becomes 100%,
        and the learning is ended.<br />(29) The learned discriminator output unit
        <b>15</b> outputs the learning discriminator of which the learning has ended
        in the learning unit <b>14</b> as a learned discriminator. Specifically, the
        weight (hereinafter referred to as a parameter) of coupling between the layers
        of the units constituting the neural network constituting the learned discriminator
        is periodically transmitted to the learning assistance device <b>20</b> over
        the network <b>30</b>.<br />(30) Next, the learning assistance device <b>20</b>
        will be described. As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 4</figref>,
        the learning assistance device <b>20</b> includes a learned discriminator
        acquisition unit <b>22</b>, a discriminator storage unit <b>23</b>, a correct
        answer data acquisition unit <b>24</b>, a correct answer data storage unit
        <b>25</b>, a learning unit <b>26</b>, and a discriminator output unit <b>27</b>.<br
        />(31) The learned discriminator acquisition unit <b>22</b> receives a parameter
        of the multilayered neural network <b>40</b> constituting the learned discriminators
        transmitted from the plurality of terminal devices <b>10</b> over the network
        <b>30</b>. The received parameter is temporarily stored in the discriminator
        storage unit <b>23</b>. The multilayered neural network <b>40</b> is provided
        in the learning assistance device <b>20</b> in advance and the parameter received
        from each terminal device <b>10</b> is set as the weight of the coupling between
        the respective layers of the units of the multilayered neural network <b>40</b>
        provided in the learning assistance device <b>20</b>. By re-setting the parameter
        received from each terminal device in this weight, the same learned discriminator
        as each terminal device <b>10</b> can be acquired.<br />(32) The correct answer
        data acquisition unit <b>24</b> causes each of the learned discriminators
        collected from each of the terminal devices <b>10</b> to discriminate the
        same input image data to acquire a plurality of discrimination results, and
        determines the correct answer data of the input image data from the plurality
        of discrimination results.<br />(33) A large number of pieces of image data
        are often stored in a database without correct answer data attached thereto.
        In order to attach the correct answer data to the image data, for example,
        the image data is input to the discriminator so as to acquire a discrimination
        result, and a user such as an image interpretation doctor performs a determination
        that the answer is a correct answer or an incorrect answer with respect to
        the discrimination result, and registers the discrimination result as the
        image correct answer data in association with the correct answer data and
        the input image data in the case of the correct answer. In case of the incorrect
        answer, the user creates a mask image of the correct answer data and registers
        the mask image as image correct answer data in association with input image
        data. Work of creating the correct answer data in this way is laborious and
        it is difficult to manually generate a large number of pieces of correct answer
        data.<br />(34) Therefore, the correct answer data acquisition unit <b>24</b>
        determines the largest number of same discrimination results as correct answer
        data of the input image data from the discrimination results obtained by inputting
        the same input image data to the learned discriminators collected from the
        respective terminal devices <b>10</b>. Thus, in a case where the correct answer
        data is determined from a plurality of discrimination results obtained by
        using the learned discriminators collected from the respective terminal devices
        <b>10</b>, accurate correct answer data can be automatically generated, and
        therefore, it is possible to easily obtain a large number of pieces of correct
        answer data. The obtained correct answer data is accumulated in the storage
        (the correct answer data storage unit <b>25</b>) as image correct answer data
        in association with the input image data.<br />(35) The learning unit <b>26</b>
        is provided with a deep learning discriminator configured of a multilayered
        neural network <b>40</b>. The image correct answer data accumulated in the
        correct answer data storage unit <b>25</b> is sequentially input to the deep
        learning discriminator, so that learning is performed.<br />(36) In a stage
        the learning had progressed to a certain extent and accuracy of the discrimination
        of the deep learning discriminator has improved, the discriminator output
        unit <b>27</b> generates an image processing program (actually operated discriminator)
        incorporating the deep learning discriminator learned by the learning unit
        <b>26</b> and a learning program (the learning discriminator), and distributes
        the programs to each terminal device <b>10</b> over the network <b>30</b>.
        Since software intended for medical purposes is a target of the Pharmaceutical
        and Medical Device Act (the revised Pharmaceutical Affairs Act), the software
        is required to meet a criterion prescribed in the Pharmaceutical and Medical
        Device Act. Therefore, it is preferable to confirm that a deep learning discriminator
        before distribution exceeds an evaluation standard using an evaluation image
        set formed through a combination of a plurality of images in which the criterion
        prescribed in the Pharmaceutical and Medical Device Act can be evaluated,
        and then, distribute deep learning discriminator.<br />(37) Even after the
        discriminator output unit <b>27</b> distributes the image processing program
        and the learning program to the respective terminal devices <b>10</b>, the
        learning unit <b>26</b> sequentially inputs the image correct answer data
        accumulated in the correct answer data storage unit <b>25</b> to the deep
        learning discriminator as it is and causes the deep learning discriminator
        to perform learning. That is, the learning unit <b>26</b> causes the learning
        discriminator output by the discriminator output unit <b>27</b> to perform
        additional learning, and the discriminator output unit <b>27</b> outputs the
        additionally learned learning discriminator as a new learning discriminator.<br
        />(38) Next, a flow of a deep learning process of the embodiment will be described
        with reference to a transition diagram of <figref idref=\\\"DRAWINGS\\\">FIG.
        5</figref> and a flowchart of <figref idref=\\\"DRAWINGS\\\">FIG. 6</figref>.<br
        />(39) First, in the learning assistance device <b>20</b>, the discriminator
        output unit <b>27</b> distributes the actually operated discriminator NNo
        and the learning discriminator NNt to the plurality of terminal devices <b>10</b>,
        and to the medical institution A, . . . , the medical institution X over the
        network <b>30</b> (S<b>1</b>).<br />(40) In the terminal device <b>10</b>,
        the discriminator acquisition unit <b>12</b> acquires the actually operated
        discriminator NNo and the learning discriminator NNt (S<b>2</b>). The actually
        operated discriminator NNo is used for diagnosis by an image interpretation
        doctor, and the discrimination result acquisition unit <b>13</b> discriminates
        image data (input) that is a diagnosis target and obtains a discrimination
        result (output) (see <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>). Further,
        in the medical institution A, the learning unit <b>14</b> of the terminal
        device <b>10</b> causes the learning discriminator NNt to perform learning
        using the image correct answer data T stored in the image database <b>53</b>
        (S<b>3</b>) and generates the learned discrimination unit NNt-A (S<b>4</b>).
        Similarly, in the medical institution X, the learning unit <b>14</b> of the
        terminal device <b>10</b> causes the learning discriminator NNt to perform
        learning using the image correct answer data T and generates the learned discriminator
        NNt-X (S<b>4</b>).<br />(41) Periodically, in each terminal device <b>10</b>,
        the learned discriminator output unit <b>15</b> transmits the learned discriminator
        to the learning assistance device <b>20</b> (S<b>5</b>). The parameter of
        the learned discriminator NNt-A is transmitted from the terminal device <b>10</b>
        of the medical institution A to the learning assistance device <b>20</b>,
        and the parameter of the learned discriminator NNt-X is transmitted from the
        terminal device <b>10</b> of the medical institution X to the learning assistance
        device <b>20</b> (see a solid arrow (1) in <figref idref=\\\"DRAWINGS\\\">FIG.
        5</figref>).<br />(42) In the learning assistance device <b>20</b>, the learned
        discriminator acquisition unit <b>22</b> temporarily stores the parameters
        of the learned discriminators received from the plurality of terminal devices
        <b>10</b> in the discriminator storage unit <b>23</b>. By setting this parameter
        in the multilayered neural network <b>40</b> provided in the learning assistance
        device <b>20</b>, the learned discriminator learned by each terminal device
        <b>10</b> is acquired (S<b>6</b>).<br />(43) The correct answer data acquisition
        unit <b>24</b> inputs the input image data P to the learned discriminator
        of each terminal device <b>10</b> to obtain the discrimination result. In
        the example of <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>, a result a,
        a result b, a result c, . . . , a result g are obtained, and a largest number
        of results b are determined to be correct answer data of the input image data
        P (S<b>7</b>). The input image data P and the correct answer data are accumulated
        in the storage <b>25</b> in association with each other.<br />(44) The learning
        unit <b>26</b> causes the deep learning discriminator NNl to perform learning
        using the input image data P and the result b (correct answer data) (S<b>8</b>).
        Periodically, a new version of the actually operated discriminator NNo and
        the learning discriminator NNt are generated on the basis of the deep learning
        discriminator NNl (S<b>9</b>). The discriminator output unit <b>27</b> distributes
        the new version of the actually operated discriminator NNo and learning discriminator
        NNt to each of the terminal devices <b>10</b> (S<b>10</b>; see an arrow (2)
        of a broken line in <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>).<br />(45)
        In the terminal device <b>10</b> of the medical institution A, the discriminator
        acquisition unit <b>12</b> acquires the new version of the actually operated
        discriminator NNo and learning discriminator NNt again (S<b>2</b>). The learning
        unit <b>14</b> of the terminal device <b>10</b> causes the new version of
        learning discriminator NNt to perform learning using the image correct answer
        data T stored in the image database <b>53</b> (S<b>3</b>) and generate the
        learned discriminator NNt-A again (S<b>4</b>).<br />(46) In the terminal device
        <b>10</b> of the medical institution X, the discriminator acquisition unit
        <b>12</b> acquires the new version of the actually operated discriminator
        NNo and learning discriminator NNt again (S<b>2</b>). The learning unit <b>14</b>
        of the terminal device <b>10</b> causes the new version of learning discriminator
        NNt to perform learning using the image correct answer data T stored in the
        image database <b>53</b> (S<b>3</b>) and generates a learned discriminator
        NNt-X (S<b>4</b>). Consequently, the process from S<b>5</b> to S<b>10</b>
        is performed, as in the same manner as described above.<br />(47) The processes
        of S<b>2</b> to S<b>10</b> are repeated, and the learning assistance device
        <b>20</b> generates an actually operated discriminator and a learning discriminator
        of which the performance has been improved while generating the correct answer
        data, and distributes the actually operated discriminator and the learning
        discriminator to the terminal device <b>10</b>.<br />(48) As described above,
        by the terminal device <b>10</b> placed in each medical institution performing
        learning using the image data stored in the medical institution, a discriminator
        improving discrimination performance is generated in each medical institution.
        By the learning assistance device <b>20</b> generating the correct answer
        data using the discriminators of which the performance have been improved
        in each medical institution, a large amount of accurate correct answer data
        can be generated, and deep Learning can be performed using this correct answer
        data.<br />(49) Although the case where the mask image of the correct answer
        data for the image data has the information indicating what the area on the
        image is has been described above, the discriminator may be configured to
        (1) determine an organ area and an organ name by determining what organ each
        pixel position of the image data is, (2) determine a lesion area and a type
        of lesion by determining a type of lesion of each pixel in units of pixels
        of the image data. Alternatively, correct answer data for one image may be
        specified as a disease name or an image diagnostic name, and (3) the disease
        name may be specified from the image data.<br />(50) Next, a second embodiment
        will be described. The second embodiment is different from the first embodiment
        in the method of determining the correct answer data. Since a schematic configuration
        of the learning assistance system <b>1</b> is the same as that of the first
        embodiment, detailed description thereof will be omitted. <figref idref=\\\"DRAWINGS\\\">FIG.
        7</figref> is a block diagram illustrating a schematic configuration of a
        terminal device <b>10</b> and a learning assistance device <b>20</b> according
        to the second embodiment. The same configurations as those of the first embodiment
        are denoted by the same reference numerals as those of the first embodiment,
        detailed description thereof will be omitted, and only different configurations
        will be described.<br />(51) As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        7</figref>, the terminal device <b>10</b> includes a discriminator acquisition
        unit <b>12</b>, a discrimination result acquisition unit <b>13</b>, a learning
        unit <b>14</b>, and a learned discriminator output unit <b>15</b>. The learning
        assistance device <b>20</b> includes a learned discriminator acquisition unit
        <b>22</b>, a discriminator storage unit <b>23</b>, a correct answer data acquisition
        unit <b>24</b><i>a</i>, a correct answer data storage unit <b>25</b>, a learning
        unit <b>26</b>, and a discriminator output unit <b>27</b>. A configuration
        of the terminal device <b>10</b> is the same as that of the first embodiment
        except that the correct answer data acquisition unit <b>24</b><i>a </i>of
        the learning assistance device <b>20</b> includes an evaluation unit <b>28</b>
        and an evaluation image storage unit <b>29</b>.<br />(52) In the case where
        the correct answer data is determined by majority vote, sufficient learning
        may be performed in each terminal device <b>10</b> as in the first embodiment,
        but for example, in a case where a discriminator not additionally learned
        in the terminal device <b>10</b> is received as the learned discriminator
        or in a case where a learned discriminator with a small number of additional
        learnings is used, a determination result is likely to be the same for the
        same input image data P, and a determination result of the discriminator in
        which such learning is not sufficiently performed is highly likely to be the
        correct answer data.<br />(53) Therefore, the learning assistance device <b>20</b>
        evaluates the learned discriminators collected from the respective terminal
        device <b>10</b> in advance. The learning assistance device <b>20</b> sets
        a plurality of cases of disease covering a representative case pattern and
        also having correct answer data for image data as an evaluation image set
        SET, and the evaluation unit <b>28</b> evaluates the learned discriminators
        sent from the respective terminal devices <b>10</b> using the image set SET,
        and determines the weight of the discriminator according to a height of the
        correct answer rate. Further, since software intended for medical purposes
        is a target of the Pharmaceutical and Medical Device Act (the revised Pharmaceutical
        Affairs Act), the software is required to meet a criterion prescribed in the
        Pharmaceutical and Medical Device Act. Therefore, it is preferable for an
        evaluation image set SET formed through a combination of a plurality of images
        in which the criterion prescribed in the Pharmaceutical and Medical Device
        Act can be evaluated to be stored in the storage (the evaluation image storage
        unit <b>29</b>) in advance. However, this evaluation image set SET is not
        sufficient for use in deep learning.<br />(54) The weights are determined
        for the respective learned discriminators collected from the terminal device
        <b>10</b> according to the correct answer rate of the evaluation image set,
        the weights of the learned discriminators having the same result among the
        discrimination results are added, and the discrimination result having the
        largest added weight is set as correct answer data of the input image.<br
        />(55) Since a flow of the deep learning process is the same as that of the
        first embodiment, the flow will be omitted.<br />(56) Next, a third embodiment
        will be described. The third embodiment is different from the first and second
        embodiments in the method of determining the correct answer data. Since a
        schematic configuration of the learning assistance system <b>1</b> is the
        same as that of the first embodiment, detailed description thereof will be
        omitted. <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref> is a block diagram
        illustrating a schematic configuration of a terminal device <b>10</b> and
        a learning assistance device <b>20</b> according to the third embodiment.
        The same configurations as those of the first embodiment are denoted by the
        same reference numerals as those of the first embodiment, detailed description
        thereof will be omitted, and only different configurations will be described.<br
        />(57) As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref>, the
        terminal device <b>10</b> includes a discriminator acquisition unit <b>12</b>,
        a discrimination result acquisition unit <b>13</b>, a learning unit <b>14</b>,
        and a learned discriminator output unit <b>15</b><i>a</i>. The learning assistance
        device <b>20</b> includes a learned discriminator acquisition unit <b>22</b><i>a</i>,
        a discriminator storage unit <b>23</b>, a correct answer data acquisition
        unit <b>24</b><i>b</i>, a correct answer data storage unit <b>25</b>, a learning
        unit <b>26</b>, and a discriminator output unit <b>27</b>. The learned discriminator
        output unit <b>15</b><i>a </i>of the terminal device <b>10</b>, and the learned
        discriminator acquisition unit <b>22</b><i>a </i>and the correct answer data
        acquisition unit <b>24</b><i>b </i>of the learning assistance device <b>20</b>
        are different from those of the first embodiment.<br />(58) As in the second
        embodiment, even in a case where the performance of the learned discriminator
        is evaluated, evaluation results using the evaluation image set may be insufficient
        in a case where there is a problem with the number of cases of disease for
        evaluation of the learning assistance device <b>20</b> or coverage of the
        cases of disease. However, in a case where the learned discrimination learns
        a certain number of pieces of image correct answer data, it can be presumed
        that the performance is improving as appropriate.<br />(59) Therefore, in
        a case where the learned discriminator output unit <b>15</b><i>a </i>of the
        terminal device <b>10</b> transmits the parameter of the learned discriminator,
        the learned discriminator output unit <b>15</b><i>a </i>of the terminal device
        <b>10</b> transmits the number of pieces of image correct answer data learned
        by the learned discriminator to the learning assistance device <b>20</b>.<br
        />(60) The learned discriminator acquisition unit <b>22</b><i>a </i>of the
        learning assistance device <b>20</b> receives the number of pieces of image
        correct answer data learned by the learned discriminator at each terminal
        device <b>10</b> together in a case where the learned discriminator acquisition
        unit <b>22</b><i>a </i>of the learning assistance device <b>20</b> receives
        the parameter. The weight is determined so that the weight increases as the
        number of pieces of image correct answer data learned by the correct answer
        data acquisition unit <b>24</b><i>b </i>and the learned discriminators of
        each terminal device <b>10</b> increases. The weights of the learned discriminators
        having the same discrimination result are added, and the discrimination result
        with the largest added weight is set as the correct answer data of the input
        image.<br />(61) Since a flow of deep learning process is the same as that
        of the first embodiment, description thereof is omitted.<br />(62) Next, a
        fourth embodiment will be described. The fourth embodiment is different from
        the first, second, and third embodiments in the method of determining the
        correct answer data. Since a schematic configuration of the learning assistance
        system <b>1</b> is the same as that of the first embodiment, detailed description
        thereof will be omitted. <figref idref=\\\"DRAWINGS\\\">FIG. 9</figref> is
        a block diagram illustrating a schematic configuration of a terminal device
        <b>10</b> and a learning assistance device <b>20</b> according to the fourth
        embodiment. The same configurations as those of the first embodiment are denoted
        by the same reference numerals as those of the first embodiment, detailed
        description thereof will be omitted, and only different configurations will
        be described.<br />(63) As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        9</figref>, the terminal device <b>10</b> includes a discriminator acquisition
        unit <b>12</b>, a discrimination result acquisition unit <b>13</b>, a learning
        unit <b>14</b>, and a learned discriminator output unit <b>15</b><i>b</i>.
        The learning assistance device <b>20</b> includes a learned discriminator
        acquisition unit <b>22</b><i>b</i>, a discriminator storage unit <b>23</b>,
        a correct answer data acquisition unit <b>24</b><i>c</i>, a correct answer
        data storage unit <b>25</b>, a learning unit <b>26</b>, and a discriminator
        output unit <b>27</b>. The learned discriminator output unit <b>15</b><i>b
        </i>of the terminal device <b>10</b> and the learned discriminator acquisition
        unit <b>22</b><i>b </i>and the correct answer data acquisition unit <b>24</b><i>c
        </i>of the learning assistance device <b>20</b> are different from those of
        the first embodiment.<br />(64) The number of pieces of image correct answer
        data for each type of cases of disease is biased due to the characteristics
        of the medical facility, the regional nature, or the like, and the number
        of pieces of image correct answer data is small in any kind of diseases even
        though the number of all pieces of image correct answer data is large, the
        performance is likely not to be improved in the disease. Therefore, the number
        of pieces of image correct answer data learned by the learned discriminator
        of each medical facility is received from each terminal device <b>10</b> for
        each type of cases of disease.<br />(65) Therefore, in a case where the learned
        discriminator output unit <b>15</b><i>b </i>of the terminal device <b>10</b>
        transmits the parameter of the learned discriminator, the learned discriminator
        output unit <b>15</b><i>b </i>of the terminal device <b>10</b> transmits the
        number of pieces of image correct answer data for each type of cases of disease
        learned by the learned discriminator to the learning assistance device <b>20</b>.
        Specifically, it is determined which case of disease the image correct answer
        data learned by the learning unit <b>14</b> of the terminal device <b>10</b>
        relates to, for example, on the basis of a DICOM tag attached to the image
        of image correct answer data, and the number of learned image correct answer
        data is changed for each type of cases of disease. The type of cases of disease
        is classified by disease name (which may be an image diagnostic name in case
        of image inspection) or a type of disease name. In a case where a plurality
        of organs are collectively processed by one discriminator, an organ name may
        be used.<br />(66) The learned discriminator acquisition unit <b>22</b><i>b
        </i>of the learning assistance device <b>20</b> receives the number of pieces
        of image correct answer data for each type of cases of disease learned by
        the learned discriminator at each terminal device <b>10</b> in a case where
        the learned discriminator acquisition unit <b>22</b><i>b </i>of the learning
        assistance device <b>20</b> receives the parameter.<br />(67) In the correct
        answer data acquisition unit <b>24</b><i>c</i>, it is estimated that the performance
        of the learned discriminator is higher as the number of learned image correct
        answer data is larger. To reflect this, the number of pieces of image correct
        answer data learned by each facility for each type of cases of disease is
        counted for the learned discriminator of each terminal device <b>10</b>, and,
        the weight for each type of cases of disease is determined for each learned
        discriminator so that weight is increased as the number increases. In addition,
        weights of the learned discriminators having the same discrimination result
        are added in correspondence to the type of cases of disease of the input image,
        and the discrimination result having the largest added weight is set as the
        correct answer data of the input image.<br />(68) Since a flow of deep learning
        process is the same as that of the first embodiment, description thereof will
        be omitted.<br />(69) With a scheme according to the embodiment, it is possible
        to evaluate the learned discriminator in consideration of, the number of pieces
        of image correct answer data of each medical facility, the type of cases of
        disease, and the like.<br />(70) Further, the evaluation image set according
        to the second embodiment may be set as an evaluation image set capable of
        evaluating a discriminator for each type of disease, and the weight of the
        learned discriminator of each terminal device may be determined according
        to the correct answer rate for each type of disease. The weights of the learned
        discriminators having the same discrimination result may be added according
        to the type of cases of disease of the input image, and the discrimination
        result having the largest added weight may be set as the correct answer data
        of the input image.<br />(71) Further, although the weight is automatically
        determined in the fourth embodiment, the weight may be determined manually
        and stored in the learning assistance device <b>20</b> in advance in consideration
        of importance of the facility, a confident disease of each facility, or the
        like.<br />(72) Although the embodiment in which the learning assistance device
        <b>20</b> and the terminal device <b>10</b> are connected via the network
        has been described in the above description, an image processing program incorporating
        a discriminator functioning as an actually operated discriminator and a learning
        program incorporating a discriminator functioning as a learning discriminator
        may be stored in a recording medium such as a DVD-ROM and distributed to each
        medical institution, instead of over the network.<br />(73) In this embodiment,
        the discriminator acquisition unit <b>12</b> of the terminal device <b>10</b>
        reads the image processing program and the learning program from the DVD-ROM
        to the terminal device <b>10</b> and installs the image processing program
        and the learning program, and reads the identification information ID of the
        image correct answer data used for learning of the learning discriminator
        from the recording medium. Further, the learned discriminator output unit
        <b>15</b> of the terminal device <b>10</b> records the parameter of the learned
        discriminator in the DVD-ROM, and distributes the parameter to an operator
        of the learning assistance device <b>20</b> by mailing or the like.<br />(74)
        Further, the learned discriminator acquisition unit <b>22</b> of the learning
        assistance device <b>20</b> reads the parameters of the learned discriminator
        recorded on the DVD-ROM. Furthermore, the discriminator output unit <b>27</b>
        of the learning assistance device <b>20</b> records the image processing program
        and the learning program on a DVD-ROM and sends the image processing program
        and the learning program to an operator of the terminal device <b>10</b> by
        mailing or the like.<br />(75) As described in detail above, in the present
        invention, accurate correct answer data of an image is automatically generated
        using a discriminator of which the performance has been improved using the
        medical images stored in each medical institution. Thus, it is possible to
        use a large amount of medical images for deep learning.<br />(76) Although
        the case where the learning assistance device and the terminal device function
        on a general-purpose computer has been described above, a dedicated circuit
        such as an application specific integrated circuit (ASIC) or field programmable
        gate arrays (FPGA) that permanently stores a program for executing some of
        functions may be provided. Alternatively, a program instruction stored in
        a dedicated circuit and a program instruction executed by a general-purpose
        CPU programmed to use a program of a dedicated circuit may be combined. As
        described above, the program instructions may be executed through any combination
        of hardware configurations of the computer.\",\"claimsHtml\":\"1. A learning
        assistance device comprising a processor configured to: output learning discriminators
        to a plurality of respective terminal devices; acquire a plurality of learned
        discriminators obtained by causing each of the learning discriminators provided
        in a plurality of respective terminal devices to perform learning at the respective
        terminal devices by using an image and correct answer data thereof stored
        in the respective terminal devices; and acquire a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, each of the plurality of discrimination
        results being obtained from each of the plurality of learned discriminators,
        determine correct answer data of the input image on the basis of the plurality
        of discrimination results, and output a new learning discriminator to the
        respective terminal device, the new learning discriminator being obtained
        by causing the learning discriminator to perform learning again using the
        input image and the determined correct answer data, wherein the processor
        repeatedly performs a process of acquiring the plurality of learned discriminators
        obtained by causing each of the learning discriminators, each of the learning
        discriminators being output from the learning assistance device to the respective
        terminal devices and being provided for the respective terminal devices, to
        perform learning at the respective terminal devices by using an image and
        correct answer data thereof stored in the respective terminal devices, determining
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired from the plurality of respective
        terminal devices to discriminate the new same input image, and outputting
        a new learning discriminator to the respective terminal device, the new learning
        discriminator being obtained by causing the learning discriminator output
        from the learning assistance device to the respective terminal devices at
        a last time to perform learning again using the new input image and the determined
        new correct answer data at the learning assistance device.  <br /> 2. The
        learning assistance device according to claim 1, wherein the processor outputs
        an actually operated discriminator learning the image and the correct answer
        data thereof used for learning by the new learning discriminator. <br /> 3.
        The learning assistance device according to claim 1, wherein the processor
        acquires the learned discriminator from the plurality of terminal devices
        over a network, and the processor outputs the new learning discriminator to
        the plurality of terminal devices over the network.  <br /> 4. The learning
        assistance device according to claim 1, wherein the processor determines a
        discrimination result having the largest number of same results among the
        plurality of discrimination results, as correct answer data of the input image.
        <br /> 5. The learning assistance device according to claim 1, wherein the
        processor determines a weight of each of the plurality of learned discriminators
        according to the terminal device learned by the learned discriminator, adds
        the weights of the learned discriminators having the same result among the
        discrimination results, and sets a discrimination result having the largest
        added weight as correct answer data of the input image. <br /> 6. The learning
        assistance device according to claim 5, wherein the processor determines the
        weight of each of the learned discriminators learned at each of the terminal
        devices according to the number of pieces of correct answer data learned by
        the learned discriminator at each terminal device, adds the weights of the
        learned discriminators having the same discrimination result, and sets a discrimination
        result having the largest added weight as correct answer data of the input
        image. <br /> 7. The learning assistance device according to claim 5, wherein
        the processor determines weights for types of cases of disease of the image
        learned by the respective learned discriminators with respect to the respective
        learned discriminators, adds the weights corresponding to the types of cases
        of disease of the input image in the learned discriminator having the same
        discrimination result, and sets a discrimination result having the largest
        added weight as correct answer data of the input image. <br /> 8. The learning
        assistance device according to claim 5, wherein the processor: evaluates a
        correct answer rate using an image set including a plurality of images serving
        as a reference with respect to the plurality of learned discriminators, and
        wherein the processor determines the weight of each of the plurality of learned
        discriminators according to the correct answer rate that is obtained, adds
        the weights of the respective learned discriminators having the same discrimination
        results, and sets the discrimination result having the largest added weight
        as correct answer data of the input image.  <br /> 9. A method of operating
        a learning assistance device including a processor, the method comprising:
        outputting learning discriminators to a plurality of respective terminal devices;
        acquiring a plurality of learned discriminators obtained by causing each of
        the learning discriminators provided in a plurality of respective terminal
        devices to perform learning at the respective terminal devices by using an
        image and correct answer data thereof stored in the respective terminal devices;
        acquiring a plurality of discrimination results obtained by causing each of
        the plurality of learned discriminators to discriminate the same input image,
        each of the plurality of discrimination results being obtained from each of
        the plurality of learned discriminators, determining correct answer data of
        the input image on the basis of the plurality of discrimination results, and
        outputting a new learning discriminator to the respective terminal device,
        the new learning discriminator being obtained by causing the learning discriminator
        to perform learning again using the input image and the determined correct
        answer data; and repeatedly performing a process of acquiring the plurality
        of learned discriminators obtained by causing each of the learning discriminators,
        each of the learning discriminators being output from the learning assistance
        device to the respective terminal devices and being provided for the respective
        terminal devices, to perform learning at the respective terminal devices by
        using an image and correct answer data thereof stored in the respective terminal
        devices, determining a new correct answer data of a new same input image different
        from the input image on the basis of a plurality of discrimination results
        obtained by causing the plurality of learned discriminators acquired from
        the respective terminal devices to discriminate the new same input image,
        and outputting a new learning discriminator to the respective terminal device,
        the new learning discriminator being obtained by causing the learning discriminator
        output from the learning assistance device to the respective terminal devices
        at a last time to perform learning again using the new input image and the
        determined new correct answer data at the learning assistance device.  <br
        /> 10. A non-transitory computer-readable recording medium storing therein
        a learning assistance program causing a computer to: output learning discriminators
        to a plurality of respective terminal devices; acquire a plurality of learned
        discriminators obtained by causing each of the learning discriminators provided
        in a plurality of respective terminal devices to perform learning at the respective
        terminal devices by using an image and correct answer data thereof stored
        in the respective terminal devices; and acquire a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, each of the plurality of discrimination
        results being obtained from each of the plurality of learned discriminators,
        determine correct answer data of the input image on the basis of the plurality
        of discrimination results, and output a new learning discriminator to the
        respective terminal device, the new learning discriminator being obtained
        by causing the learning discriminator to perform learning again using the
        input image and the determined correct answer data, repeatedly perform a process
        of acquiring the plurality of learned discriminators obtained by causing each
        of the learning discriminators, each of the learning discriminators being
        output from the learning assistance device to the respective terminal devices
        and being provided for the respective terminal devices, to perform learning
        at the respective terminal devices by using an image and correct answer data
        thereof stored in the respective terminal devices, determine a new correct
        answer data of a new same input image different from the input image on the
        basis of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired from the plurality of respective terminal
        devices to discriminate the new same input image, and output a new learning
        discriminator to the respective terminal device, the new learning discriminator
        being obtained by causing the learning discriminator output from the learning
        assistance device to the respective terminal devices at a last time to perform
        learning again using the new input image and the determined new correct answer
        data at the learning assistance device.\",\"briefHtml\":\"CROSS-REFERENCE
        TO RELATED APPLICATION<br />(1) This application claims priority from Japanese
        Patent Application No. 2017-187096, filed on Sep. 27, 2017, the disclosure
        of which is incorporated by reference herein in its entirety.<br />BACKGROUND<br
        />Field of the Invention<br />(2) The present invention relates to a learning
        assistance device that assists in generation of a discriminator using machine
        learning, a method of operating the learning assistance device, a learning
        assistance program, a learning assistance system, and a terminal device.<br
        />Related Art<br />(3) In the related art, machine learning has been used
        to learn features of data and perform recognition or classification of images
        or the like. In recent years, various learning schemes have been developed,
        a processing time has been shortened due to an improved processing capability
        of a computer, and deep learning in which a system learns features of image
        data or the like at a deeper level can be performed. By performing the deep
        learning, features of images or the like can be recognized with very high
        accuracy, and improvement of performance of discrimination is expected.<br
        />(4) In the medical field, artificial intelligence (AI) that recognizes features
        of images with high accuracy by performing learning using deep learning is
        desired. For deep learning, it is indispensable to perform learning using
        a large amount of high quality data according to purposes. Therefore, it is
        important to prepare learning data efficiently. Image data of a large number
        of cases of disease is accumulated in each medical institution with the spread
        of a picture archiving and communication system (PACS). Therefore, learning
        using image data of various cases of disease accumulated in each medical institution
        is considered.<br />(5) Further, in recent years, a technical level of artificial
        intelligence has been improving in a variety of fields, and the artificial
        intelligence is incorporated into a variety of services and started to be
        used and utilized. In particular, services provided to various edge terminals
        over a network are increasing. For example, JP2008-046729A discloses a device
        in which a learning model is incorporated into a moving image topic division
        device that automatically divides a moving image at a switching point of a
        topic. JP2008-046729A discloses that the learning model has been distributed
        to a client terminal, the topic division is automatically executed using the
        distributed learning model at each client terminal, and content corrected
        by a user for a result of the automatic topic division is fed back for updating
        of the learning model. After the feedback corrected by the user is accumulated
        in an integration module over a network, a learning model reconstructed using
        the accumulated feedback is distributed to each client terminal over the network
        again.<br />(6) However, in the medical field, since data to be learned is
        medical data of a patient, confidentiality is very high, and it is necessary
        to handle data carefully for use as learning data. Further, correct answer
        data is not attached to image data. Alternatively, even in a case where there
        is the correct answer data, the correct answer data is often managed without
        being associated with original image data. Therefore, it is difficult and
        costly to efficiently collect learning data to which the correct answer data
        is added.<br />SUMMARY<br />(7) Therefore, in order to solve the above-described
        problems, an object of the present invention is to provide a learning assistance
        device which enables learning of a large amount and a variety of learning
        data necessary for deep learning in a medical field, a method of operating
        the learning assistance device, a learning assistance program, a learning
        assistance system, and a terminal device.<br />(8) A learning assistance device
        of the present invention comprises: a learned discriminator acquisition unit
        that acquires a plurality of learned discriminators obtained by causing each
        of learning discriminators provided in a plurality of respective terminal
        devices to perform learning using an image and correct answer data thereof;
        and a discriminator output unit that acquires a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, determines correct answer data of the
        input image on the basis of the plurality of discrimination results, and outputs
        a new learning discriminator obtained by causing the learning discriminator
        to perform learning again using the input image and the determined correct
        answer data, wherein the learning assistance device repeatedly performs a
        process in which the learning discriminator acquisition unit acquires the
        plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, and the discriminator output unit determines a new correct answer
        data of a new same input image different from the input image on the basis
        of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired by the learned discriminator acquisition
        unit to discriminate the new same input image, and outputs a new learning
        discriminator obtained by causing the learning discriminator output by the
        discriminator output unit to perform learning again using the new input image
        and the determined new correct answer data.<br />(9) A method of operating
        a learning assistance device of the present invention is a method of operating
        a learning assistance device including a learned discriminator acquisition
        unit and a discriminator output unit, the method comprising: acquiring a plurality
        of learned discriminators obtained by causing each of learning discriminators
        provided in a plurality of respective terminal devices to perform learning
        using an image and correct answer data thereof by the learned discriminator
        acquisition unit; acquiring a plurality of discrimination results obtained
        by causing each of the plurality of learned discriminators to discriminate
        the same input image, determines correct answer data of the input image on
        the basis of the plurality of discrimination results, and outputs a new learning
        discriminator obtained by causing the learning discriminator to perform learning
        again using the input image and the determined correct answer data by the
        discriminator output unit; and repeatedly performing a process in which the
        learned discriminator acquisition unit acquires the plurality of learned discriminators
        obtained by causing the each of learning discriminators, the each of learning
        discriminators being output from the discriminator output unit and being provided
        for each of the plurality of terminal devices, to perform learning using an
        image and correct answer data thereof, and the discriminator output unit determines
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired by the learned discriminator
        acquisition unit to discriminate the new same input image, and outputs a new
        learning discriminator obtained by causing the learning discriminator output
        by the discriminator output unit to perform learning again using the new input
        image and the determined new correct answer data.<br />(10) A learning assistance
        program according to the present invention causes a computer to function as:
        a learned discriminator acquisition unit that acquires a plurality of learned
        discriminators obtained by causing each of learning discriminators provided
        in a plurality of respective terminal devices to perform learning using an
        image and correct answer data thereof; and a discriminator output unit that
        acquires a plurality of discrimination results obtained by causing each of
        the plurality of learned discriminators to discriminate the same input image,
        determines correct answer data of the input image on the basis of the plurality
        of discrimination results, and outputs a new learning discriminator obtained
        by causing the learning discriminator to perform learning again using the
        input image and the determined correct answer data, wherein the program causes
        a process in which the learned discriminator acquisition unit acquires the
        plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, and the discriminator output unit determines a new correct answer
        data of a new same input image different from the input image on the basis
        of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired by the learned discriminator acquisition
        unit to discriminate the new same input image, and outputs a new learning
        discriminator obtained by causing the learning discriminator output by the
        discriminator output unit to perform learning again using the new input image
        and the determined new correct answer data, to be repeatedly performed.<br
        />(11) \u201CAcquire a learned discriminator\u201D may be acquiring the learned
        discriminator or may be receiving a parameter of the discriminator and setting
        the parameter in a prepared discriminator to acquire the learned discriminator.
        For example, in a case where the discriminator is a multilayered neural network,
        a program in which a learned multilayered neural network is incorporated may
        be acquired or a weight of coupling between layers of units of the multilayered
        neural network may be acquired as a parameter and the parameter may be set
        in the prepared multilayered neural network, so that the learned multilayered
        neural network can be acquired.<br />(12) Further, the discriminator output
        unit further outputs an actually operated discriminator learning the image
        and the correct answer data thereof used for learning by the new learning
        discriminator.<br />(13) The \u201Cactually operated discriminator\u201D is
        a discriminator capable of acquiring a discrimination result of input image
        data, which cannot perform additional learning, and the \u201Clearning discriminator\u201D
        is a discriminator that can perform additional learning using an image and
        correct answer data of the image. Further, the actually operated discriminator
        to be output is a discriminator caused to perform learning using an image
        and correct answer data of the image that are all the same as the image and
        the correct answer data of the image learned by the output learning discriminator.<br
        />(14) Further, the learned discriminator acquisition unit may acquire the
        learned discriminator from the plurality of terminal devices over a network,
        and the discriminator output unit may output the new learning discriminator
        to the plurality of terminal devices over the network.<br />(15) Further,
        the discriminator output unit may determine a discrimination result having
        the largest number of same results among the plurality of discrimination results,
        as correct answer data of the input image.<br />(16) Further, the discriminator
        output unit may determine a weight of each of the plurality of learned discriminators
        according to the terminal device learned by the learned discriminator, adds
        the weights of the learned discriminators having the same result among the
        discrimination results, and sets a discrimination result having the largest
        added weight as correct answer data of the input image.<br />(17) Further,
        the discriminator output unit may determine the weight of each of the learned
        discriminators learned at each of the terminal devices according to the number
        of pieces of correct answer data learned by the learned discriminator at each
        terminal device, add the weights of the learned discriminators having the
        same discrimination result, and set a discrimination result having the largest
        added weight as correct answer data of the input image.<br />(18) Further,
        the discriminator output unit may determine weights for types of cases of
        disease of the image learned by the respective learned discriminators with
        respect to the respective learned discriminators, add the weights corresponding
        to the types of cases of disease of the input image in the learned discriminator
        having the same discrimination result, and set a discrimination result having
        the largest added weight as correct answer data of the input image.<br />(19)
        Further, the learning assistance device may further comprise an evaluation
        unit that evaluates a correct answer rate using an image set including a plurality
        of images serving as a reference with respect to the plurality of learned
        discriminators, wherein the discriminator output unit may determine the weight
        of each of the plurality of learned discriminators according to the correct
        answer rate obtained by the evaluation unit, add the weights of the respective
        learned discriminators having the same discrimination results, and set the
        discrimination result having the largest added weight as correct answer data
        of the input image.<br />(20) A learning assistance system according to the
        present invention is a learning assistance system in which a learning assistance
        device and a plurality of terminal devices are connected over a network, wherein
        the terminal device includes a learned discriminator output unit that outputs
        a learned discriminator obtained by causing a learning discriminator to perform
        learning using an image and correct answer data thereof over the network,
        the learning assistance device includes a learned discriminator acquisition
        unit that acquires a plurality of learned discriminators from the plurality
        of terminal devices over the network, and a discriminator output unit that
        acquires a plurality of discrimination results obtained by causing the plurality
        of learned discriminators to discriminate the same input image, determines
        correct answer data of the input image on the basis of the plurality of discrimination
        results, and outputs a new learning discriminator obtained by causing the
        learning discriminator to perform learning again using the input image and
        the determined correct answer data to the plurality of terminal devices over
        the network, and the terminal device includes a discriminator acquisition
        unit that receives the learning discriminator output from the learning assistance
        device over the network.<br />(21) Further, in the learning assistance system,
        the discriminator acquisition unit may further acquire an actually operated
        discriminator learning the same image and correct answer data of the image
        as those of the learning discriminator output from the learning assistance
        device over a network, and the terminal device may further include a discrimination
        result acquisition unit that acquires a discrimination result of discriminating
        an image that is a discrimination target using the actually operated discriminator.<br
        />(22) A terminal device according to the present invention comprises a discriminator
        acquisition unit that acquires a learning discriminator, and a learned actually
        operated discriminator learned using the same image and correct answer data
        of the image as those of the learning discriminator; a discrimination result
        acquisition unit that acquires a discrimination result of discriminating an
        image that is a discrimination target using the actually operated discriminator;
        and a learned discriminator output unit that outputs a learned discriminator
        obtained by causing the learning discriminator to perform learning using an
        image and correct answer data thereof.<br />(23) Further, in the terminal
        device, the discriminator acquisition unit may acquire the learning discriminator
        and the actually operated discriminator from a learning assistance device
        over a network, and the learned discriminator output unit may send and output
        the learned discriminator over the network.<br />(24) Another learning assistance
        device of the present invention comprises a memory that stores instructions
        to be executed by a computer, and a processor configured to execute the stored
        instructions, wherein the processor executes an acquisition process of acquiring
        a plurality of learned discriminators obtained by causing each of learning
        discriminators provided in a plurality of respective terminal devices to perform
        learning using an image and correct answer data thereof, and an output process
        of acquiring a plurality of discrimination results obtained by causing each
        of the plurality of learned discriminators to discriminate the same input
        image, determining correct answer data of the input image on the basis of
        the plurality of discrimination results, and outputting a new learning discriminator
        obtained by causing the learning discriminator to perform learning again using
        the input image and the determined correct answer data, and the learning assistance
        device repeatedly performs a process of acquiring the plurality of learned
        discriminators obtained by causing the each of learning discriminators, the
        each of learning discriminators being output from the discriminator output
        unit and being provided for each of the plurality of terminal devices, to
        perform learning using an image and correct answer data thereof, determining
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired by the learned discriminator
        acquisition unit to discriminate the new same input image, and outputting
        a new learning discriminator obtained by causing the learning discriminator
        output by the discriminator output unit to perform learning again using the
        new input image and the determined new correct answer data.<br />(25) According
        to the present invention, the learning assistance device acquires the plurality
        of learned discriminators obtained by causing learning discriminators provided
        in a plurality of respective terminal devices to perform learning using image
        correct answer data, acquires the plurality of discrimination results of discriminating
        the same input image, determines the correct answer data of the input image
        on the basis of the plurality of discrimination results, and causes the discriminator
        to perform learning using the input image and the determined correct answer
        data. Therefore, it is possible to automatically generate the data for learning
        from the image to which the correct answer data is not attached, perform deep
        learning using a large amount of images, and improve performance of discrimination.\",\"backgroundTextHtml\":null,\"subHeadingM0Html\":null,\"subHeadingM1Html\":null,\"subHeadingM2Html\":null,\"subHeadingM3Html\":null,\"subHeadingM4Html\":null,\"subHeadingM5Html\":null,\"subHeadingM6Html\":null,\"usClassIssued\":null,\"issuedUsDigestRefClassifi\":null,\"datePublYear\":\"2021\",\"applicationYear\":\"2018\",\"pfDerwentWeekYear\":null,\"pfApplicationYear\":null,\"pfPublYear\":null,\"reissueApplNumber\":null,\"abstractHeader\":null,\"abstractedPublicationDerwent\":null,\"affidavit130BFlag\":null,\"affidavit130BText\":null,\"applicantGroup\":[\"FUJIFILM
        Corporation Tokyo JP\"],\"applicantHeader\":null,\"applicationFilingDateInt\":20180906,\"applicationFilingDateIntKwicHits\":null,\"applicationRefFilingType\":\"utility\",\"applicationReferenceGroup\":null,\"applicationSeriesAndNumber\":\"16123456\",\"applicationSeriesCode\":\"16\",\"assignee1\":null,\"assigneeDescriptiveText\":null,\"patentAssigneeTerms\":null,\"associateAttorneyName\":null,\"attorneyName\":[\"Birch,
        Stewart, Kolasch & Birch, LLP\"],\"biologicalDepositInformation\":null,\"applicationType\":null,\"unlinkedDerwentRegistryNumber\":null,\"unlinkedRingIndexNumbersRarerFragments\":null,\"claimStatement\":\"What
        is claimed is:\",\"claimsTextAmended\":null,\"continuedProsecutionAppl\":null,\"cpcAdditionalLong\":null,\"cpcCisClassificationOrig\":null,\"cpcCombinationClassificationOrig\":null,\"cpcInventive\":[\"G16H30/40
        20180101\",\"G06N3/08 20130101\",\"G06V10/7784 20220101\",\"G06K9/6256 20130101\",\"G06N20/00
        20190101\",\"G16H30/20 20180101\",\"G06V10/774 20220101\",\"G06K9/6263 20130101\",\"G16H50/70
        20180101\",\"G06N3/0454 20130101\",\"G06N3/0427 20130101\",\"G06K9/6262 20130101\"],\"cpcInventiveCurrentDateKwicHits\":null,\"cpcAdditional\":null,\"cpcAdditionalCurrentDateKwicHits\":null,\"cpcOrigClassificationGroup\":[\"G
        G06K G06K9/6256 20130101 F I 20210126 US\",\"G G06K G06K9/6263 20130101 L
        I 20210126 US\",\"G G06N G06N3/0427 20130101 L I 20210126 US\",\"G G06N G06N3/0454
        20130101 L I 20210126 US\",\"G G06N G06N3/08 20130101 L I 20210126 US\",\"G
        G16H G16H30/40 20180101 L I 20210126 US\",\"G G16H G16H50/70 20180101 L I
        20210126 US\"],\"curIntlPatentClassificationGroup\":null,\"curUsClassificationUsPrimaryClass\":null,\"curUsClassificationUsSecondaryClass\":null,\"customerNumber\":null,\"depositAccessionNumber\":null,\"depositDescription\":null,\"derwentClassAlpha\":null,\"designatedstatesRouteGroup\":null,\"docAccessionNumber\":null,\"drawingDescription\":null,\"editionField\":null,\"exchangeWeek\":null,\"exemplaryClaimNumber\":[\"1\"],\"familyIdentifierOrig\":null,\"fieldOfSearchCpcClassification\":[\"G06K
        9/6256\",\"G06K 9/6263\",\"G16H 30/40\",\"G16H 50/70\",\"G06N 3/08\",\"G06N
        3/0454\",\"G06N 3/0427\"],\"fieldOfSearchCpcMainClass\":[\"G06K\",\"G06K\",\"G16H\",\"G16H\",\"G06N\",\"G06N\",\"G06N\"],\"fieldOfSearchIpcMainClass\":null,\"fieldOfSearchIpcMainClassSubclass\":null,\"fieldOfSearchSubclasses\":[\"159\"],\"foreignRefGroup\":[\"JP
        2001-319226 A 20011100 cited by applicant\",\"JP 2008-46729 A 20080200 cited
        by applicant\"],\"foreignRefPubDate\":[\"20011100\",\"20080200\"],\"foreignRefPubDateKwicHits\":[\"20011100\",\"20080200\"],\"foreignRefCitationClassification\":[\"N/A\",\"N/A\"],\"foreignRefPatentNumber\":[\"2001-319226\",\"2008-46729\"],\"foreignRefCitationCpc\":[\"N/A\",\"N/A\"],\"foreignRefCountryCode\":[\"JP\",\"JP\"],\"iceXmlIndicator\":\"Y\",\"internationalClassificationHeader\":null,\"internationalClassificationInformationalGroup\":null,\"intlPubClassificationGroup\":[\"20060101
        A G06K G06K9/62 F I B US H 20210126\",\"20060101 A G06N G06N3/04 L I B US
        H 20210126\",\"20180101 A G16H G16H50/70 L I B US H 20210126\",\"20060101
        A G06N G06N3/08 L I B US H 20210126\",\"20180101 A G16H G16H30/40 L I B US
        H 20210126\"],\"intlPubClassificationNonInvention\":null,\"inventorCitizenship\":null,\"inventorCorrection\":null,\"inventorDeceased\":null,\"inventorStreetAddress\":null,\"inventorText\":null,\"jpoFiClassification\":null,\"legalRepresentativeCity\":null,\"legalRepresentativeCountry\":\"[]\",\"legalRepresentativeName\":null,\"legalRepresentativePostcode\":null,\"legalRepresentativeState\":null,\"legalRepresentativeStreetAddress\":null,\"legalRepresentativeText\":null,\"messengerDocsFlag\":null,\"newRecordPatentDerwent\":null,\"numberOfClaims\":\"10\",\"numberOfDrawingSheets\":\"9\",\"numberOfFigures\":\"9\",\"numberOfPagesInSpecification\":null,\"numberOfPagesOfSpecification\":null,\"objectContents\":null,\"objectDescription\":null,\"parentDocCountry\":null,\"parentGrantDocCountry\":null,\"patentBibliographicHeader\":null,\"pctOrRegionalPublishingSerial\":null,\"pfDerwentWeekNum\":null,\"principalAttorneyName\":null,\"priorityApplicationCountry\":null,\"priorityClaimsCountry\":[\"JP\"],\"priorityNumberDerived\":[\"2017JP-2017-187096\"],\"publicationIssueNumber\":null,\"refCitedPatentDocNumber\":null,\"refCitedPatentDocDate\":null,\"refCitedPatentDocKindCode\":null,\"referenceCitedCode\":null,\"referenceCitedGroup\":null,\"referenceCitedSearchPhase\":null,\"referenceCitedText\":null,\"registrationNumber\":null,\"reissueApplCountry\":null,\"reissueParentKind\":null,\"reissueParentNumber\":null,\"reissueParentPubCountry\":null,\"reissuePatentGroup\":null,\"reissuePatentParentStatus\":null,\"reissuedPatentApplCountry\":null,\"reissuedPatentApplKind\":null,\"reissuedPatentApplNumber\":null,\"relatedApplChildPatentCountry\":null,\"relatedApplChildPatentName\":null,\"relatedApplChildPatentNumber\":null,\"relatedApplCountryCode\":null,\"relatedApplParentGrantPatentKind\":null,\"relatedApplParentGrantPatentName\":null,\"relatedApplParentPatentKind\":null,\"relatedApplParentPatentName\":null,\"relatedApplParentPctDoc\":null,\"relatedApplParentStatusCode\":null,\"relatedApplPatentNumber\":null,\"relatedApplRelatedPub\":null,\"relatedApplTypeOfCorrection\":null,\"rule47Flag\":null,\"selectedDrawingCharacter\":null,\"selectedDrawingFigure\":null,\"statutoryInventionText\":null,\"termOfExtension\":\"127\",\"termOfPatentGrant\":null,\"titleTermsData\":null,\"additionalIndexingTerm\":null,\"applicationYearSearch\":\"2018\",\"pfApplicationYearSearch\":null,\"assigneeCountry\":[\"JP\"],\"certOfCorrectionFlag\":null,\"citedPatentLiteratureAddressInformation\":null,\"citedPatentLiteratureClassificationIpc\":null,\"citedPatentLiteratureOrganizationName\":null,\"citedPatentLiteratureRefNumber\":null,\"crossReferenceNumber\":null,\"country\":\"US\",\"cpiManualCodes\":null,\"cpiSecondaryAccessionNumber\":null,\"curIntlPatentAllClassificationLong\":null,\"currentUsOriginalClassificationLong\":null,\"datePublSearch\":\"2021-01-26T00:00:00.000+00:00\",\"datePublYearSearch\":\"2021\",\"epiManualCodes\":null,\"fieldOfSearchMainClassNational\":[\"382\"],\"inventorCountry\":[\"JP\"],\"ipcAllMainClassification\":[\"G06N\",\"G16H\",\"G06K\"],\"issuedUsClassificationFull\":null,\"issuedUsDigestRefClassification\":null,\"jpoFiCurrentAdditionalClassification\":null,\"jpoFiCurrentInventiveClassification\":null,\"legalFirmName\":[\"Birch,
        Stewart, Kolasch & Birch, LLP\"],\"locarnoMainClassification\":null,\"nonCpiSecondaryAccessionNumber\":null,\"objectId\":null,\"otherRefPub\":[\"Japanese
        Office Action for corresponding Japanese Application No. 2017-187096, dated
        Jul. 21, 2020, with English translation. cited by applicant\\n<br />\"],\"pageNumber\":null,\"patentAssigneeCode\":null,\"patentAssigneeNameTotal\":null,\"patentFamilyDate\":null,\"patentFamilyDocNumber\":null,\"patentFamilyKind\":null,\"patentFamilyKindCode\":null,\"patentFamilyLanguage\":null,\"patentFamilyName\":null,\"patentNumberOfLocalApplication\":null,\"pct102eDate\":null,\"pct371c124Date\":null,\"pct371c124DateKwicHits\":null,\"pctFilingDate\":null,\"pctFilingDateKwicHits\":null,\"pctFilingDocCountryCode\":null,\"pctFilingKind\":null,\"pctFilingNumber\":null,\"pctName\":null,\"pctOrRegionalPublishingCountry\":null,\"pctOrRegionalPublishingKind\":null,\"pctOrRegionalPublishingName\":null,\"pctOrRegionalPublishingText\":null,\"pctPubDate\":null,\"pctPubDateKwicHits\":null,\"pctPubDocIdentifier\":null,\"pctPubNumber\":null,\"pfApplicationDateSearch\":null,\"pfApplicationType\":null,\"pfDerwentWeekDate\":null,\"pfPublDateSearch\":null,\"pfPublDateSearchKwicHits\":null,\"pfPublYearSearch\":null,\"polymerIndexingCodes\":null,\"polymerMultipunchCodeRecordNumber\":null,\"polymerMultipunchCodes\":null,\"priorPublishedDocCountryCode\":[\"US\"],\"priorPublishedDocDate\":[\"2019-03-28T00:00:00Z\"],\"priorPublishedDocDateKwicHits\":null,\"priorPublishedDocIdentifier\":[\"US
        20190095759 A1\"],\"priorPublishedDocKindCode\":[\"A1\"],\"priorPublishedDocNumber\":[\"20190095759\"],\"priorityApplYear\":[\"2017\"],\"priorityApplicationDate\":null,\"priorityClaimsDateSearch\":null,\"priorityClaimsDocNumber\":[\"2017-187096\"],\"priorityPatentDid\":null,\"priorityPatentNumber\":null,\"ptabCertFlag\":null,\"pubRefCountryCode\":null,\"pubRefDocNumber\":\"10902286\",\"pubRefDocNumber1\":\"10902286\",\"publicationData\":null,\"recordPatentNumber\":null,\"reexaminationFlag\":null,\"refCitedOthers\":null,\"refCitedPatentDocCountryCode\":null,\"refCitedPatentDocName\":null,\"refCitedPatentRelevantPassage\":null,\"reissueParentIssueDate\":null,\"reissuedPatentApplFilingDate\":null,\"relatedAccessionNumbers\":null,\"relatedApplChildPatentDate\":null,\"relatedApplFilingDateKwicHits\":null,\"relatedApplNumber\":null,\"relatedApplPatentIssueDate\":null,\"relatedApplPatentIssueDateKwicHits\":null,\"relatedDocumentKindCode\":null,\"securityLegend\":null,\"sequenceCwu\":null,\"sequenceListNewRules\":null,\"sequenceListOldRules\":null,\"sequencesListText\":null,\"standardTitleTerms\":null,\"supplementalExaminationFlag\":null,\"usBotanicLatinName\":null,\"usBotanicVariety\":null,\"usRefClassification\":[\"382/226\",\"N/A\"],\"usRefCpcClassification\":[\"G06K
        9/00228\",\"N/A\"],\"usRefGroup\":[\"US 2007/0217688 A1 Sabe 20070900 cited
        by examiner G06K 9/00228 382/226\",\"US 2015/0242760 A1 Miao et al. 20150800
        cited by applicant\"],\"usRefIssueDate\":[\"20070900\",\"20150800\"],\"usRefIssueDateKwicHits\":[\"20070900\",\"20150800\"],\"usRefPatenteeName\":[\"Sabe\",\"Miao
        et al.\"],\"volumeNumber\":null,\"correspondenceNameAddress\":null,\"correspondenceAddressCustomerNumber\":null,\"ibmtdbAccessionNumber\":null,\"inventorsName\":[\"Kanada;
        Shoji\"],\"applicationKindCode\":\"B2\",\"inventorNameDerived\":null,\"intlPubClassificationClass\":[\"G06K\",\"G06N\",\"G16H\",\"G06N\",\"G16H\"],\"issuedUsOrigClassification\":null,\"curCpcSubclassFull\":[\"G06N\",\"G16H\",\"G06V\",\"G06K\"],\"cpcCurAdditionalClass\":null,\"cpcCurInventiveClass\":[\"G16H\",\"G06N\",\"G06V\",\"G06K\",\"G06N\",\"G16H\",\"G06V\",\"G06K\",\"G16H\",\"G06N\",\"G06N\",\"G06K\"],\"cpcCurClassificationGroup\":[\"G
        G16H G16H30/40 20180101 L I B H 20191011 US\",\"G G06N G06N3/08 20130101 L
        I B H 20191011 US\",\"G G06V G06V10/7784 20220101 L I B H 20220712 EP\",\"G
        G06K G06K9/6256 20130101 F I B H 20190822 US\",\"G G06N G06N20/00 20190101
        L I B C 20210127 US\",\"G G16H G16H30/20 20180101 L I B H 20210127 US\",\"G
        G06V G06V10/774 20220101 L I B H 20220715 EP\",\"G G06K G06K9/6263 20130101
        L I B H 20191011 US\",\"G G16H G16H50/70 20180101 L I B H 20191011 US\",\"G
        G06N G06N3/0454 20130101 L I B H 20191011 US\",\"G G06N G06N3/0427 20130101
        L I B H 20191011 US\",\"G G06K G06K9/6262 20130101 L I B H 20210127 US\"],\"curCpcClassificationFull\":[\"G06N20/00
        20190101\",\"G06V10/774 20220101\",\"G16H30/20 20180101\",\"G06K9/6263 20130101\",\"G06N3/08
        20130101\",\"G16H50/70 20180101\",\"G16H30/40 20180101\",\"G06K9/6256 20130101\",\"G06K9/6262
        20130101\",\"G06V10/7784 20220101\",\"G06N3/0454 20130101\",\"G06N3/0427 20130101\"],\"cpcCombinationClassificationCur\":null,\"cpcCombinationTallyCur\":null,\"intlFurtherClassification\":null,\"currentUsPatentClass\":[\"1\"],\"internationalClassificationInfom\":null,\"cpcOrigInventvClssifHlghts\":[\"G06K9/6256
        20130101\",\"G06K9/6263 20130101\",\"G06N3/0427 20130101\",\"G06N3/0454 20130101\",\"G06N3/08
        20130101\",\"G16H30/40 20180101\",\"G16H50/70 20180101\"],\"idWithoutSolrPartition\":\"US-US-10902286\",\"curIntlPatentClassifictionPrimaryDateKwicHits\":null,\"curIntlPatentClssifSecHlights\":null,\"publicationReferenceDocumentNumberOne\":\"10902286\",\"descriptionStart\":11,\"descriptionEnd\":18}"
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:52:36 GMT
      Server-Timing:
      - intid;desc=c2f7a05bd20b1ad7
    status:
      code: 200
      message: ''
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 2168779, "hl_snippets": "2", "op": "OR", "q": "\"10902286\".PN.",
      "queryName": "\"10902286\".PN.", "highlights": "1", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "USPAT", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "\"10902286\".PN."}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '715'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":2168779,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"USPAT","countryCodes":[]}],"q":"\"10902286\".PN.","queryName":"\"10902286\".PN.","userEnteredQuery":"\"10902286\".PN.","viewName":"tile","op":"OR","highlights":"1","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"\"10902286\".PN.","error":null,"terms":["\"10902286\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":33,"highlightingTime":0,"cursorMarker":"AoJwwK3/nfcCNzY1ODA3Njk2IVVTLVVTLTEwOTAyMjg2","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-10902286-B2","publicationReferenceDocumentNumber":"10902286","compositeId":"65807696!US-US-10902286","publicationReferenceDocumentNumber1":"10902286","datePublishedKwicHits":null,"datePublished":"2021-01-26T00:00:00Z","inventionTitle":"Learning
        assistance device, method of operating learning assistance device, learning
        assistance program, learning assistance system, and terminal device","type":"USPAT","mainClassificationCode":"1/1","applicantName":["FUJIFILM
        Corporation"],"assigneeName":["FUJIFILM Corporation"],"uspcFullClassificationFlattened":null,"ipcCodeFlattened":"G06K9/62;G06N3/04","cpcInventiveFlattened":"G16H30/40;G06N3/08;G06V10/7784;G06K9/6256;G06N20/00;G16H30/20;G06V10/774;G06K9/6263;G16H50/70;G06N3/0454;G06N3/0427;G06K9/6262","cpcAdditionalFlattened":null,"applicationFilingDate":["2018-09-06T00:00:00Z"],"applicationFilingDateKwicHits":null,"relatedApplFilingDate":null,"primaryExaminer":"Sabouri;
        Mazda","assistantExaminer":null,"applicationNumber":"16/123456","frontPageStart":1,"frontPageEnd":1,"drawingsStart":2,"drawingsEnd":10,"specificationStart":11,"specificationEnd":18,"claimsStart":18,"claimsEnd":20,"abstractStart":1,"abstractEnd":1,"bibStart":1,"bibEnd":1,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":20,"pageCountDisplay":"20","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/G86/022/109","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":"Kanada;
        Shoji","familyIdentifierCur":65807696,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":["2017-09-27T00:00:00Z"],"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        10902286 B2","derwentAccessionNumber":null,"documentSize":88828,"score":14.108329,"governmentInterest":null,"kindCode":["B2"],"urpn":["2007/0217688","2015/0242760"],"urpnCode":["2007/0217688","2015/0242760"],"descriptionEnd":18,"publicationReferenceDocumentNumberOne":"10902286","descriptionStart":11}],"qtime":30}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:52:59 GMT
      Server-Timing:
      - intid;desc=ff968d003adaa4a5
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://ppubs.uspto.gov/dirsearch-public/patents/US-10902286-B2/highlight?queryId=1&source=USPAT&includeSections=True
  response:
    body:
      string: "{\"guid\":\"US-10902286-B2\",\"publicationReferenceDocumentNumber\":\"10902286\",\"compositeId\":\"65807696!US-US-10902286\",\"publicationReferenceDocumentNumber1\":\"10902286\",\"datePublishedKwicHits\":null,\"datePublished\":\"2021-01-26T00:00:00Z\",\"inventionTitle\":\"Learning
        assistance device, method of operating learning assistance device, learning
        assistance program, learning assistance system, and terminal device\",\"type\":\"USPAT\",\"mainClassificationCode\":\"1/1\",\"applicantName\":[\"FUJIFILM
        Corporation\"],\"assigneeName\":[\"FUJIFILM Corporation\"],\"uspcFullClassificationFlattened\":null,\"ipcCodeFlattened\":\"G06K9/62;G06N3/04\",\"cpcInventiveFlattened\":\"G16H30/40;G06N3/08;G06V10/7784;G06K9/6256;G06N20/00;G16H30/20;G06V10/774;G06K9/6263;G16H50/70;G06N3/0454;G06N3/0427;G06K9/6262\",\"cpcAdditionalFlattened\":null,\"applicationFilingDate\":[\"2018-09-06T00:00:00Z\"],\"applicationFilingDateKwicHits\":null,\"relatedApplFilingDate\":null,\"primaryExaminer\":\"Sabouri;
        Mazda\",\"assistantExaminer\":null,\"applicationNumber\":\"16/123456\",\"frontPageStart\":1,\"frontPageEnd\":1,\"drawingsStart\":2,\"drawingsEnd\":10,\"specificationStart\":11,\"specificationEnd\":18,\"claimsStart\":18,\"claimsEnd\":20,\"abstractStart\":1,\"abstractEnd\":1,\"bibStart\":1,\"bibEnd\":1,\"certCorrectionStart\":0,\"certCorrectionEnd\":0,\"certReexaminationStart\":0,\"certReexaminationEnd\":0,\"supplementalStart\":0,\"supplementalEnd\":0,\"ptabStart\":0,\"ptabEnd\":0,\"amendStart\":0,\"amendEnd\":0,\"searchReportStart\":0,\"searchReportEnd\":0,\"pageCount\":20,\"pageCountDisplay\":\"20\",\"previouslyViewed\":false,\"unused\":false,\"imageLocation\":\"uspat/US/G86/022/109\",\"imageFileName\":\"00000001.tif\",\"cpcCodes\":null,\"queryId\":1,\"tags\":null,\"inventorsShort\":\"Kanada;
        Shoji\",\"familyIdentifierCur\":65807696,\"familyIdentifierCurStr\":\"65807696\",\"languageIndicator\":\"EN\",\"databaseName\":\"USPT\",\"dwImageDoctypeList\":null,\"dwImageLocList\":null,\"dwPageCountList\":null,\"dwImageDocidList\":null,\"patentFamilyMembers\":null,\"patentFamilyCountry\":null,\"patentFamilySerialNumber\":null,\"documentIdWithDashesDw\":null,\"pfPublDate\":null,\"pfPublDateKwicHits\":null,\"priorityClaimsDate\":[\"2017-09-27T00:00:00Z\"],\"priorityClaimsDateKwicHits\":null,\"pfApplicationSerialNumber\":null,\"pfApplicationDescriptor\":null,\"pfLanguage\":null,\"pfApplicationDate\":null,\"pfApplicationDateKwicHits\":null,\"clippedUri\":null,\"source\":null,\"documentId\":\"<span
        term=\\\"us10902286b2\\\" class=\\\"highlight18\\\">US 10902286 B2</span>\",\"derwentAccessionNumber\":null,\"documentSize\":88828,\"score\":0.0,\"governmentInterest\":null,\"kindCode\":[\"B2\"],\"urpn\":[\"2007/0217688\",\"2015/0242760\"],\"urpnCode\":[\"2007/0217688\",\"2015/0242760\"],\"abstractedPatentNumber\":null,\"assigneeCity\":[\"Tokyo\"],\"assigneePostalCode\":[\"N/A\"],\"assigneeState\":[\"N/A\"],\"assigneeTypeCode\":[\"03\"],\"curIntlPatentClassificationPrimary\":[\"G06K9/62
        20060101\"],\"curIntlPatentClassificationPrimaryDateKwicHits\":null,\"designatedStates\":null,\"examinerGroup\":\"2641\",\"issuedUsCrossRefClassification\":null,\"jpoFtermCurrent\":null,\"languageOfSpecification\":null,\"chosenDrawingsReference\":null,\"derwentClass\":null,\"inventionTitleHighlights\":null,\"cpcOrigInventiveClassificationHighlights\":[\"G06K9/6256
        20130101\",\"G06K9/6263 20130101\",\"G06N3/0427 20130101\",\"G06N3/0454 20130101\",\"G06N3/08
        20130101\",\"G16H30/40 20180101\",\"G16H50/70 20180101\"],\"cpcInventiveDateKwicHits\":null,\"cpcOrigAdditionalClassification\":null,\"cpcAdditionalDateKwicHits\":null,\"curIntlPatentClssficationSecHighlights\":null,\"fieldOfSearchClassSubclassHighlights\":[\"382/159\"],\"cpcCombinationSetsCurHighlights\":null,\"applicantCountry\":[\"JP\"],\"applicantCity\":[\"Tokyo\"],\"applicantState\":[\"N/A\"],\"applicantZipCode\":[\"N/A\"],\"applicantAuthorityType\":[\"assignee\"],\"applicantDescriptiveText\":null,\"applicationSerialNumber\":[\"123456\"],\"inventorCity\":[\"Tokyo\"],\"inventorState\":[\"N/A\"],\"inventorPostalCode\":[\"N/A\"],\"standardTitleTermsHighlights\":null,\"primaryExaminerHighlights\":\"Sabouri;
        Mazda\",\"continuityData\":null,\"inventors\":null,\"uspcFullClassification\":null,\"uspcCodeFmtFlattened\":null,\"ipcCode\":null,\"applicationNumberHighlights\":[\"16/123456\"],\"dateProduced\":\"2021-01-06T00:00:00Z\",\"auxFamilyMembersGroupTempPlaceHolder\":null,\"priorityCountryCode\":null,\"cpcCurAdditionalClassification\":null,\"internationalClassificationMain\":null,\"internationalClassificationSecondary\":null,\"internationalClassificationInformational\":null,\"europeanClassification\":null,\"europeanClassificationMain\":null,\"europeanClassificationSecondary\":null,\"lanuageIndicator\":null,\"intlPubClassificationPrimary\":[\"G06K9/62
        20060101 G06K009/62\"],\"intlPubClassificationPrimaryDateKwicHits\":null,\"intlPubClassificationSecondary\":[\"G06N3/04
        20060101 G06N003/04\",\"G16H50/70 20180101 G16H050/70\",\"G06N3/08 20060101
        G06N003/08\",\"G16H30/40 20180101 G16H030/40\"],\"intlPubClassificationSecondaryDateKwicHits\":null,\"publicationDate\":null,\"derwentWeekInt\":0,\"derwentWeek\":null,\"currentUsOriginalClassification\":\"1/1\",\"currentUsCrossReferenceClassification\":null,\"locarnoClassification\":null,\"equivalentAbstractText\":null,\"hagueIntlRegistrationNumber\":null,\"hagueIntlFilingDate\":null,\"hagueIntlFilingDateKwicHits\":null,\"hagueIntlRegistrationDate\":null,\"hagueIntlRegistrationDateKwicHits\":null,\"hagueIntlRegistrationPubDate\":null,\"hagueIntlRegistrationPubDateKwicHits\":null,\"curIntlPatentClassificationNoninvention\":null,\"curIntlPatentClassificationNoninventionDateKwicHits\":null,\"curIntlPatentClassificationSecondary\":[\"G06N3/04
        20060101\",\"G16H50/70 20180101\",\"G06N3/08 20060101\",\"G16H30/40 20180101\"],\"curIntlPatentClassificationSecondaryDateKwicHits\":null,\"abstractHtml\":\"A
        learning assistance device acquires a plurality of learned discriminators
        obtained by causing learning discriminators provided in a plurality of respective
        terminal devices to perform learning using image correct answer data, acquires
        a plurality of discrimination results obtained by causing a plurality of learned
        discriminators to discriminate the same input image, determines the correct
        answer data of the input image on the basis of the plurality of discrimination
        results, causes the discriminator to perform learning the input image and
        the correct answer data, and outputs a result thereof as a new learning discriminator
        to each terminal device.\",\"descriptionHtml\":\"BRIEF DESCRIPTION OF THE
        DRAWINGS<br />(1) <figref idref=\\\"DRAWINGS\\\">FIG. 1</figref> is a diagram
        illustrating a schematic configuration of a learning assistance system of
        the present invention.<br />(2) <figref idref=\\\"DRAWINGS\\\">FIG. 2</figref>
        is a diagram illustrating a schematic configuration of a medical information
        system.<br />(3) <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref> illustrates
        an example of a multilayered neural network.<br />(4) <figref idref=\\\"DRAWINGS\\\">FIG.
        4</figref> is a block diagram illustrating a schematic configuration of a
        terminal device and a learning assistance device according to a first embodiment.<br
        />(5) <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref> is a diagram illustrating
        learning of a discriminator.<br />(6) <figref idref=\\\"DRAWINGS\\\">FIG.
        6</figref> is a flowchart showing a flow of a process of causing the discriminator
        to perform learning.<br />(7) <figref idref=\\\"DRAWINGS\\\">FIG. 7</figref>
        is a block diagram illustrating a schematic configuration of a terminal device
        and a learning assistance device according to a second embodiment.<br />(8)
        <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref> is a block diagram illustrating
        a schematic configuration of a terminal device and a learning assistance device
        according to a third embodiment.<br />(9) <figref idref=\\\"DRAWINGS\\\">FIG.
        9</figref> is a block diagram illustrating a schematic configuration of a
        terminal device and a learning assistance device according to a fourth embodiment.<br
        />DETAILED DESCRIPTION<br />(10) <figref idref=\\\"DRAWINGS\\\">FIG. 1</figref>
        illustrates a schematic configuration of a learning assistance system <b>1</b>
        according to a first embodiment of the present invention. The learning assistance
        system <b>1</b> is configured by connecting a plurality of terminal devices
        <b>10</b> installed in a plurality of medical institutions A, B, . . . , X
        and a learning assistance device <b>20</b> placed on a cloud side over a network
        <b>30</b>.<br />(11) The learning assistance device <b>20</b> includes a well-known
        hardware configuration such as a central processing unit (CPU), a memory,
        a storage, an input and output interface, a communication interface, an input
        device, a display device, and a data bus, and is a high-performance computer
        in which a well-known operation system or the like is installed and which
        has a server function. Further, a graphics processing unit (GPU) may be provided,
        as necessary. Alternatively, the learning assistance device <b>20</b> may
        be a virtualized virtual server provided using one or a plurality of computers.
        The learning assistance program of the present invention is installed in a
        server, and functions as a learning assistance device by a program instruction
        being executed by the CPU of the computer.<br />(12) The terminal device <b>10</b>
        is a computer for image processing provided in the respective medical institutions
        A, B, . . . , X, and includes a well-known hardware configuration such as
        a CPU, a memory, a storage, an input and output interface, a communication
        interface, an input device, a display device, and a data bus. A well-known
        operation system or the like is installed in the terminal device <b>10</b>.
        The terminal device <b>10</b> includes a display as a display device. Further,
        a GPU may be provided, as necessary.<br />(13) The network <b>30</b> is a
        wide area network (WAN) that widely connects the terminal devices <b>10</b>
        placed at the plurality of medical institutions A, B, . . . , X to the learning
        assistance device <b>20</b> via a public network or a private network.<br
        />(14) Further, as illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 2</figref>,
        the terminal device <b>10</b> is connected to respective medical information
        systems <b>50</b> of the respective medical institutions A, B, . . . , X over
        a local area network (LAN) <b>51</b>. The medical information system <b>50</b>
        includes a modality (an imaging device) <b>52</b>, an image database <b>53</b>,
        and an image interpretation medical workstation <b>54</b>, and is configured
        so that transmission and reception of image data to and from each other are
        performed over the network <b>51</b>. It should be noted that in the network
        <b>51</b>, it is desirable to use a communication cable such as an optical
        fiber so that image data can be transferred at a high speed.<br />(15) The
        modality <b>52</b> includes a device that images an examination target part
        of a subject to generate an examination image representing the part, adds
        accessory information defined in a DICOM standard to the image, and outputs
        the resultant image. Specific examples of the device include a computed tomography
        (CT) device, a magnetic resonance imaging (MRI) device, a positron emission
        tomography (PET) device, an ultrasonic device, and a computed radiography
        (CR) device using a planar X-ray detector (FPD: flat panel detector).<br />(16)
        In the image database <b>53</b>, a software program for providing a function
        of a database management system (DBMS) is incorporated in a general-purpose
        computer, and a large capacity storage is included. This storage may be a
        large capacity hard disk device, or may be a disk device connected to a network
        attached storage (NAS) or a storage area network (SAN) connected to the network
        <b>51</b>. Further, the image data captured by the modality <b>52</b> is transmitted
        to and stored in the image database <b>53</b> over the network <b>51</b> according
        to a storage format and a communication standard conforming to a DICOM standard.<br
        />(17) The image interpretation medical workstation <b>54</b> is a computer
        that is used for an image interpretation doctor of a radiology department
        to interpret an image and create an interpretation report. The image interpretation
        medical workstation <b>54</b> performs a display of the image data received
        from the image database <b>53</b> and performs automatic detection of a portion
        likely to be a lesion in the image.<br />(18) In the embodiment, a case where
        an image processing program in which a discriminator functioning as an actually
        operated discriminator is incorporated in each terminal device <b>10</b> is
        provided from the learning assistance device <b>20</b>, and a learning program
        in which a discriminator functioning as a learning program separately from
        the image processing program is incorporated is provided will be described.
        The image processing program and the learning program distributed to each
        terminal device <b>10</b> are installed in the terminal device <b>10</b> to
        function as an image processing device in which the actually operated discriminator
        is incorporated, and a learning discriminator.<br />(19) Further, a case where
        the actually operated discriminator and the learning discriminator are multilayered
        neural networks subjected to deep learning to be able to discriminate a plurality
        of types of organ areas and/or lesion areas will be described. In the multilayered
        neural network, a calculation process is performed on a plurality of pieces
        of different calculation result data obtained by a preceding layer for input
        data, that is, extraction result data of a feature amount using various kernels
        in each layer, data of the feature amount obtained by the calculation process
        is acquired, and a further calculation process is performed on the data of
        the feature amount in the next and subsequent processing layers. Thus, it
        is possible to improve a recognition rate of the feature amount and to discriminate
        which of a plurality of types of areas the input image data is.<br />(20)
        <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref> is a diagram illustrating an
        example of the multilayered neural network. As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        3</figref>, the multilayered neural network <b>40</b> includes a plurality
        of layers including an input layer <b>41</b> and an output layer <b>42</b>.
        In <figref idref=\\\"DRAWINGS\\\">FIG. 3</figref>, a layer before the output
        layer <b>42</b> is denoted by reference numeral <b>43</b>.<br />(21) In the
        multilayered neural network <b>40</b>, the image data is input to the input
        layer <b>41</b> and a discrimination result of an area is output. In a case
        where learning is performed, the output discrimination result is compared
        with correct answer data, and a weight of coupling between the respective
        layers of units (indicated by circles in <figref idref=\\\"DRAWINGS\\\">FIG.
        3</figref>) included in the respective layers of the multilayered neural network
        <b>40</b> is corrected from the output side (the output layer <b>42</b>) to
        the input side (the input layer <b>41</b>) according to whether an answer
        is a correct answer or an incorrect answer. The correction of the weight of
        coupling is repeatedly performed a predetermined number of times, or is repeatedly
        performed until a correct answer rate of the output discrimination result
        is 100% or is equal to or greater than a predetermined threshold value using
        a large number of pieces of image data with correct answer data, and the learning
        ends.<br />(22) <figref idref=\\\"DRAWINGS\\\">FIG. 4</figref> is a block
        diagram illustrating a schematic configuration of the terminal device <b>10</b>
        and the learning assistance device <b>20</b>. Functions of the terminal device
        <b>10</b> and the learning assistance device <b>20</b> will be described in
        detail with reference to <figref idref=\\\"DRAWINGS\\\">FIG. 4</figref>. First,
        the terminal device <b>10</b> will be described.<br />(23) The terminal device
        <b>10</b> includes a discriminator acquisition unit <b>12</b>, a discrimination
        result acquisition unit <b>13</b>, a learning unit <b>14</b>, and a learned
        discriminator output unit <b>15</b>.<br />(24) The discriminator acquisition
        unit <b>12</b> acquires a learning discriminator and an actually operated
        discriminator. For example, the image processing program and the learning
        program are received from the learning assistance device <b>20</b> over the
        network <b>30</b>, and the received image processing program is installed.
        Accordingly, image processing in which the actually operated discriminator
        is incorporated becomes executable in the terminal device <b>10</b> and functions
        as the discrimination result acquisition unit <b>13</b>. Similarly, the learning
        program is installed, and the learning discriminator becomes executable and
        functions as the learning unit <b>14</b>. It should be noted that the learning
        discriminator is a discriminator that has learned the same image correct answer
        data as the actually operated discriminator received from the learning assistance
        device <b>20</b>. In the following description, the image processing in which
        the actually operated discriminator is incorporated is simply referred to
        as an actually operated discriminator. It should be noted that the image correct
        answer data refers to a combination of the image data and correct answer data
        thereof. Details of the image correct answer data will be described below.<br
        />(25) The discrimination result acquisition unit <b>13</b> inputs a discrimination
        target image data to the actually operated discriminator and acquires a discrimination
        result. The actually operated discriminator is a discriminator of which discrimination
        performance has been guaranteed in the learning assistance device <b>20</b>,
        and in each of the medical institutions A, B, X, the discrimination is performed
        on the image data that is a diagnosis target using the actually operated discriminator.
        Further, the discrimination result acquisition unit <b>13</b> may perform
        discrimination of the image data that is a diagnosis target sent from the
        image interpretation medical workstation <b>54</b> to the terminal device
        <b>10</b> over the network <b>51</b>, and transmit a discrimination result
        from the terminal device <b>10</b> to the image interpretation medical workstation
        <b>54</b>.<br />(26) The learning unit <b>14</b> causes the learning discriminator
        to perform learning using the image data and the correct answer data thereof.
        The correct answer data includes a mask image showing an area such as an organ
        or abnormal shadow of the image data, and information indicating what the
        area of the mask image is (for example, an area of an organ such as a liver,
        a kidney, or a lung or an area of an abnormal shadow such as a liver cancer,
        a kidney cancer, or a pulmonary nodule).<br />(27) The correct answer data
        may be created by an image interpretation doctor or the like of each of the
        medical institutions A, B, . . . , X observing the image data. For example,
        the image data is extracted from the image database <b>53</b>, the discrimination
        result acquisition unit <b>13</b> inputs the image data to the actually operated
        discriminator and acquires a discrimination result, and a user such as the
        image interpretation doctor determines whether the discrimination result is
        a correct answer or an incorrect answer, and stores a discrimination result
        together with the input image data and correct answer data in the image database
        <b>53</b> as image correct answer data in the case of the correct answer.
        In the case of an incorrect answer, the user generates a mask image of the
        correct answer data, assigns the correct answer data to the image data, and
        stores the resultant data in the image database <b>53</b> as image correct
        answer data.<br />(28) Therefore, the learning unit <b>14</b> causes the multilayered
        neural network <b>40</b> of the learning discriminator to perform learning
        using a large number of pieces of image correct answer data stored in the
        image database <b>53</b>. First, the image data of the image correct answer
        data is input to the multilayered neural network <b>40</b>, and a discrimination
        result is output. Then, the output discrimination result is compared with
        the correct answer data, and a weight of coupling between the respective layers
        of the units included in the respective layers of the multilayered neural
        network <b>40</b> from the output side to the input side is corrected according
        to whether the answer is a correct answer or an incorrect answer. The correction
        of the weight of the coupling is repeatedly performed using a large number
        of pieces of correct answer data a predetermined number of times or until
        the correct answer rate of the output discrimination result becomes 100%,
        and the learning is ended.<br />(29) The learned discriminator output unit
        <b>15</b> outputs the learning discriminator of which the learning has ended
        in the learning unit <b>14</b> as a learned discriminator. Specifically, the
        weight (hereinafter referred to as a parameter) of coupling between the layers
        of the units constituting the neural network constituting the learned discriminator
        is periodically transmitted to the learning assistance device <b>20</b> over
        the network <b>30</b>.<br />(30) Next, the learning assistance device <b>20</b>
        will be described. As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 4</figref>,
        the learning assistance device <b>20</b> includes a learned discriminator
        acquisition unit <b>22</b>, a discriminator storage unit <b>23</b>, a correct
        answer data acquisition unit <b>24</b>, a correct answer data storage unit
        <b>25</b>, a learning unit <b>26</b>, and a discriminator output unit <b>27</b>.<br
        />(31) The learned discriminator acquisition unit <b>22</b> receives a parameter
        of the multilayered neural network <b>40</b> constituting the learned discriminators
        transmitted from the plurality of terminal devices <b>10</b> over the network
        <b>30</b>. The received parameter is temporarily stored in the discriminator
        storage unit <b>23</b>. The multilayered neural network <b>40</b> is provided
        in the learning assistance device <b>20</b> in advance and the parameter received
        from each terminal device <b>10</b> is set as the weight of the coupling between
        the respective layers of the units of the multilayered neural network <b>40</b>
        provided in the learning assistance device <b>20</b>. By re-setting the parameter
        received from each terminal device in this weight, the same learned discriminator
        as each terminal device <b>10</b> can be acquired.<br />(32) The correct answer
        data acquisition unit <b>24</b> causes each of the learned discriminators
        collected from each of the terminal devices <b>10</b> to discriminate the
        same input image data to acquire a plurality of discrimination results, and
        determines the correct answer data of the input image data from the plurality
        of discrimination results.<br />(33) A large number of pieces of image data
        are often stored in a database without correct answer data attached thereto.
        In order to attach the correct answer data to the image data, for example,
        the image data is input to the discriminator so as to acquire a discrimination
        result, and a user such as an image interpretation doctor performs a determination
        that the answer is a correct answer or an incorrect answer with respect to
        the discrimination result, and registers the discrimination result as the
        image correct answer data in association with the correct answer data and
        the input image data in the case of the correct answer. In case of the incorrect
        answer, the user creates a mask image of the correct answer data and registers
        the mask image as image correct answer data in association with input image
        data. Work of creating the correct answer data in this way is laborious and
        it is difficult to manually generate a large number of pieces of correct answer
        data.<br />(34) Therefore, the correct answer data acquisition unit <b>24</b>
        determines the largest number of same discrimination results as correct answer
        data of the input image data from the discrimination results obtained by inputting
        the same input image data to the learned discriminators collected from the
        respective terminal devices <b>10</b>. Thus, in a case where the correct answer
        data is determined from a plurality of discrimination results obtained by
        using the learned discriminators collected from the respective terminal devices
        <b>10</b>, accurate correct answer data can be automatically generated, and
        therefore, it is possible to easily obtain a large number of pieces of correct
        answer data. The obtained correct answer data is accumulated in the storage
        (the correct answer data storage unit <b>25</b>) as image correct answer data
        in association with the input image data.<br />(35) The learning unit <b>26</b>
        is provided with a deep learning discriminator configured of a multilayered
        neural network <b>40</b>. The image correct answer data accumulated in the
        correct answer data storage unit <b>25</b> is sequentially input to the deep
        learning discriminator, so that learning is performed.<br />(36) In a stage
        the learning had progressed to a certain extent and accuracy of the discrimination
        of the deep learning discriminator has improved, the discriminator output
        unit <b>27</b> generates an image processing program (actually operated discriminator)
        incorporating the deep learning discriminator learned by the learning unit
        <b>26</b> and a learning program (the learning discriminator), and distributes
        the programs to each terminal device <b>10</b> over the network <b>30</b>.
        Since software intended for medical purposes is a target of the Pharmaceutical
        and Medical Device Act (the revised Pharmaceutical Affairs Act), the software
        is required to meet a criterion prescribed in the Pharmaceutical and Medical
        Device Act. Therefore, it is preferable to confirm that a deep learning discriminator
        before distribution exceeds an evaluation standard using an evaluation image
        set formed through a combination of a plurality of images in which the criterion
        prescribed in the Pharmaceutical and Medical Device Act can be evaluated,
        and then, distribute deep learning discriminator.<br />(37) Even after the
        discriminator output unit <b>27</b> distributes the image processing program
        and the learning program to the respective terminal devices <b>10</b>, the
        learning unit <b>26</b> sequentially inputs the image correct answer data
        accumulated in the correct answer data storage unit <b>25</b> to the deep
        learning discriminator as it is and causes the deep learning discriminator
        to perform learning. That is, the learning unit <b>26</b> causes the learning
        discriminator output by the discriminator output unit <b>27</b> to perform
        additional learning, and the discriminator output unit <b>27</b> outputs the
        additionally learned learning discriminator as a new learning discriminator.<br
        />(38) Next, a flow of a deep learning process of the embodiment will be described
        with reference to a transition diagram of <figref idref=\\\"DRAWINGS\\\">FIG.
        5</figref> and a flowchart of <figref idref=\\\"DRAWINGS\\\">FIG. 6</figref>.<br
        />(39) First, in the learning assistance device <b>20</b>, the discriminator
        output unit <b>27</b> distributes the actually operated discriminator NNo
        and the learning discriminator NNt to the plurality of terminal devices <b>10</b>,
        and to the medical institution A, . . . , the medical institution X over the
        network <b>30</b> (S<b>1</b>).<br />(40) In the terminal device <b>10</b>,
        the discriminator acquisition unit <b>12</b> acquires the actually operated
        discriminator NNo and the learning discriminator NNt (S<b>2</b>). The actually
        operated discriminator NNo is used for diagnosis by an image interpretation
        doctor, and the discrimination result acquisition unit <b>13</b> discriminates
        image data (input) that is a diagnosis target and obtains a discrimination
        result (output) (see <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>). Further,
        in the medical institution A, the learning unit <b>14</b> of the terminal
        device <b>10</b> causes the learning discriminator NNt to perform learning
        using the image correct answer data T stored in the image database <b>53</b>
        (S<b>3</b>) and generates the learned discrimination unit NNt-A (S<b>4</b>).
        Similarly, in the medical institution X, the learning unit <b>14</b> of the
        terminal device <b>10</b> causes the learning discriminator NNt to perform
        learning using the image correct answer data T and generates the learned discriminator
        NNt-X (S<b>4</b>).<br />(41) Periodically, in each terminal device <b>10</b>,
        the learned discriminator output unit <b>15</b> transmits the learned discriminator
        to the learning assistance device <b>20</b> (S<b>5</b>). The parameter of
        the learned discriminator NNt-A is transmitted from the terminal device <b>10</b>
        of the medical institution A to the learning assistance device <b>20</b>,
        and the parameter of the learned discriminator NNt-X is transmitted from the
        terminal device <b>10</b> of the medical institution X to the learning assistance
        device <b>20</b> (see a solid arrow (1) in <figref idref=\\\"DRAWINGS\\\">FIG.
        5</figref>).<br />(42) In the learning assistance device <b>20</b>, the learned
        discriminator acquisition unit <b>22</b> temporarily stores the parameters
        of the learned discriminators received from the plurality of terminal devices
        <b>10</b> in the discriminator storage unit <b>23</b>. By setting this parameter
        in the multilayered neural network <b>40</b> provided in the learning assistance
        device <b>20</b>, the learned discriminator learned by each terminal device
        <b>10</b> is acquired (S<b>6</b>).<br />(43) The correct answer data acquisition
        unit <b>24</b> inputs the input image data P to the learned discriminator
        of each terminal device <b>10</b> to obtain the discrimination result. In
        the example of <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>, a result a,
        a result b, a result c, . . . , a result g are obtained, and a largest number
        of results b are determined to be correct answer data of the input image data
        P (S<b>7</b>). The input image data P and the correct answer data are accumulated
        in the storage <b>25</b> in association with each other.<br />(44) The learning
        unit <b>26</b> causes the deep learning discriminator NNl to perform learning
        using the input image data P and the result b (correct answer data) (S<b>8</b>).
        Periodically, a new version of the actually operated discriminator NNo and
        the learning discriminator NNt are generated on the basis of the deep learning
        discriminator NNl (S<b>9</b>). The discriminator output unit <b>27</b> distributes
        the new version of the actually operated discriminator NNo and learning discriminator
        NNt to each of the terminal devices <b>10</b> (S<b>10</b>; see an arrow (2)
        of a broken line in <figref idref=\\\"DRAWINGS\\\">FIG. 5</figref>).<br />(45)
        In the terminal device <b>10</b> of the medical institution A, the discriminator
        acquisition unit <b>12</b> acquires the new version of the actually operated
        discriminator NNo and learning discriminator NNt again (S<b>2</b>). The learning
        unit <b>14</b> of the terminal device <b>10</b> causes the new version of
        learning discriminator NNt to perform learning using the image correct answer
        data T stored in the image database <b>53</b> (S<b>3</b>) and generate the
        learned discriminator NNt-A again (S<b>4</b>).<br />(46) In the terminal device
        <b>10</b> of the medical institution X, the discriminator acquisition unit
        <b>12</b> acquires the new version of the actually operated discriminator
        NNo and learning discriminator NNt again (S<b>2</b>). The learning unit <b>14</b>
        of the terminal device <b>10</b> causes the new version of learning discriminator
        NNt to perform learning using the image correct answer data T stored in the
        image database <b>53</b> (S<b>3</b>) and generates a learned discriminator
        NNt-X (S<b>4</b>). Consequently, the process from S<b>5</b> to S<b>10</b>
        is performed, as in the same manner as described above.<br />(47) The processes
        of S<b>2</b> to S<b>10</b> are repeated, and the learning assistance device
        <b>20</b> generates an actually operated discriminator and a learning discriminator
        of which the performance has been improved while generating the correct answer
        data, and distributes the actually operated discriminator and the learning
        discriminator to the terminal device <b>10</b>.<br />(48) As described above,
        by the terminal device <b>10</b> placed in each medical institution performing
        learning using the image data stored in the medical institution, a discriminator
        improving discrimination performance is generated in each medical institution.
        By the learning assistance device <b>20</b> generating the correct answer
        data using the discriminators of which the performance have been improved
        in each medical institution, a large amount of accurate correct answer data
        can be generated, and deep Learning can be performed using this correct answer
        data.<br />(49) Although the case where the mask image of the correct answer
        data for the image data has the information indicating what the area on the
        image is has been described above, the discriminator may be configured to
        (1) determine an organ area and an organ name by determining what organ each
        pixel position of the image data is, (2) determine a lesion area and a type
        of lesion by determining a type of lesion of each pixel in units of pixels
        of the image data. Alternatively, correct answer data for one image may be
        specified as a disease name or an image diagnostic name, and (3) the disease
        name may be specified from the image data.<br />(50) Next, a second embodiment
        will be described. The second embodiment is different from the first embodiment
        in the method of determining the correct answer data. Since a schematic configuration
        of the learning assistance system <b>1</b> is the same as that of the first
        embodiment, detailed description thereof will be omitted. <figref idref=\\\"DRAWINGS\\\">FIG.
        7</figref> is a block diagram illustrating a schematic configuration of a
        terminal device <b>10</b> and a learning assistance device <b>20</b> according
        to the second embodiment. The same configurations as those of the first embodiment
        are denoted by the same reference numerals as those of the first embodiment,
        detailed description thereof will be omitted, and only different configurations
        will be described.<br />(51) As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        7</figref>, the terminal device <b>10</b> includes a discriminator acquisition
        unit <b>12</b>, a discrimination result acquisition unit <b>13</b>, a learning
        unit <b>14</b>, and a learned discriminator output unit <b>15</b>. The learning
        assistance device <b>20</b> includes a learned discriminator acquisition unit
        <b>22</b>, a discriminator storage unit <b>23</b>, a correct answer data acquisition
        unit <b>24</b><i>a</i>, a correct answer data storage unit <b>25</b>, a learning
        unit <b>26</b>, and a discriminator output unit <b>27</b>. A configuration
        of the terminal device <b>10</b> is the same as that of the first embodiment
        except that the correct answer data acquisition unit <b>24</b><i>a </i>of
        the learning assistance device <b>20</b> includes an evaluation unit <b>28</b>
        and an evaluation image storage unit <b>29</b>.<br />(52) In the case where
        the correct answer data is determined by majority vote, sufficient learning
        may be performed in each terminal device <b>10</b> as in the first embodiment,
        but for example, in a case where a discriminator not additionally learned
        in the terminal device <b>10</b> is received as the learned discriminator
        or in a case where a learned discriminator with a small number of additional
        learnings is used, a determination result is likely to be the same for the
        same input image data P, and a determination result of the discriminator in
        which such learning is not sufficiently performed is highly likely to be the
        correct answer data.<br />(53) Therefore, the learning assistance device <b>20</b>
        evaluates the learned discriminators collected from the respective terminal
        device <b>10</b> in advance. The learning assistance device <b>20</b> sets
        a plurality of cases of disease covering a representative case pattern and
        also having correct answer data for image data as an evaluation image set
        SET, and the evaluation unit <b>28</b> evaluates the learned discriminators
        sent from the respective terminal devices <b>10</b> using the image set SET,
        and determines the weight of the discriminator according to a height of the
        correct answer rate. Further, since software intended for medical purposes
        is a target of the Pharmaceutical and Medical Device Act (the revised Pharmaceutical
        Affairs Act), the software is required to meet a criterion prescribed in the
        Pharmaceutical and Medical Device Act. Therefore, it is preferable for an
        evaluation image set SET formed through a combination of a plurality of images
        in which the criterion prescribed in the Pharmaceutical and Medical Device
        Act can be evaluated to be stored in the storage (the evaluation image storage
        unit <b>29</b>) in advance. However, this evaluation image set SET is not
        sufficient for use in deep learning.<br />(54) The weights are determined
        for the respective learned discriminators collected from the terminal device
        <b>10</b> according to the correct answer rate of the evaluation image set,
        the weights of the learned discriminators having the same result among the
        discrimination results are added, and the discrimination result having the
        largest added weight is set as correct answer data of the input image.<br
        />(55) Since a flow of the deep learning process is the same as that of the
        first embodiment, the flow will be omitted.<br />(56) Next, a third embodiment
        will be described. The third embodiment is different from the first and second
        embodiments in the method of determining the correct answer data. Since a
        schematic configuration of the learning assistance system <b>1</b> is the
        same as that of the first embodiment, detailed description thereof will be
        omitted. <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref> is a block diagram
        illustrating a schematic configuration of a terminal device <b>10</b> and
        a learning assistance device <b>20</b> according to the third embodiment.
        The same configurations as those of the first embodiment are denoted by the
        same reference numerals as those of the first embodiment, detailed description
        thereof will be omitted, and only different configurations will be described.<br
        />(57) As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG. 8</figref>, the
        terminal device <b>10</b> includes a discriminator acquisition unit <b>12</b>,
        a discrimination result acquisition unit <b>13</b>, a learning unit <b>14</b>,
        and a learned discriminator output unit <b>15</b><i>a</i>. The learning assistance
        device <b>20</b> includes a learned discriminator acquisition unit <b>22</b><i>a</i>,
        a discriminator storage unit <b>23</b>, a correct answer data acquisition
        unit <b>24</b><i>b</i>, a correct answer data storage unit <b>25</b>, a learning
        unit <b>26</b>, and a discriminator output unit <b>27</b>. The learned discriminator
        output unit <b>15</b><i>a </i>of the terminal device <b>10</b>, and the learned
        discriminator acquisition unit <b>22</b><i>a </i>and the correct answer data
        acquisition unit <b>24</b><i>b </i>of the learning assistance device <b>20</b>
        are different from those of the first embodiment.<br />(58) As in the second
        embodiment, even in a case where the performance of the learned discriminator
        is evaluated, evaluation results using the evaluation image set may be insufficient
        in a case where there is a problem with the number of cases of disease for
        evaluation of the learning assistance device <b>20</b> or coverage of the
        cases of disease. However, in a case where the learned discrimination learns
        a certain number of pieces of image correct answer data, it can be presumed
        that the performance is improving as appropriate.<br />(59) Therefore, in
        a case where the learned discriminator output unit <b>15</b><i>a </i>of the
        terminal device <b>10</b> transmits the parameter of the learned discriminator,
        the learned discriminator output unit <b>15</b><i>a </i>of the terminal device
        <b>10</b> transmits the number of pieces of image correct answer data learned
        by the learned discriminator to the learning assistance device <b>20</b>.<br
        />(60) The learned discriminator acquisition unit <b>22</b><i>a </i>of the
        learning assistance device <b>20</b> receives the number of pieces of image
        correct answer data learned by the learned discriminator at each terminal
        device <b>10</b> together in a case where the learned discriminator acquisition
        unit <b>22</b><i>a </i>of the learning assistance device <b>20</b> receives
        the parameter. The weight is determined so that the weight increases as the
        number of pieces of image correct answer data learned by the correct answer
        data acquisition unit <b>24</b><i>b </i>and the learned discriminators of
        each terminal device <b>10</b> increases. The weights of the learned discriminators
        having the same discrimination result are added, and the discrimination result
        with the largest added weight is set as the correct answer data of the input
        image.<br />(61) Since a flow of deep learning process is the same as that
        of the first embodiment, description thereof is omitted.<br />(62) Next, a
        fourth embodiment will be described. The fourth embodiment is different from
        the first, second, and third embodiments in the method of determining the
        correct answer data. Since a schematic configuration of the learning assistance
        system <b>1</b> is the same as that of the first embodiment, detailed description
        thereof will be omitted. <figref idref=\\\"DRAWINGS\\\">FIG. 9</figref> is
        a block diagram illustrating a schematic configuration of a terminal device
        <b>10</b> and a learning assistance device <b>20</b> according to the fourth
        embodiment. The same configurations as those of the first embodiment are denoted
        by the same reference numerals as those of the first embodiment, detailed
        description thereof will be omitted, and only different configurations will
        be described.<br />(63) As illustrated in <figref idref=\\\"DRAWINGS\\\">FIG.
        9</figref>, the terminal device <b>10</b> includes a discriminator acquisition
        unit <b>12</b>, a discrimination result acquisition unit <b>13</b>, a learning
        unit <b>14</b>, and a learned discriminator output unit <b>15</b><i>b</i>.
        The learning assistance device <b>20</b> includes a learned discriminator
        acquisition unit <b>22</b><i>b</i>, a discriminator storage unit <b>23</b>,
        a correct answer data acquisition unit <b>24</b><i>c</i>, a correct answer
        data storage unit <b>25</b>, a learning unit <b>26</b>, and a discriminator
        output unit <b>27</b>. The learned discriminator output unit <b>15</b><i>b
        </i>of the terminal device <b>10</b> and the learned discriminator acquisition
        unit <b>22</b><i>b </i>and the correct answer data acquisition unit <b>24</b><i>c
        </i>of the learning assistance device <b>20</b> are different from those of
        the first embodiment.<br />(64) The number of pieces of image correct answer
        data for each type of cases of disease is biased due to the characteristics
        of the medical facility, the regional nature, or the like, and the number
        of pieces of image correct answer data is small in any kind of diseases even
        though the number of all pieces of image correct answer data is large, the
        performance is likely not to be improved in the disease. Therefore, the number
        of pieces of image correct answer data learned by the learned discriminator
        of each medical facility is received from each terminal device <b>10</b> for
        each type of cases of disease.<br />(65) Therefore, in a case where the learned
        discriminator output unit <b>15</b><i>b </i>of the terminal device <b>10</b>
        transmits the parameter of the learned discriminator, the learned discriminator
        output unit <b>15</b><i>b </i>of the terminal device <b>10</b> transmits the
        number of pieces of image correct answer data for each type of cases of disease
        learned by the learned discriminator to the learning assistance device <b>20</b>.
        Specifically, it is determined which case of disease the image correct answer
        data learned by the learning unit <b>14</b> of the terminal device <b>10</b>
        relates to, for example, on the basis of a DICOM tag attached to the image
        of image correct answer data, and the number of learned image correct answer
        data is changed for each type of cases of disease. The type of cases of disease
        is classified by disease name (which may be an image diagnostic name in case
        of image inspection) or a type of disease name. In a case where a plurality
        of organs are collectively processed by one discriminator, an organ name may
        be used.<br />(66) The learned discriminator acquisition unit <b>22</b><i>b
        </i>of the learning assistance device <b>20</b> receives the number of pieces
        of image correct answer data for each type of cases of disease learned by
        the learned discriminator at each terminal device <b>10</b> in a case where
        the learned discriminator acquisition unit <b>22</b><i>b </i>of the learning
        assistance device <b>20</b> receives the parameter.<br />(67) In the correct
        answer data acquisition unit <b>24</b><i>c</i>, it is estimated that the performance
        of the learned discriminator is higher as the number of learned image correct
        answer data is larger. To reflect this, the number of pieces of image correct
        answer data learned by each facility for each type of cases of disease is
        counted for the learned discriminator of each terminal device <b>10</b>, and,
        the weight for each type of cases of disease is determined for each learned
        discriminator so that weight is increased as the number increases. In addition,
        weights of the learned discriminators having the same discrimination result
        are added in correspondence to the type of cases of disease of the input image,
        and the discrimination result having the largest added weight is set as the
        correct answer data of the input image.<br />(68) Since a flow of deep learning
        process is the same as that of the first embodiment, description thereof will
        be omitted.<br />(69) With a scheme according to the embodiment, it is possible
        to evaluate the learned discriminator in consideration of, the number of pieces
        of image correct answer data of each medical facility, the type of cases of
        disease, and the like.<br />(70) Further, the evaluation image set according
        to the second embodiment may be set as an evaluation image set capable of
        evaluating a discriminator for each type of disease, and the weight of the
        learned discriminator of each terminal device may be determined according
        to the correct answer rate for each type of disease. The weights of the learned
        discriminators having the same discrimination result may be added according
        to the type of cases of disease of the input image, and the discrimination
        result having the largest added weight may be set as the correct answer data
        of the input image.<br />(71) Further, although the weight is automatically
        determined in the fourth embodiment, the weight may be determined manually
        and stored in the learning assistance device <b>20</b> in advance in consideration
        of importance of the facility, a confident disease of each facility, or the
        like.<br />(72) Although the embodiment in which the learning assistance device
        <b>20</b> and the terminal device <b>10</b> are connected via the network
        has been described in the above description, an image processing program incorporating
        a discriminator functioning as an actually operated discriminator and a learning
        program incorporating a discriminator functioning as a learning discriminator
        may be stored in a recording medium such as a DVD-ROM and distributed to each
        medical institution, instead of over the network.<br />(73) In this embodiment,
        the discriminator acquisition unit <b>12</b> of the terminal device <b>10</b>
        reads the image processing program and the learning program from the DVD-ROM
        to the terminal device <b>10</b> and installs the image processing program
        and the learning program, and reads the identification information ID of the
        image correct answer data used for learning of the learning discriminator
        from the recording medium. Further, the learned discriminator output unit
        <b>15</b> of the terminal device <b>10</b> records the parameter of the learned
        discriminator in the DVD-ROM, and distributes the parameter to an operator
        of the learning assistance device <b>20</b> by mailing or the like.<br />(74)
        Further, the learned discriminator acquisition unit <b>22</b> of the learning
        assistance device <b>20</b> reads the parameters of the learned discriminator
        recorded on the DVD-ROM. Furthermore, the discriminator output unit <b>27</b>
        of the learning assistance device <b>20</b> records the image processing program
        and the learning program on a DVD-ROM and sends the image processing program
        and the learning program to an operator of the terminal device <b>10</b> by
        mailing or the like.<br />(75) As described in detail above, in the present
        invention, accurate correct answer data of an image is automatically generated
        using a discriminator of which the performance has been improved using the
        medical images stored in each medical institution. Thus, it is possible to
        use a large amount of medical images for deep learning.<br />(76) Although
        the case where the learning assistance device and the terminal device function
        on a general-purpose computer has been described above, a dedicated circuit
        such as an application specific integrated circuit (ASIC) or field programmable
        gate arrays (FPGA) that permanently stores a program for executing some of
        functions may be provided. Alternatively, a program instruction stored in
        a dedicated circuit and a program instruction executed by a general-purpose
        CPU programmed to use a program of a dedicated circuit may be combined. As
        described above, the program instructions may be executed through any combination
        of hardware configurations of the computer.\",\"claimsHtml\":\"1. A learning
        assistance device comprising a processor configured to: output learning discriminators
        to a plurality of respective terminal devices; acquire a plurality of learned
        discriminators obtained by causing each of the learning discriminators provided
        in a plurality of respective terminal devices to perform learning at the respective
        terminal devices by using an image and correct answer data thereof stored
        in the respective terminal devices; and acquire a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, each of the plurality of discrimination
        results being obtained from each of the plurality of learned discriminators,
        determine correct answer data of the input image on the basis of the plurality
        of discrimination results, and output a new learning discriminator to the
        respective terminal device, the new learning discriminator being obtained
        by causing the learning discriminator to perform learning again using the
        input image and the determined correct answer data, wherein the processor
        repeatedly performs a process of acquiring the plurality of learned discriminators
        obtained by causing each of the learning discriminators, each of the learning
        discriminators being output from the learning assistance device to the respective
        terminal devices and being provided for the respective terminal devices, to
        perform learning at the respective terminal devices by using an image and
        correct answer data thereof stored in the respective terminal devices, determining
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired from the plurality of respective
        terminal devices to discriminate the new same input image, and outputting
        a new learning discriminator to the respective terminal device, the new learning
        discriminator being obtained by causing the learning discriminator output
        from the learning assistance device to the respective terminal devices at
        a last time to perform learning again using the new input image and the determined
        new correct answer data at the learning assistance device.  <br /> 2. The
        learning assistance device according to claim 1, wherein the processor outputs
        an actually operated discriminator learning the image and the correct answer
        data thereof used for learning by the new learning discriminator. <br /> 3.
        The learning assistance device according to claim 1, wherein the processor
        acquires the learned discriminator from the plurality of terminal devices
        over a network, and the processor outputs the new learning discriminator to
        the plurality of terminal devices over the network.  <br /> 4. The learning
        assistance device according to claim 1, wherein the processor determines a
        discrimination result having the largest number of same results among the
        plurality of discrimination results, as correct answer data of the input image.
        <br /> 5. The learning assistance device according to claim 1, wherein the
        processor determines a weight of each of the plurality of learned discriminators
        according to the terminal device learned by the learned discriminator, adds
        the weights of the learned discriminators having the same result among the
        discrimination results, and sets a discrimination result having the largest
        added weight as correct answer data of the input image. <br /> 6. The learning
        assistance device according to claim 5, wherein the processor determines the
        weight of each of the learned discriminators learned at each of the terminal
        devices according to the number of pieces of correct answer data learned by
        the learned discriminator at each terminal device, adds the weights of the
        learned discriminators having the same discrimination result, and sets a discrimination
        result having the largest added weight as correct answer data of the input
        image. <br /> 7. The learning assistance device according to claim 5, wherein
        the processor determines weights for types of cases of disease of the image
        learned by the respective learned discriminators with respect to the respective
        learned discriminators, adds the weights corresponding to the types of cases
        of disease of the input image in the learned discriminator having the same
        discrimination result, and sets a discrimination result having the largest
        added weight as correct answer data of the input image. <br /> 8. The learning
        assistance device according to claim 5, wherein the processor: evaluates a
        correct answer rate using an image set including a plurality of images serving
        as a reference with respect to the plurality of learned discriminators, and
        wherein the processor determines the weight of each of the plurality of learned
        discriminators according to the correct answer rate that is obtained, adds
        the weights of the respective learned discriminators having the same discrimination
        results, and sets the discrimination result having the largest added weight
        as correct answer data of the input image.  <br /> 9. A method of operating
        a learning assistance device including a processor, the method comprising:
        outputting learning discriminators to a plurality of respective terminal devices;
        acquiring a plurality of learned discriminators obtained by causing each of
        the learning discriminators provided in a plurality of respective terminal
        devices to perform learning at the respective terminal devices by using an
        image and correct answer data thereof stored in the respective terminal devices;
        acquiring a plurality of discrimination results obtained by causing each of
        the plurality of learned discriminators to discriminate the same input image,
        each of the plurality of discrimination results being obtained from each of
        the plurality of learned discriminators, determining correct answer data of
        the input image on the basis of the plurality of discrimination results, and
        outputting a new learning discriminator to the respective terminal device,
        the new learning discriminator being obtained by causing the learning discriminator
        to perform learning again using the input image and the determined correct
        answer data; and repeatedly performing a process of acquiring the plurality
        of learned discriminators obtained by causing each of the learning discriminators,
        each of the learning discriminators being output from the learning assistance
        device to the respective terminal devices and being provided for the respective
        terminal devices, to perform learning at the respective terminal devices by
        using an image and correct answer data thereof stored in the respective terminal
        devices, determining a new correct answer data of a new same input image different
        from the input image on the basis of a plurality of discrimination results
        obtained by causing the plurality of learned discriminators acquired from
        the respective terminal devices to discriminate the new same input image,
        and outputting a new learning discriminator to the respective terminal device,
        the new learning discriminator being obtained by causing the learning discriminator
        output from the learning assistance device to the respective terminal devices
        at a last time to perform learning again using the new input image and the
        determined new correct answer data at the learning assistance device.  <br
        /> 10. A non-transitory computer-readable recording medium storing therein
        a learning assistance program causing a computer to: output learning discriminators
        to a plurality of respective terminal devices; acquire a plurality of learned
        discriminators obtained by causing each of the learning discriminators provided
        in a plurality of respective terminal devices to perform learning at the respective
        terminal devices by using an image and correct answer data thereof stored
        in the respective terminal devices; and acquire a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, each of the plurality of discrimination
        results being obtained from each of the plurality of learned discriminators,
        determine correct answer data of the input image on the basis of the plurality
        of discrimination results, and output a new learning discriminator to the
        respective terminal device, the new learning discriminator being obtained
        by causing the learning discriminator to perform learning again using the
        input image and the determined correct answer data, repeatedly perform a process
        of acquiring the plurality of learned discriminators obtained by causing each
        of the learning discriminators, each of the learning discriminators being
        output from the learning assistance device to the respective terminal devices
        and being provided for the respective terminal devices, to perform learning
        at the respective terminal devices by using an image and correct answer data
        thereof stored in the respective terminal devices, determine a new correct
        answer data of a new same input image different from the input image on the
        basis of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired from the plurality of respective terminal
        devices to discriminate the new same input image, and output a new learning
        discriminator to the respective terminal device, the new learning discriminator
        being obtained by causing the learning discriminator output from the learning
        assistance device to the respective terminal devices at a last time to perform
        learning again using the new input image and the determined new correct answer
        data at the learning assistance device.\",\"briefHtml\":\"CROSS-REFERENCE
        TO RELATED APPLICATION<br />(1) This application claims priority from Japanese
        Patent Application No. 2017-187096, filed on Sep. 27, 2017, the disclosure
        of which is incorporated by reference herein in its entirety.<br />BACKGROUND<br
        />Field of the Invention<br />(2) The present invention relates to a learning
        assistance device that assists in generation of a discriminator using machine
        learning, a method of operating the learning assistance device, a learning
        assistance program, a learning assistance system, and a terminal device.<br
        />Related Art<br />(3) In the related art, machine learning has been used
        to learn features of data and perform recognition or classification of images
        or the like. In recent years, various learning schemes have been developed,
        a processing time has been shortened due to an improved processing capability
        of a computer, and deep learning in which a system learns features of image
        data or the like at a deeper level can be performed. By performing the deep
        learning, features of images or the like can be recognized with very high
        accuracy, and improvement of performance of discrimination is expected.<br
        />(4) In the medical field, artificial intelligence (AI) that recognizes features
        of images with high accuracy by performing learning using deep learning is
        desired. For deep learning, it is indispensable to perform learning using
        a large amount of high quality data according to purposes. Therefore, it is
        important to prepare learning data efficiently. Image data of a large number
        of cases of disease is accumulated in each medical institution with the spread
        of a picture archiving and communication system (PACS). Therefore, learning
        using image data of various cases of disease accumulated in each medical institution
        is considered.<br />(5) Further, in recent years, a technical level of artificial
        intelligence has been improving in a variety of fields, and the artificial
        intelligence is incorporated into a variety of services and started to be
        used and utilized. In particular, services provided to various edge terminals
        over a network are increasing. For example, JP2008-046729A discloses a device
        in which a learning model is incorporated into a moving image topic division
        device that automatically divides a moving image at a switching point of a
        topic. JP2008-046729A discloses that the learning model has been distributed
        to a client terminal, the topic division is automatically executed using the
        distributed learning model at each client terminal, and content corrected
        by a user for a result of the automatic topic division is fed back for updating
        of the learning model. After the feedback corrected by the user is accumulated
        in an integration module over a network, a learning model reconstructed using
        the accumulated feedback is distributed to each client terminal over the network
        again.<br />(6) However, in the medical field, since data to be learned is
        medical data of a patient, confidentiality is very high, and it is necessary
        to handle data carefully for use as learning data. Further, correct answer
        data is not attached to image data. Alternatively, even in a case where there
        is the correct answer data, the correct answer data is often managed without
        being associated with original image data. Therefore, it is difficult and
        costly to efficiently collect learning data to which the correct answer data
        is added.<br />SUMMARY<br />(7) Therefore, in order to solve the above-described
        problems, an object of the present invention is to provide a learning assistance
        device which enables learning of a large amount and a variety of learning
        data necessary for deep learning in a medical field, a method of operating
        the learning assistance device, a learning assistance program, a learning
        assistance system, and a terminal device.<br />(8) A learning assistance device
        of the present invention comprises: a learned discriminator acquisition unit
        that acquires a plurality of learned discriminators obtained by causing each
        of learning discriminators provided in a plurality of respective terminal
        devices to perform learning using an image and correct answer data thereof;
        and a discriminator output unit that acquires a plurality of discrimination
        results obtained by causing each of the plurality of learned discriminators
        to discriminate the same input image, determines correct answer data of the
        input image on the basis of the plurality of discrimination results, and outputs
        a new learning discriminator obtained by causing the learning discriminator
        to perform learning again using the input image and the determined correct
        answer data, wherein the learning assistance device repeatedly performs a
        process in which the learning discriminator acquisition unit acquires the
        plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, and the discriminator output unit determines a new correct answer
        data of a new same input image different from the input image on the basis
        of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired by the learned discriminator acquisition
        unit to discriminate the new same input image, and outputs a new learning
        discriminator obtained by causing the learning discriminator output by the
        discriminator output unit to perform learning again using the new input image
        and the determined new correct answer data.<br />(9) A method of operating
        a learning assistance device of the present invention is a method of operating
        a learning assistance device including a learned discriminator acquisition
        unit and a discriminator output unit, the method comprising: acquiring a plurality
        of learned discriminators obtained by causing each of learning discriminators
        provided in a plurality of respective terminal devices to perform learning
        using an image and correct answer data thereof by the learned discriminator
        acquisition unit; acquiring a plurality of discrimination results obtained
        by causing each of the plurality of learned discriminators to discriminate
        the same input image, determines correct answer data of the input image on
        the basis of the plurality of discrimination results, and outputs a new learning
        discriminator obtained by causing the learning discriminator to perform learning
        again using the input image and the determined correct answer data by the
        discriminator output unit; and repeatedly performing a process in which the
        learned discriminator acquisition unit acquires the plurality of learned discriminators
        obtained by causing the each of learning discriminators, the each of learning
        discriminators being output from the discriminator output unit and being provided
        for each of the plurality of terminal devices, to perform learning using an
        image and correct answer data thereof, and the discriminator output unit determines
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired by the learned discriminator
        acquisition unit to discriminate the new same input image, and outputs a new
        learning discriminator obtained by causing the learning discriminator output
        by the discriminator output unit to perform learning again using the new input
        image and the determined new correct answer data.<br />(10) A learning assistance
        program according to the present invention causes a computer to function as:
        a learned discriminator acquisition unit that acquires a plurality of learned
        discriminators obtained by causing each of learning discriminators provided
        in a plurality of respective terminal devices to perform learning using an
        image and correct answer data thereof; and a discriminator output unit that
        acquires a plurality of discrimination results obtained by causing each of
        the plurality of learned discriminators to discriminate the same input image,
        determines correct answer data of the input image on the basis of the plurality
        of discrimination results, and outputs a new learning discriminator obtained
        by causing the learning discriminator to perform learning again using the
        input image and the determined correct answer data, wherein the program causes
        a process in which the learned discriminator acquisition unit acquires the
        plurality of learned discriminators obtained by causing the each of learning
        discriminators, the each of learning discriminators being output from the
        discriminator output unit and being provided for each of the plurality of
        terminal devices, to perform learning using an image and correct answer data
        thereof, and the discriminator output unit determines a new correct answer
        data of a new same input image different from the input image on the basis
        of a plurality of discrimination results obtained by causing the plurality
        of learned discriminators acquired by the learned discriminator acquisition
        unit to discriminate the new same input image, and outputs a new learning
        discriminator obtained by causing the learning discriminator output by the
        discriminator output unit to perform learning again using the new input image
        and the determined new correct answer data, to be repeatedly performed.<br
        />(11) \u201CAcquire a learned discriminator\u201D may be acquiring the learned
        discriminator or may be receiving a parameter of the discriminator and setting
        the parameter in a prepared discriminator to acquire the learned discriminator.
        For example, in a case where the discriminator is a multilayered neural network,
        a program in which a learned multilayered neural network is incorporated may
        be acquired or a weight of coupling between layers of units of the multilayered
        neural network may be acquired as a parameter and the parameter may be set
        in the prepared multilayered neural network, so that the learned multilayered
        neural network can be acquired.<br />(12) Further, the discriminator output
        unit further outputs an actually operated discriminator learning the image
        and the correct answer data thereof used for learning by the new learning
        discriminator.<br />(13) The \u201Cactually operated discriminator\u201D is
        a discriminator capable of acquiring a discrimination result of input image
        data, which cannot perform additional learning, and the \u201Clearning discriminator\u201D
        is a discriminator that can perform additional learning using an image and
        correct answer data of the image. Further, the actually operated discriminator
        to be output is a discriminator caused to perform learning using an image
        and correct answer data of the image that are all the same as the image and
        the correct answer data of the image learned by the output learning discriminator.<br
        />(14) Further, the learned discriminator acquisition unit may acquire the
        learned discriminator from the plurality of terminal devices over a network,
        and the discriminator output unit may output the new learning discriminator
        to the plurality of terminal devices over the network.<br />(15) Further,
        the discriminator output unit may determine a discrimination result having
        the largest number of same results among the plurality of discrimination results,
        as correct answer data of the input image.<br />(16) Further, the discriminator
        output unit may determine a weight of each of the plurality of learned discriminators
        according to the terminal device learned by the learned discriminator, adds
        the weights of the learned discriminators having the same result among the
        discrimination results, and sets a discrimination result having the largest
        added weight as correct answer data of the input image.<br />(17) Further,
        the discriminator output unit may determine the weight of each of the learned
        discriminators learned at each of the terminal devices according to the number
        of pieces of correct answer data learned by the learned discriminator at each
        terminal device, add the weights of the learned discriminators having the
        same discrimination result, and set a discrimination result having the largest
        added weight as correct answer data of the input image.<br />(18) Further,
        the discriminator output unit may determine weights for types of cases of
        disease of the image learned by the respective learned discriminators with
        respect to the respective learned discriminators, add the weights corresponding
        to the types of cases of disease of the input image in the learned discriminator
        having the same discrimination result, and set a discrimination result having
        the largest added weight as correct answer data of the input image.<br />(19)
        Further, the learning assistance device may further comprise an evaluation
        unit that evaluates a correct answer rate using an image set including a plurality
        of images serving as a reference with respect to the plurality of learned
        discriminators, wherein the discriminator output unit may determine the weight
        of each of the plurality of learned discriminators according to the correct
        answer rate obtained by the evaluation unit, add the weights of the respective
        learned discriminators having the same discrimination results, and set the
        discrimination result having the largest added weight as correct answer data
        of the input image.<br />(20) A learning assistance system according to the
        present invention is a learning assistance system in which a learning assistance
        device and a plurality of terminal devices are connected over a network, wherein
        the terminal device includes a learned discriminator output unit that outputs
        a learned discriminator obtained by causing a learning discriminator to perform
        learning using an image and correct answer data thereof over the network,
        the learning assistance device includes a learned discriminator acquisition
        unit that acquires a plurality of learned discriminators from the plurality
        of terminal devices over the network, and a discriminator output unit that
        acquires a plurality of discrimination results obtained by causing the plurality
        of learned discriminators to discriminate the same input image, determines
        correct answer data of the input image on the basis of the plurality of discrimination
        results, and outputs a new learning discriminator obtained by causing the
        learning discriminator to perform learning again using the input image and
        the determined correct answer data to the plurality of terminal devices over
        the network, and the terminal device includes a discriminator acquisition
        unit that receives the learning discriminator output from the learning assistance
        device over the network.<br />(21) Further, in the learning assistance system,
        the discriminator acquisition unit may further acquire an actually operated
        discriminator learning the same image and correct answer data of the image
        as those of the learning discriminator output from the learning assistance
        device over a network, and the terminal device may further include a discrimination
        result acquisition unit that acquires a discrimination result of discriminating
        an image that is a discrimination target using the actually operated discriminator.<br
        />(22) A terminal device according to the present invention comprises a discriminator
        acquisition unit that acquires a learning discriminator, and a learned actually
        operated discriminator learned using the same image and correct answer data
        of the image as those of the learning discriminator; a discrimination result
        acquisition unit that acquires a discrimination result of discriminating an
        image that is a discrimination target using the actually operated discriminator;
        and a learned discriminator output unit that outputs a learned discriminator
        obtained by causing the learning discriminator to perform learning using an
        image and correct answer data thereof.<br />(23) Further, in the terminal
        device, the discriminator acquisition unit may acquire the learning discriminator
        and the actually operated discriminator from a learning assistance device
        over a network, and the learned discriminator output unit may send and output
        the learned discriminator over the network.<br />(24) Another learning assistance
        device of the present invention comprises a memory that stores instructions
        to be executed by a computer, and a processor configured to execute the stored
        instructions, wherein the processor executes an acquisition process of acquiring
        a plurality of learned discriminators obtained by causing each of learning
        discriminators provided in a plurality of respective terminal devices to perform
        learning using an image and correct answer data thereof, and an output process
        of acquiring a plurality of discrimination results obtained by causing each
        of the plurality of learned discriminators to discriminate the same input
        image, determining correct answer data of the input image on the basis of
        the plurality of discrimination results, and outputting a new learning discriminator
        obtained by causing the learning discriminator to perform learning again using
        the input image and the determined correct answer data, and the learning assistance
        device repeatedly performs a process of acquiring the plurality of learned
        discriminators obtained by causing the each of learning discriminators, the
        each of learning discriminators being output from the discriminator output
        unit and being provided for each of the plurality of terminal devices, to
        perform learning using an image and correct answer data thereof, determining
        a new correct answer data of a new same input image different from the input
        image on the basis of a plurality of discrimination results obtained by causing
        the plurality of learned discriminators acquired by the learned discriminator
        acquisition unit to discriminate the new same input image, and outputting
        a new learning discriminator obtained by causing the learning discriminator
        output by the discriminator output unit to perform learning again using the
        new input image and the determined new correct answer data.<br />(25) According
        to the present invention, the learning assistance device acquires the plurality
        of learned discriminators obtained by causing learning discriminators provided
        in a plurality of respective terminal devices to perform learning using image
        correct answer data, acquires the plurality of discrimination results of discriminating
        the same input image, determines the correct answer data of the input image
        on the basis of the plurality of discrimination results, and causes the discriminator
        to perform learning using the input image and the determined correct answer
        data. Therefore, it is possible to automatically generate the data for learning
        from the image to which the correct answer data is not attached, perform deep
        learning using a large amount of images, and improve performance of discrimination.\",\"backgroundTextHtml\":null,\"subHeadingM0Html\":null,\"subHeadingM1Html\":null,\"subHeadingM2Html\":null,\"subHeadingM3Html\":null,\"subHeadingM4Html\":null,\"subHeadingM5Html\":null,\"subHeadingM6Html\":null,\"usClassIssued\":null,\"issuedUsDigestRefClassifi\":null,\"datePublYear\":\"2021\",\"applicationYear\":\"2018\",\"pfDerwentWeekYear\":null,\"pfApplicationYear\":null,\"pfPublYear\":null,\"reissueApplNumber\":null,\"abstractHeader\":null,\"abstractedPublicationDerwent\":null,\"affidavit130BFlag\":null,\"affidavit130BText\":null,\"applicantGroup\":[\"FUJIFILM
        Corporation Tokyo JP\"],\"applicantHeader\":null,\"applicationFilingDateInt\":20180906,\"applicationFilingDateIntKwicHits\":null,\"applicationRefFilingType\":\"utility\",\"applicationReferenceGroup\":null,\"applicationSeriesAndNumber\":\"16123456\",\"applicationSeriesCode\":\"16\",\"assignee1\":null,\"assigneeDescriptiveText\":null,\"patentAssigneeTerms\":null,\"associateAttorneyName\":null,\"attorneyName\":[\"Birch,
        Stewart, Kolasch & Birch, LLP\"],\"biologicalDepositInformation\":null,\"applicationType\":null,\"unlinkedDerwentRegistryNumber\":null,\"unlinkedRingIndexNumbersRarerFragments\":null,\"claimStatement\":\"What
        is claimed is:\",\"claimsTextAmended\":null,\"continuedProsecutionAppl\":null,\"cpcAdditionalLong\":null,\"cpcCisClassificationOrig\":null,\"cpcCombinationClassificationOrig\":null,\"cpcInventive\":[\"G16H30/40
        20180101\",\"G06N3/08 20130101\",\"G06V10/7784 20220101\",\"G06K9/6256 20130101\",\"G06N20/00
        20190101\",\"G16H30/20 20180101\",\"G06V10/774 20220101\",\"G06K9/6263 20130101\",\"G16H50/70
        20180101\",\"G06N3/0454 20130101\",\"G06N3/0427 20130101\",\"G06K9/6262 20130101\"],\"cpcInventiveCurrentDateKwicHits\":null,\"cpcAdditional\":null,\"cpcAdditionalCurrentDateKwicHits\":null,\"cpcOrigClassificationGroup\":[\"G
        G06K G06K9/6256 20130101 F I 20210126 US\",\"G G06K G06K9/6263 20130101 L
        I 20210126 US\",\"G G06N G06N3/0427 20130101 L I 20210126 US\",\"G G06N G06N3/0454
        20130101 L I 20210126 US\",\"G G06N G06N3/08 20130101 L I 20210126 US\",\"G
        G16H G16H30/40 20180101 L I 20210126 US\",\"G G16H G16H50/70 20180101 L I
        20210126 US\"],\"curIntlPatentClassificationGroup\":null,\"curUsClassificationUsPrimaryClass\":null,\"curUsClassificationUsSecondaryClass\":null,\"customerNumber\":null,\"depositAccessionNumber\":null,\"depositDescription\":null,\"derwentClassAlpha\":null,\"designatedstatesRouteGroup\":null,\"docAccessionNumber\":null,\"drawingDescription\":null,\"editionField\":null,\"exchangeWeek\":null,\"exemplaryClaimNumber\":[\"1\"],\"familyIdentifierOrig\":null,\"fieldOfSearchCpcClassification\":[\"G06K
        9/6256\",\"G06K 9/6263\",\"G16H 30/40\",\"G16H 50/70\",\"G06N 3/08\",\"G06N
        3/0454\",\"G06N 3/0427\"],\"fieldOfSearchCpcMainClass\":[\"G06K\",\"G06K\",\"G16H\",\"G16H\",\"G06N\",\"G06N\",\"G06N\"],\"fieldOfSearchIpcMainClass\":null,\"fieldOfSearchIpcMainClassSubclass\":null,\"fieldOfSearchSubclasses\":[\"159\"],\"foreignRefGroup\":[\"JP
        2001-319226 A 20011100 cited by applicant\",\"JP 2008-46729 A 20080200 cited
        by applicant\"],\"foreignRefPubDate\":[\"20011100\",\"20080200\"],\"foreignRefPubDateKwicHits\":[\"20011100\",\"20080200\"],\"foreignRefCitationClassification\":[\"N/A\",\"N/A\"],\"foreignRefPatentNumber\":[\"2001-319226\",\"2008-46729\"],\"foreignRefCitationCpc\":[\"N/A\",\"N/A\"],\"foreignRefCountryCode\":[\"JP\",\"JP\"],\"iceXmlIndicator\":\"Y\",\"internationalClassificationHeader\":null,\"internationalClassificationInformationalGroup\":null,\"intlPubClassificationGroup\":[\"20060101
        A G06K G06K9/62 F I B US H 20210126\",\"20060101 A G06N G06N3/04 L I B US
        H 20210126\",\"20180101 A G16H G16H50/70 L I B US H 20210126\",\"20060101
        A G06N G06N3/08 L I B US H 20210126\",\"20180101 A G16H G16H30/40 L I B US
        H 20210126\"],\"intlPubClassificationNonInvention\":null,\"inventorCitizenship\":null,\"inventorCorrection\":null,\"inventorDeceased\":null,\"inventorStreetAddress\":null,\"inventorText\":null,\"jpoFiClassification\":null,\"legalRepresentativeCity\":null,\"legalRepresentativeCountry\":\"[]\",\"legalRepresentativeName\":null,\"legalRepresentativePostcode\":null,\"legalRepresentativeState\":null,\"legalRepresentativeStreetAddress\":null,\"legalRepresentativeText\":null,\"messengerDocsFlag\":null,\"newRecordPatentDerwent\":null,\"numberOfClaims\":\"10\",\"numberOfDrawingSheets\":\"9\",\"numberOfFigures\":\"9\",\"numberOfPagesInSpecification\":null,\"numberOfPagesOfSpecification\":null,\"objectContents\":null,\"objectDescription\":null,\"parentDocCountry\":null,\"parentGrantDocCountry\":null,\"patentBibliographicHeader\":null,\"pctOrRegionalPublishingSerial\":null,\"pfDerwentWeekNum\":null,\"principalAttorneyName\":null,\"priorityApplicationCountry\":null,\"priorityClaimsCountry\":[\"JP\"],\"priorityNumberDerived\":[\"2017JP-2017-187096\"],\"publicationIssueNumber\":null,\"refCitedPatentDocNumber\":null,\"refCitedPatentDocDate\":null,\"refCitedPatentDocKindCode\":null,\"referenceCitedCode\":null,\"referenceCitedGroup\":null,\"referenceCitedSearchPhase\":null,\"referenceCitedText\":null,\"registrationNumber\":null,\"reissueApplCountry\":null,\"reissueParentKind\":null,\"reissueParentNumber\":null,\"reissueParentPubCountry\":null,\"reissuePatentGroup\":null,\"reissuePatentParentStatus\":null,\"reissuedPatentApplCountry\":null,\"reissuedPatentApplKind\":null,\"reissuedPatentApplNumber\":null,\"relatedApplChildPatentCountry\":null,\"relatedApplChildPatentName\":null,\"relatedApplChildPatentNumber\":null,\"relatedApplCountryCode\":null,\"relatedApplParentGrantPatentKind\":null,\"relatedApplParentGrantPatentName\":null,\"relatedApplParentPatentKind\":null,\"relatedApplParentPatentName\":null,\"relatedApplParentPctDoc\":null,\"relatedApplParentStatusCode\":null,\"relatedApplPatentNumber\":null,\"relatedApplRelatedPub\":null,\"relatedApplTypeOfCorrection\":null,\"rule47Flag\":null,\"selectedDrawingCharacter\":null,\"selectedDrawingFigure\":null,\"statutoryInventionText\":null,\"termOfExtension\":\"127\",\"termOfPatentGrant\":null,\"titleTermsData\":null,\"additionalIndexingTerm\":null,\"applicationYearSearch\":\"2018\",\"pfApplicationYearSearch\":null,\"assigneeCountry\":[\"JP\"],\"certOfCorrectionFlag\":null,\"citedPatentLiteratureAddressInformation\":null,\"citedPatentLiteratureClassificationIpc\":null,\"citedPatentLiteratureOrganizationName\":null,\"citedPatentLiteratureRefNumber\":null,\"crossReferenceNumber\":null,\"country\":\"US\",\"cpiManualCodes\":null,\"cpiSecondaryAccessionNumber\":null,\"curIntlPatentAllClassificationLong\":null,\"currentUsOriginalClassificationLong\":null,\"datePublSearch\":\"2021-01-26T00:00:00.000+00:00\",\"datePublYearSearch\":\"2021\",\"epiManualCodes\":null,\"fieldOfSearchMainClassNational\":[\"382\"],\"inventorCountry\":[\"JP\"],\"ipcAllMainClassification\":[\"G06N\",\"G16H\",\"G06K\"],\"issuedUsClassificationFull\":null,\"issuedUsDigestRefClassification\":null,\"jpoFiCurrentAdditionalClassification\":null,\"jpoFiCurrentInventiveClassification\":null,\"legalFirmName\":[\"Birch,
        Stewart, Kolasch & Birch, LLP\"],\"locarnoMainClassification\":null,\"nonCpiSecondaryAccessionNumber\":null,\"objectId\":null,\"otherRefPub\":[\"Japanese
        Office Action for corresponding Japanese Application No. 2017-187096, dated
        Jul. 21, 2020, with English translation. cited by applicant\\n<br />\"],\"pageNumber\":null,\"patentAssigneeCode\":null,\"patentAssigneeNameTotal\":null,\"patentFamilyDate\":null,\"patentFamilyDocNumber\":null,\"patentFamilyKind\":null,\"patentFamilyKindCode\":null,\"patentFamilyLanguage\":null,\"patentFamilyName\":null,\"patentNumberOfLocalApplication\":null,\"pct102eDate\":null,\"pct371c124Date\":null,\"pct371c124DateKwicHits\":null,\"pctFilingDate\":null,\"pctFilingDateKwicHits\":null,\"pctFilingDocCountryCode\":null,\"pctFilingKind\":null,\"pctFilingNumber\":null,\"pctName\":null,\"pctOrRegionalPublishingCountry\":null,\"pctOrRegionalPublishingKind\":null,\"pctOrRegionalPublishingName\":null,\"pctOrRegionalPublishingText\":null,\"pctPubDate\":null,\"pctPubDateKwicHits\":null,\"pctPubDocIdentifier\":null,\"pctPubNumber\":null,\"pfApplicationDateSearch\":null,\"pfApplicationType\":null,\"pfDerwentWeekDate\":null,\"pfPublDateSearch\":null,\"pfPublDateSearchKwicHits\":null,\"pfPublYearSearch\":null,\"polymerIndexingCodes\":null,\"polymerMultipunchCodeRecordNumber\":null,\"polymerMultipunchCodes\":null,\"priorPublishedDocCountryCode\":[\"US\"],\"priorPublishedDocDate\":[\"2019-03-28T00:00:00Z\"],\"priorPublishedDocDateKwicHits\":null,\"priorPublishedDocIdentifier\":[\"US
        20190095759 A1\"],\"priorPublishedDocKindCode\":[\"A1\"],\"priorPublishedDocNumber\":[\"20190095759\"],\"priorityApplYear\":[\"2017\"],\"priorityApplicationDate\":null,\"priorityClaimsDateSearch\":null,\"priorityClaimsDocNumber\":[\"2017-187096\"],\"priorityPatentDid\":null,\"priorityPatentNumber\":null,\"ptabCertFlag\":null,\"pubRefCountryCode\":null,\"pubRefDocNumber\":\"10902286\",\"pubRefDocNumber1\":\"10902286\",\"publicationData\":null,\"recordPatentNumber\":null,\"reexaminationFlag\":null,\"refCitedOthers\":null,\"refCitedPatentDocCountryCode\":null,\"refCitedPatentDocName\":null,\"refCitedPatentRelevantPassage\":null,\"reissueParentIssueDate\":null,\"reissuedPatentApplFilingDate\":null,\"relatedAccessionNumbers\":null,\"relatedApplChildPatentDate\":null,\"relatedApplFilingDateKwicHits\":null,\"relatedApplNumber\":null,\"relatedApplPatentIssueDate\":null,\"relatedApplPatentIssueDateKwicHits\":null,\"relatedDocumentKindCode\":null,\"securityLegend\":null,\"sequenceCwu\":null,\"sequenceListNewRules\":null,\"sequenceListOldRules\":null,\"sequencesListText\":null,\"standardTitleTerms\":null,\"supplementalExaminationFlag\":null,\"usBotanicLatinName\":null,\"usBotanicVariety\":null,\"usRefClassification\":[\"382/226\",\"N/A\"],\"usRefCpcClassification\":[\"G06K
        9/00228\",\"N/A\"],\"usRefGroup\":[\"US 2007/0217688 A1 Sabe 20070900 cited
        by examiner G06K 9/00228 382/226\",\"US 2015/0242760 A1 Miao et al. 20150800
        cited by applicant\"],\"usRefIssueDate\":[\"20070900\",\"20150800\"],\"usRefIssueDateKwicHits\":[\"20070900\",\"20150800\"],\"usRefPatenteeName\":[\"Sabe\",\"Miao
        et al.\"],\"volumeNumber\":null,\"correspondenceNameAddress\":null,\"correspondenceAddressCustomerNumber\":null,\"ibmtdbAccessionNumber\":null,\"inventorsName\":[\"Kanada;
        Shoji\"],\"applicationKindCode\":\"B2\",\"inventorNameDerived\":null,\"intlPubClassificationClass\":[\"G06K\",\"G06N\",\"G16H\",\"G06N\",\"G16H\"],\"issuedUsOrigClassification\":null,\"curCpcSubclassFull\":[\"G06N\",\"G16H\",\"G06V\",\"G06K\"],\"cpcCurAdditionalClass\":null,\"cpcCurInventiveClass\":[\"G16H\",\"G06N\",\"G06V\",\"G06K\",\"G06N\",\"G16H\",\"G06V\",\"G06K\",\"G16H\",\"G06N\",\"G06N\",\"G06K\"],\"cpcCurClassificationGroup\":[\"G
        G16H G16H30/40 20180101 L I B H 20191011 US\",\"G G06N G06N3/08 20130101 L
        I B H 20191011 US\",\"G G06V G06V10/7784 20220101 L I B H 20220712 EP\",\"G
        G06K G06K9/6256 20130101 F I B H 20190822 US\",\"G G06N G06N20/00 20190101
        L I B C 20210127 US\",\"G G16H G16H30/20 20180101 L I B H 20210127 US\",\"G
        G06V G06V10/774 20220101 L I B H 20220715 EP\",\"G G06K G06K9/6263 20130101
        L I B H 20191011 US\",\"G G16H G16H50/70 20180101 L I B H 20191011 US\",\"G
        G06N G06N3/0454 20130101 L I B H 20191011 US\",\"G G06N G06N3/0427 20130101
        L I B H 20191011 US\",\"G G06K G06K9/6262 20130101 L I B H 20210127 US\"],\"curCpcClassificationFull\":[\"G06N20/00
        20190101\",\"G06V10/774 20220101\",\"G16H30/20 20180101\",\"G06K9/6263 20130101\",\"G06N3/08
        20130101\",\"G16H50/70 20180101\",\"G16H30/40 20180101\",\"G06K9/6256 20130101\",\"G06K9/6262
        20130101\",\"G06V10/7784 20220101\",\"G06N3/0454 20130101\",\"G06N3/0427 20130101\"],\"cpcCombinationClassificationCur\":null,\"cpcCombinationTallyCur\":null,\"intlFurtherClassification\":null,\"currentUsPatentClass\":[\"1\"],\"internationalClassificationInfom\":null,\"cpcOrigInventvClssifHlghts\":[\"G06K9/6256
        20130101\",\"G06K9/6263 20130101\",\"G06N3/0427 20130101\",\"G06N3/0454 20130101\",\"G06N3/08
        20130101\",\"G16H30/40 20180101\",\"G16H50/70 20180101\"],\"idWithoutSolrPartition\":\"US-US-10902286\",\"curIntlPatentClassifictionPrimaryDateKwicHits\":null,\"curIntlPatentClssifSecHlights\":null,\"publicationReferenceDocumentNumberOne\":\"10902286\",\"descriptionStart\":11,\"descriptionEnd\":18}"
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:53:00 GMT
      Server-Timing:
      - intid;desc=dcdc9ad7afbbdca1
    status:
      code: 200
      message: ''
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 2168779, "hl_snippets": "2", "op": "OR", "q": "\"10902286\".PN.",
      "queryName": "\"10902286\".PN.", "highlights": "1", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "USPAT", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "\"10902286\".PN."}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '715'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":2168779,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"USPAT","countryCodes":[]}],"q":"\"10902286\".PN.","queryName":"\"10902286\".PN.","userEnteredQuery":"\"10902286\".PN.","viewName":"tile","op":"OR","highlights":"1","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"\"10902286\".PN.","error":null,"terms":["\"10902286\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":23,"highlightingTime":0,"cursorMarker":"AoJwwK3/nfcCNzY1ODA3Njk2IVVTLVVTLTEwOTAyMjg2","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-10902286-B2","publicationReferenceDocumentNumber":"10902286","compositeId":"65807696!US-US-10902286","publicationReferenceDocumentNumber1":"10902286","datePublishedKwicHits":null,"datePublished":"2021-01-26T00:00:00Z","inventionTitle":"Learning
        assistance device, method of operating learning assistance device, learning
        assistance program, learning assistance system, and terminal device","type":"USPAT","mainClassificationCode":"1/1","applicantName":["FUJIFILM
        Corporation"],"assigneeName":["FUJIFILM Corporation"],"uspcFullClassificationFlattened":null,"ipcCodeFlattened":"G06K9/62;G06N3/04","cpcInventiveFlattened":"G16H30/40;G06N3/08;G06V10/7784;G06K9/6256;G06N20/00;G16H30/20;G06V10/774;G06K9/6263;G16H50/70;G06N3/0454;G06N3/0427;G06K9/6262","cpcAdditionalFlattened":null,"applicationFilingDate":["2018-09-06T00:00:00Z"],"applicationFilingDateKwicHits":null,"relatedApplFilingDate":null,"primaryExaminer":"Sabouri;
        Mazda","assistantExaminer":null,"applicationNumber":"16/123456","frontPageStart":1,"frontPageEnd":1,"drawingsStart":2,"drawingsEnd":10,"specificationStart":11,"specificationEnd":18,"claimsStart":18,"claimsEnd":20,"abstractStart":1,"abstractEnd":1,"bibStart":1,"bibEnd":1,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":20,"pageCountDisplay":"20","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/G86/022/109","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":"Kanada;
        Shoji","familyIdentifierCur":65807696,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":["2017-09-27T00:00:00Z"],"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        10902286 B2","derwentAccessionNumber":null,"documentSize":88828,"score":14.152454,"governmentInterest":null,"kindCode":["B2"],"urpn":["2007/0217688","2015/0242760"],"urpnCode":["2007/0217688","2015/0242760"],"publicationReferenceDocumentNumberOne":"10902286","descriptionStart":11,"descriptionEnd":18}],"qtime":20}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:54:42 GMT
      Server-Timing:
      - intid;desc=b6210b11dc566e37
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 17:54:42 GMT
      apigw-requestid:
      - fDZHcgdQIAMEPnQ=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/application/US/16123456
  response:
    body:
      string: '{"country":"US","internal":"false","corrAppNum":"16123456","id":"16123456","type":"application","list":[{"appNum":"2017187096","appDate":1506470400000,"pubList":[{"pubCountry":"JP","pubNum":"2019061579","pubDate":1555545600000,"kindCode":"A","pubDateStr":"04/18/2019"},{"pubCountry":"JP","pubNum":"6768620","pubDate":1602633600000,"kindCode":"B2","pubDateStr":"10/14/2020"}],"countryCode":"JP","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/27/2017","docListMsg":null,"docNum":{"country":"JP","docNumber":"2017187096","format":null,"date":"09/27/2017","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"16123456","appDate":1536192000000,"pubList":[{"pubCountry":"US","pubNum":"20190095759","pubDate":1553731200000,"kindCode":"A1","pubDateStr":"03/28/2019"},{"pubCountry":"US","pubNum":"10902286","pubDate":1611619200000,"kindCode":"B2","pubDateStr":"01/26/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"09/06/2018","docListMsg":null,"docNum":{"country":"US","docNumber":"201816123456","format":null,"date":"09/06/2018","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"17130468","appDate":1608595200000,"pubList":[{"pubCountry":"US","pubNum":"20210110206","pubDate":1618444800000,"kindCode":"A1","pubDateStr":"04/15/2021"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"US","docNumber":"16123456","kindCode":"A"},{"country":"US","docNumber":"17130468","kindCode":"A"},{"country":"JP","docNumber":"2017187096","kindCode":"A"}],"appDateStr":"12/22/2020","docListMsg":null,"docNum":{"country":"US","docNumber":"202017130468","format":null,"date":"12/22/2020","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '1885'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:54:42 GMT
      apigw-requestid:
      - fDZHcgVloAMEPbg=
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      Accept:
      - application/xml
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://assignment-api.uspto.gov/patent/lookup?filter=ApplicationNumber&query=16123456&rows=20&start=0&sort=&facet=False
  response:
    body:
      string: "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n\n<lst name=\"responseHeader\">\n
        \ <bool name=\"zkConnected\">true</bool>\n  <int name=\"status\">0</int>\n
        \ <int name=\"QTime\">12618</int>\n  <lst name=\"params\">\n    <str name=\"q\">*:*</str>\n
        \   <str name=\"qt\">/apisearch</str>\n    <str name=\"start\">0</str>\n    <str
        name=\"sort\">patAssignorEarliestExDate desc</str>\n    <str name=\"fq\">applNum:16123456</str>\n
        \   <str name=\"rows\">20</str>\n    <str name=\"wt\">javabin</str>\n    <str
        name=\"version\">2</str>\n  </lst>\n</lst>\n<result name=\"response\" numFound=\"1\"
        start=\"0\">\n  <doc>\n    <str name=\"id\">46816-108</str>\n    <str name=\"displayId\">046816-0108</str>\n
        \   <str name=\"reelNo\">46816</str>\n    <str name=\"frameNo\">108</str>\n
        \   <date name=\"lastUpdateDate\">2021-01-26T07:00:52Z</date>\n    <str name=\"purgeIndicator\">N</str>\n
        \   <date name=\"recordedDate\">2018-09-07T05:00:00Z</date>\n    <str name=\"pageCount\">3</str>\n
        \   <str name=\"conveyanceText\">ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT
        FOR DETAILS).</str>\n    <str name=\"assignmentRecordHasImages\">Y</str>\n
        \   <str name=\"attorneyDockNum\">1982-1042PUS1</str>\n    <str name=\"corrName\">BIRCH,
        STEWART, KOLASCH &amp; BIRCH, LLP</str>\n    <str name=\"corrAddress1\">8110
        GATEHOUSE ROAD, SUITE 100E</str>\n    <str name=\"corrAddress2\">FALLS CHURCH,
        VA 22042</str>\n    <date name=\"patAssignorEarliestExDate\">2018-07-11T05:00:00Z</date>\n
        \   <arr name=\"patAssignorName\">\n      <str>KANADA, SHOJI</str>\n    </arr>\n
        \   <arr name=\"patAssignorExDate\">\n      <date>2018-07-11T05:00:00Z</date>\n
        \   </arr>\n    <arr name=\"patAssignorDateAck\">\n      <str>0001-01-01T00:00:00Z</str>\n
        \   </arr>\n    <arr name=\"patAssigneeName\">\n      <str>FUJIFILM CORPORATION</str>\n
        \   </arr>\n    <arr name=\"patAssigneeAddress1\">\n      <str>26-30, NISHIAZABU
        2-CHOME, MINATO-KU</str>\n    </arr>\n    <arr name=\"patAssigneeAddress2\">\n
        \     <str>NULL</str>\n    </arr>\n    <arr name=\"patAssigneeCity\">\n      <str>TOKYO</str>\n
        \   </arr>\n    <arr name=\"patAssigneeState\">\n      <str>NULL</str>\n    </arr>\n
        \   <arr name=\"patAssigneeCountryName\">\n      <str>JAPAN</str>\n    </arr>\n
        \   <arr name=\"patAssigneePostcode\">\n      <str>106-8620</str>\n    </arr>\n
        \   <arr name=\"inventionTitle\">\n      <str>LEARNING ASSISTANCE DEVICE,
        METHOD OF OPERATING LEARNING ASSISTANCE DEVICE, LEARNING ASSISTANCE PROGRAM,
        LEARNING ASSISTANCE SYSTEM, AND TERMINAL DEVICE</str>\n    </arr>\n    <arr
        name=\"inventionTitleLang\">\n      <str>en</str>\n    </arr>\n    <arr name=\"applNum\">\n
        \     <str>16123456</str>\n    </arr>\n    <arr name=\"filingDate\">\n      <date>2018-09-06T05:00:00Z</date>\n
        \   </arr>\n    <arr name=\"intlPublDate\">\n      <date>0001-01-01T00:00:00Z</date>\n
        \   </arr>\n    <arr name=\"intlRegNum\">\n      <str>NULL</str>\n    </arr>\n
        \   <arr name=\"inventors\">\n      <str>Shoji KANADA</str>\n    </arr>\n
        \   <arr name=\"issueDate\">\n      <date>2021-01-26T05:00:00Z</date>\n    </arr>\n
        \   <arr name=\"patNum\">\n      <str>10902286</str>\n    </arr>\n    <arr
        name=\"pctNum\">\n      <str>NULL</str>\n    </arr>\n    <arr name=\"publDate\">\n
        \     <date>2019-03-28T05:00:00Z</date>\n    </arr>\n    <arr name=\"publNum\">\n
        \     <str>20190095759</str>\n    </arr>\n    <int name=\"patAssignorNameSize\">1</int>\n
        \   <int name=\"patAssignorNameTypeSize\">1</int>\n    <int name=\"patAssignorExDateSize\">1</int>\n
        \   <int name=\"patAssignorDateAckSize\">1</int>\n    <int name=\"patAssigneeNameSize\">2</int>\n
        \   <int name=\"patAssigneeNameTypeSize\">0</int>\n    <int name=\"patAssigneeAddress1Size\">1</int>\n
        \   <int name=\"patAssigneeAddress2Size\">1</int>\n    <int name=\"patAssigneeCitySize\">1</int>\n
        \   <int name=\"patAssigneeStateSize\">1</int>\n    <int name=\"patAssigneeCountryNameSize\">1</int>\n
        \   <int name=\"patAssigneePostcodeSize\">1</int>\n    <int name=\"inventionTitleSize\">1</int>\n
        \   <int name=\"inventionTitleIdSize\">1</int>\n    <int name=\"inventionTitleLangSize\">1</int>\n
        \   <int name=\"applNumSize\">1</int>\n    <int name=\"filingDateSize\">1</int>\n
        \   <int name=\"intlPublDateSize\">1</int>\n    <int name=\"intlRegNumSize\">1</int>\n
        \   <int name=\"inventorsSize\">1</int>\n    <int name=\"issueDateSize\">1</int>\n
        \   <int name=\"patNumSize\">1</int>\n    <int name=\"pctNumSize\">1</int>\n
        \   <int name=\"publDateSize\">1</int>\n    <int name=\"publNumSize\">1</int>\n
        \   <str name=\"inventionTitleFirst\">LEARNING ASSISTANCE DEVICE, METHOD OF
        OPERATING LEARNING ASSISTANCE DEVICE, LEARNING ASSISTANCE PROGRAM, LEARNING
        ASSISTANCE SYSTEM, AND TERMINAL DEVICE</str>\n    <str name=\"inventionTitleLangFirst\">en</str>\n
        \   <str name=\"applNumFirst\">16123456</str>\n    <date name=\"filingDateFirst\">2018-09-06T05:00:00Z</date>\n
        \   <date name=\"intlPublDateFirst\">0001-01-01T00:00:00Z</date>\n    <str
        name=\"intlRegNumFirst\">NULL</str>\n    <str name=\"inventorsFirst\">Shoji
        KANADA</str>\n    <date name=\"issueDateFirst\">2021-01-26T05:00:00Z</date>\n
        \   <str name=\"patNumFirst\">10902286</str>\n    <str name=\"pctNumFirst\">NULL</str>\n
        \   <date name=\"publDateFirst\">2019-03-28T05:00:00Z</date>\n    <str name=\"publNumFirst\">20190095759</str>\n
        \   <str name=\"patAssignorNameFirst\">KANADA, SHOJI</str>\n    <str name=\"patAssigneeNameFirst\">FUJIFILM
        CORPORATION</str>\n    <arr name=\"patAssigneePostcodeFacet\">\n      <str>106-8620</str>\n
        \   </arr>\n    <arr name=\"patAssignorNameFacet\">\n      <str>KANADA, SHOJI</str>\n
        \   </arr>\n    <str name=\"corrNameFacet\">BIRCH, STEWART, KOLASCH &amp;
        BIRCH, LLP</str>\n    <long name=\"_version_\">1689942979356131328</long>\n
        \   <arr name=\"patAssigneeNameFacet\">\n      <str>FUJIFILM CORPORATION</str>\n
        \   </arr>\n    <arr name=\"patAssigneeCityFacet\">\n      <str>TOKYO</str>\n
        \   </arr>\n    <arr name=\"patAssigneeCountryNameFacet\">\n      <str>JAPAN</str>\n
        \   </arr>\n    <arr name=\"patAssigneeStateFacet\">\n      <str>NULL</str>\n
        \   </arr>\n    <str name=\"corrNameString\">BIRCH, STEWART, KOLASCH &amp;
        BIRCH, LLP</str></doc>\n</result>\n<lst name=\"facet_counts\">\n  <lst name=\"facet_queries\">\n
        \   <int name=\"conveyanceText:&quot;ASSIGNMENT OF ASSIGNORS INTEREST&quot;\">1</int>\n
        \   <int name=\"conveyanceText:&quot;SECURITY INTEREST&quot;\">0</int>\n    <int
        name=\"conveyanceText:&quot;NUNC PRO TUNC ASSIGNMENT&quot;\">0</int>\n    <int
        name=\"conveyanceText:&quot;RELEASE BY SECURED PARTY&quot;\">0</int>\n    <int
        name=\"conveyanceText:&quot;MERGER&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;CHANGE
        OF NAME&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;MERGER AND CHANGE
        OF NAME&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;LICENSE&quot;\">0</int>\n
        \   <int name=\"conveyanceText:&quot;LIEN&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;MORTGAGE&quot;\">0</int>\n
        \   <int name=\"conveyanceText:&quot;OPTION&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;DECREE
        OF DISTRIBUTION&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;LETTERS
        OF TESTAMENTARY&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;LETTERS
        OF ADMINISTRATION&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;COURT
        APPOINTMENT&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;CONDITIONAL
        ASSIGNMENT&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;COURT ORDER&quot;\">0</int>\n
        \ </lst>\n  <lst name=\"facet_fields\">\n    <lst name=\"patAssigneeCityFacet\">\n
        \     <int name=\"TOKYO\">1</int>\n    </lst>\n    <lst name=\"patAssigneeStateFacet\">\n
        \     <int name=\"NULL\">1</int>\n    </lst>\n    <lst name=\"patAssigneePostcodeFacet\">\n
        \     <int name=\"106-8620\">1</int>\n    </lst>\n    <lst name=\"patAssigneeCountryNameFacet\">\n
        \     <int name=\"JAPAN\">1</int>\n    </lst>\n    <lst name=\"patAssigneeNameFacet\">\n
        \     <int name=\"FUJIFILM CORPORATION\">1</int>\n    </lst>\n    <lst name=\"corrNameFacet\">\n
        \     <int name=\"BIRCH, STEWART, KOLASCH &amp; BIRCH, LLP\">1</int>\n    </lst>\n
        \   <lst name=\"patAssignorNameFacet\">\n      <int name=\"KANADA, SHOJI\">1</int>\n
        \   </lst>\n    <lst name=\"applNum\">\n      <int name=\"16123456\">1</int>\n
        \   </lst>\n    <lst name=\"patNum\">\n      <int name=\"10902286\">1</int>\n
        \   </lst>\n    <lst name=\"publNum\">\n      <int name=\"20190095759\">1</int>\n
        \   </lst>\n    <lst name=\"intlRegNum\">\n      <int name=\"NULL\">1</int>\n
        \   </lst>\n  </lst>\n  <lst name=\"facet_ranges\">\n    <lst name=\"patAssignorEarliestExDate\">\n
        \     <lst name=\"counts\">\n        <int name=\"2018-01-01T00:00:00Z\">1</int>\n
        \     </lst>\n      <str name=\"gap\">+1YEAR</str>\n      <int name=\"before\">0</int>\n
        \     <date name=\"start\">1898-01-01T00:00:00Z</date>\n      <date name=\"end\">2024-01-01T00:00:00Z</date>\n
        \   </lst>\n  </lst>\n  <lst name=\"facet_intervals\"/>\n  <lst name=\"facet_heatmaps\"/>\n</lst>\n<lst
        name=\"highlighting\">\n  <lst name=\"46816-108\">\n    <arr name=\"frameNo\"/>\n
        \   <arr name=\"patAssigneeCity\"/>\n    <arr name=\"pctNum\"/>\n    <arr
        name=\"patAssigneePostcode\"/>\n    <arr name=\"filingDate\"/>\n    <arr name=\"conveyanceText\"/>\n
        \   <arr name=\"patNum\"/>\n    <arr name=\"patAssignorName\"/>\n    <arr
        name=\"corrAddress1\"/>\n    <arr name=\"corrAddress2\"/>\n    <arr name=\"corrAddress3\"/>\n
        \   <arr name=\"corrName\"/>\n    <arr name=\"applNum\"/>\n    <arr name=\"inventionTitle\"/>\n
        \   <arr name=\"publNum\"/>\n    <arr name=\"patAssigneeAddress\"/>\n    <arr
        name=\"intlRegNum\"/>\n    <arr name=\"publDate\"/>\n    <arr name=\"patAssigneeCountryName\"/>\n
        \   <arr name=\"intlPublDate\"/>\n    <arr name=\"id\"/>\n    <arr name=\"patAssigneeName\"/>\n
        \   <arr name=\"issueDate\"/>\n    <arr name=\"reelNo\"/>\n  </lst>\n</lst>\n</response>\n"
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/xml
      Date:
      - Fri, 20 Jan 2023 17:55:14 GMT
      Server:
      - Apache-Coyote/1.1
      Strict-Transport-Security:
      - max-age=31536000;includeSubDomains
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - DENY
      X-XSS-Protection:
      - 1; mode=block
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      Accept:
      - application/xml
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://assignment-api.uspto.gov/patent/lookup?filter=ApplicationNumber&query=16123456&rows=20&start=0&sort=&facet=False
  response:
    body:
      string: "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n\n<lst name=\"responseHeader\">\n
        \ <bool name=\"zkConnected\">true</bool>\n  <int name=\"status\">0</int>\n
        \ <int name=\"QTime\">47</int>\n  <lst name=\"params\">\n    <str name=\"q\">*:*</str>\n
        \   <str name=\"qt\">/apisearch</str>\n    <str name=\"start\">0</str>\n    <str
        name=\"sort\">patAssignorEarliestExDate desc</str>\n    <str name=\"fq\">applNum:16123456</str>\n
        \   <str name=\"rows\">20</str>\n    <str name=\"wt\">javabin</str>\n    <str
        name=\"version\">2</str>\n  </lst>\n</lst>\n<result name=\"response\" numFound=\"1\"
        start=\"0\">\n  <doc>\n    <str name=\"id\">46816-108</str>\n    <str name=\"displayId\">046816-0108</str>\n
        \   <str name=\"reelNo\">46816</str>\n    <str name=\"frameNo\">108</str>\n
        \   <date name=\"lastUpdateDate\">2021-01-26T07:00:52Z</date>\n    <str name=\"purgeIndicator\">N</str>\n
        \   <date name=\"recordedDate\">2018-09-07T05:00:00Z</date>\n    <str name=\"pageCount\">3</str>\n
        \   <str name=\"conveyanceText\">ASSIGNMENT OF ASSIGNORS INTEREST (SEE DOCUMENT
        FOR DETAILS).</str>\n    <str name=\"assignmentRecordHasImages\">Y</str>\n
        \   <str name=\"attorneyDockNum\">1982-1042PUS1</str>\n    <str name=\"corrName\">BIRCH,
        STEWART, KOLASCH &amp; BIRCH, LLP</str>\n    <str name=\"corrAddress1\">8110
        GATEHOUSE ROAD, SUITE 100E</str>\n    <str name=\"corrAddress2\">FALLS CHURCH,
        VA 22042</str>\n    <date name=\"patAssignorEarliestExDate\">2018-07-11T05:00:00Z</date>\n
        \   <arr name=\"patAssignorName\">\n      <str>KANADA, SHOJI</str>\n    </arr>\n
        \   <arr name=\"patAssignorExDate\">\n      <date>2018-07-11T05:00:00Z</date>\n
        \   </arr>\n    <arr name=\"patAssignorDateAck\">\n      <str>0001-01-01T00:00:00Z</str>\n
        \   </arr>\n    <arr name=\"patAssigneeName\">\n      <str>FUJIFILM CORPORATION</str>\n
        \   </arr>\n    <arr name=\"patAssigneeAddress1\">\n      <str>26-30, NISHIAZABU
        2-CHOME, MINATO-KU</str>\n    </arr>\n    <arr name=\"patAssigneeAddress2\">\n
        \     <str>NULL</str>\n    </arr>\n    <arr name=\"patAssigneeCity\">\n      <str>TOKYO</str>\n
        \   </arr>\n    <arr name=\"patAssigneeState\">\n      <str>NULL</str>\n    </arr>\n
        \   <arr name=\"patAssigneeCountryName\">\n      <str>JAPAN</str>\n    </arr>\n
        \   <arr name=\"patAssigneePostcode\">\n      <str>106-8620</str>\n    </arr>\n
        \   <arr name=\"inventionTitle\">\n      <str>LEARNING ASSISTANCE DEVICE,
        METHOD OF OPERATING LEARNING ASSISTANCE DEVICE, LEARNING ASSISTANCE PROGRAM,
        LEARNING ASSISTANCE SYSTEM, AND TERMINAL DEVICE</str>\n    </arr>\n    <arr
        name=\"inventionTitleLang\">\n      <str>en</str>\n    </arr>\n    <arr name=\"applNum\">\n
        \     <str>16123456</str>\n    </arr>\n    <arr name=\"filingDate\">\n      <date>2018-09-06T05:00:00Z</date>\n
        \   </arr>\n    <arr name=\"intlPublDate\">\n      <date>0001-01-01T00:00:00Z</date>\n
        \   </arr>\n    <arr name=\"intlRegNum\">\n      <str>NULL</str>\n    </arr>\n
        \   <arr name=\"inventors\">\n      <str>Shoji KANADA</str>\n    </arr>\n
        \   <arr name=\"issueDate\">\n      <date>2021-01-26T05:00:00Z</date>\n    </arr>\n
        \   <arr name=\"patNum\">\n      <str>10902286</str>\n    </arr>\n    <arr
        name=\"pctNum\">\n      <str>NULL</str>\n    </arr>\n    <arr name=\"publDate\">\n
        \     <date>2019-03-28T05:00:00Z</date>\n    </arr>\n    <arr name=\"publNum\">\n
        \     <str>20190095759</str>\n    </arr>\n    <int name=\"patAssignorNameSize\">1</int>\n
        \   <int name=\"patAssignorNameTypeSize\">1</int>\n    <int name=\"patAssignorExDateSize\">1</int>\n
        \   <int name=\"patAssignorDateAckSize\">1</int>\n    <int name=\"patAssigneeNameSize\">2</int>\n
        \   <int name=\"patAssigneeNameTypeSize\">0</int>\n    <int name=\"patAssigneeAddress1Size\">1</int>\n
        \   <int name=\"patAssigneeAddress2Size\">1</int>\n    <int name=\"patAssigneeCitySize\">1</int>\n
        \   <int name=\"patAssigneeStateSize\">1</int>\n    <int name=\"patAssigneeCountryNameSize\">1</int>\n
        \   <int name=\"patAssigneePostcodeSize\">1</int>\n    <int name=\"inventionTitleSize\">1</int>\n
        \   <int name=\"inventionTitleIdSize\">1</int>\n    <int name=\"inventionTitleLangSize\">1</int>\n
        \   <int name=\"applNumSize\">1</int>\n    <int name=\"filingDateSize\">1</int>\n
        \   <int name=\"intlPublDateSize\">1</int>\n    <int name=\"intlRegNumSize\">1</int>\n
        \   <int name=\"inventorsSize\">1</int>\n    <int name=\"issueDateSize\">1</int>\n
        \   <int name=\"patNumSize\">1</int>\n    <int name=\"pctNumSize\">1</int>\n
        \   <int name=\"publDateSize\">1</int>\n    <int name=\"publNumSize\">1</int>\n
        \   <str name=\"inventionTitleFirst\">LEARNING ASSISTANCE DEVICE, METHOD OF
        OPERATING LEARNING ASSISTANCE DEVICE, LEARNING ASSISTANCE PROGRAM, LEARNING
        ASSISTANCE SYSTEM, AND TERMINAL DEVICE</str>\n    <str name=\"inventionTitleLangFirst\">en</str>\n
        \   <str name=\"applNumFirst\">16123456</str>\n    <date name=\"filingDateFirst\">2018-09-06T05:00:00Z</date>\n
        \   <date name=\"intlPublDateFirst\">0001-01-01T00:00:00Z</date>\n    <str
        name=\"intlRegNumFirst\">NULL</str>\n    <str name=\"inventorsFirst\">Shoji
        KANADA</str>\n    <date name=\"issueDateFirst\">2021-01-26T05:00:00Z</date>\n
        \   <str name=\"patNumFirst\">10902286</str>\n    <str name=\"pctNumFirst\">NULL</str>\n
        \   <date name=\"publDateFirst\">2019-03-28T05:00:00Z</date>\n    <str name=\"publNumFirst\">20190095759</str>\n
        \   <str name=\"patAssignorNameFirst\">KANADA, SHOJI</str>\n    <str name=\"patAssigneeNameFirst\">FUJIFILM
        CORPORATION</str>\n    <arr name=\"patAssigneePostcodeFacet\">\n      <str>106-8620</str>\n
        \   </arr>\n    <arr name=\"patAssignorNameFacet\">\n      <str>KANADA, SHOJI</str>\n
        \   </arr>\n    <str name=\"corrNameFacet\">BIRCH, STEWART, KOLASCH &amp;
        BIRCH, LLP</str>\n    <long name=\"_version_\">1689942979356131328</long>\n
        \   <arr name=\"patAssigneeNameFacet\">\n      <str>FUJIFILM CORPORATION</str>\n
        \   </arr>\n    <arr name=\"patAssigneeCityFacet\">\n      <str>TOKYO</str>\n
        \   </arr>\n    <arr name=\"patAssigneeCountryNameFacet\">\n      <str>JAPAN</str>\n
        \   </arr>\n    <arr name=\"patAssigneeStateFacet\">\n      <str>NULL</str>\n
        \   </arr>\n    <str name=\"corrNameString\">BIRCH, STEWART, KOLASCH &amp;
        BIRCH, LLP</str></doc>\n</result>\n<lst name=\"facet_counts\">\n  <lst name=\"facet_queries\">\n
        \   <int name=\"conveyanceText:&quot;ASSIGNMENT OF ASSIGNORS INTEREST&quot;\">1</int>\n
        \   <int name=\"conveyanceText:&quot;SECURITY INTEREST&quot;\">0</int>\n    <int
        name=\"conveyanceText:&quot;NUNC PRO TUNC ASSIGNMENT&quot;\">0</int>\n    <int
        name=\"conveyanceText:&quot;RELEASE BY SECURED PARTY&quot;\">0</int>\n    <int
        name=\"conveyanceText:&quot;MERGER&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;CHANGE
        OF NAME&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;MERGER AND CHANGE
        OF NAME&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;LICENSE&quot;\">0</int>\n
        \   <int name=\"conveyanceText:&quot;LIEN&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;MORTGAGE&quot;\">0</int>\n
        \   <int name=\"conveyanceText:&quot;OPTION&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;DECREE
        OF DISTRIBUTION&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;LETTERS
        OF TESTAMENTARY&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;LETTERS
        OF ADMINISTRATION&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;COURT
        APPOINTMENT&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;CONDITIONAL
        ASSIGNMENT&quot;\">0</int>\n    <int name=\"conveyanceText:&quot;COURT ORDER&quot;\">0</int>\n
        \ </lst>\n  <lst name=\"facet_fields\">\n    <lst name=\"patAssigneeCityFacet\">\n
        \     <int name=\"TOKYO\">1</int>\n    </lst>\n    <lst name=\"patAssigneeStateFacet\">\n
        \     <int name=\"NULL\">1</int>\n    </lst>\n    <lst name=\"patAssigneePostcodeFacet\">\n
        \     <int name=\"106-8620\">1</int>\n    </lst>\n    <lst name=\"patAssigneeCountryNameFacet\">\n
        \     <int name=\"JAPAN\">1</int>\n    </lst>\n    <lst name=\"patAssigneeNameFacet\">\n
        \     <int name=\"FUJIFILM CORPORATION\">1</int>\n    </lst>\n    <lst name=\"corrNameFacet\">\n
        \     <int name=\"BIRCH, STEWART, KOLASCH &amp; BIRCH, LLP\">1</int>\n    </lst>\n
        \   <lst name=\"patAssignorNameFacet\">\n      <int name=\"KANADA, SHOJI\">1</int>\n
        \   </lst>\n    <lst name=\"applNum\">\n      <int name=\"16123456\">1</int>\n
        \   </lst>\n    <lst name=\"patNum\">\n      <int name=\"10902286\">1</int>\n
        \   </lst>\n    <lst name=\"publNum\">\n      <int name=\"20190095759\">1</int>\n
        \   </lst>\n    <lst name=\"intlRegNum\">\n      <int name=\"NULL\">1</int>\n
        \   </lst>\n  </lst>\n  <lst name=\"facet_ranges\">\n    <lst name=\"patAssignorEarliestExDate\">\n
        \     <lst name=\"counts\">\n        <int name=\"2018-01-01T00:00:00Z\">1</int>\n
        \     </lst>\n      <str name=\"gap\">+1YEAR</str>\n      <int name=\"before\">0</int>\n
        \     <date name=\"start\">1898-01-01T00:00:00Z</date>\n      <date name=\"end\">2024-01-01T00:00:00Z</date>\n
        \   </lst>\n  </lst>\n  <lst name=\"facet_intervals\"/>\n  <lst name=\"facet_heatmaps\"/>\n</lst>\n<lst
        name=\"highlighting\">\n  <lst name=\"46816-108\">\n    <arr name=\"frameNo\"/>\n
        \   <arr name=\"patAssigneeCity\"/>\n    <arr name=\"pctNum\"/>\n    <arr
        name=\"patAssigneePostcode\"/>\n    <arr name=\"filingDate\"/>\n    <arr name=\"conveyanceText\"/>\n
        \   <arr name=\"patNum\"/>\n    <arr name=\"patAssignorName\"/>\n    <arr
        name=\"corrAddress1\"/>\n    <arr name=\"corrAddress2\"/>\n    <arr name=\"corrAddress3\"/>\n
        \   <arr name=\"corrName\"/>\n    <arr name=\"applNum\"/>\n    <arr name=\"inventionTitle\"/>\n
        \   <arr name=\"publNum\"/>\n    <arr name=\"patAssigneeAddress\"/>\n    <arr
        name=\"intlRegNum\"/>\n    <arr name=\"publDate\"/>\n    <arr name=\"patAssigneeCountryName\"/>\n
        \   <arr name=\"intlPublDate\"/>\n    <arr name=\"id\"/>\n    <arr name=\"patAssigneeName\"/>\n
        \   <arr name=\"issueDate\"/>\n    <arr name=\"reelNo\"/>\n  </lst>\n</lst>\n</response>\n"
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/xml
      Date:
      - Fri, 20 Jan 2023 17:56:41 GMT
      Server:
      - Apache-Coyote/1.1
      Strict-Transport-Security:
      - max-age=31536000;includeSubDomains
      X-Content-Type-Options:
      - nosniff
      X-Frame-Options:
      - DENY
      X-XSS-Protection:
      - 1; mode=block
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 17:56:41 GMT
      apigw-requestid:
      - fDZaFgJWIAMEMdg=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: '{"country":"EP","internal":"false","corrAppNum":"1000000","id":"1000000","type":"publication","list":[{"appNum":"1010536","appDate":910828800000,"pubList":[{"pubCountry":"NL","pubNum":"1010536","pubDate":958348800000,"kindCode":"C2","pubDateStr":"05/15/2000"}],"countryCode":"NL","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/12/1998","docListMsg":null,"docNum":{"country":"NL","docNumber":"1010536","format":null,"date":"11/12/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"20685698","appDate":913075200000,"pubList":[{"pubCountry":"US","pubNum":"6093011","pubDate":964483200000,"kindCode":"A","pubDateStr":"07/25/2000"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"12/08/1998","docListMsg":"org.springframework.web.client.HttpClientErrorException$NotFound:
        404 : \"{\"inputDocNum\":null,\"outputDocNum\":null,\"output-number-type\":null,\"output-number-format\":null,\"numberConversionError\":{\"appId\":null,\"message\":\"Unable
        to convert DocDb Number: US.20685698.A\"}}\"","docNum":{"country":"US","docNumber":"20685698","format":null,"date":"12/08/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"AT","pubNum":"232441","pubDate":1045267200000,"kindCode":"T","pubDateStr":"02/15/2003"}],"countryCode":"AT","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"AT","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"69905327","appDate":942019200000,"pubList":[{"pubCountry":"DE","pubNum":"69905327","pubDate":1048118400000,"kindCode":"D1","pubDateStr":"03/20/2003"}],"countryCode":"DE","docList":null,"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"DE","docNumber":"69905327","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"EP","pubNum":"1000000","pubDate":958521600000,"kindCode":"A1","pubDateStr":"05/17/2000"},{"pubCountry":"EP","pubNum":"1000000","pubDate":1045008000000,"kindCode":"B1","pubDateStr":"02/12/2003"}],"countryCode":"EP","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"EP","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '22140'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:56:42 GMT
      apigw-requestid:
      - fDZaGizOIAMEMqg=
    status:
      code: 200
      message: OK
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 17:56:43 GMT
      apigw-requestid:
      - fDZaQgWeIAMEMrQ=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: '{"country":"EP","internal":"false","corrAppNum":"1000000","id":"1000000","type":"publication","list":[{"appNum":"1010536","appDate":910828800000,"pubList":[{"pubCountry":"NL","pubNum":"1010536","pubDate":958348800000,"kindCode":"C2","pubDateStr":"05/15/2000"}],"countryCode":"NL","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/12/1998","docListMsg":null,"docNum":{"country":"NL","docNumber":"1010536","format":null,"date":"11/12/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"20685698","appDate":913075200000,"pubList":[{"pubCountry":"US","pubNum":"6093011","pubDate":964483200000,"kindCode":"A","pubDateStr":"07/25/2000"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"12/08/1998","docListMsg":"org.springframework.web.client.HttpClientErrorException$NotFound:
        404 : \"{\"inputDocNum\":null,\"outputDocNum\":null,\"output-number-type\":null,\"output-number-format\":null,\"numberConversionError\":{\"appId\":null,\"message\":\"Unable
        to convert DocDb Number: US.20685698.A\"}}\"","docNum":{"country":"US","docNumber":"20685698","format":null,"date":"12/08/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"AT","pubNum":"232441","pubDate":1045267200000,"kindCode":"T","pubDateStr":"02/15/2003"}],"countryCode":"AT","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"AT","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"69905327","appDate":942019200000,"pubList":[{"pubCountry":"DE","pubNum":"69905327","pubDate":1048118400000,"kindCode":"D1","pubDateStr":"03/20/2003"}],"countryCode":"DE","docList":null,"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"DE","docNumber":"69905327","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"EP","pubNum":"1000000","pubDate":958521600000,"kindCode":"A1","pubDateStr":"05/17/2000"},{"pubCountry":"EP","pubNum":"1000000","pubDate":1045008000000,"kindCode":"B1","pubDateStr":"02/12/2003"}],"countryCode":"EP","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"EP","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '22140'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:56:43 GMT
      apigw-requestid:
      - fDZaRgfqIAMEM_A=
    status:
      code: 200
      message: OK
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 2168779, "hl_snippets": "2", "op": "OR", "q": "\"1000000\".PN.",
      "queryName": "\"1000000\".PN.", "highlights": "1", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "US-PGPUB", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "\"1000000\".PN."}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '715'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":0,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":2168779,"numResults":0,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"US-PGPUB","countryCodes":[]}],"q":"\"1000000\".PN.","queryName":"\"1000000\".PN.","userEnteredQuery":"\"1000000\".PN.","viewName":"tile","op":"OR","highlights":"1","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"\"1000000\".PN.","error":null,"terms":["\"1000000\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":21,"highlightingTime":0,"cursorMarker":"*","totalResults":0,"numberOfFamilies":0,"error":null,"patents":[],"qtime":17}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:57:04 GMT
      Server-Timing:
      - intid;desc=d8d77886292a2795
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 17:57:04 GMT
      apigw-requestid:
      - fDZdjgrYoAMEMrg=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: '{"country":"EP","internal":"false","corrAppNum":"1000000","id":"1000000","type":"publication","list":[{"appNum":"1010536","appDate":910828800000,"pubList":[{"pubCountry":"NL","pubNum":"1010536","pubDate":958348800000,"kindCode":"C2","pubDateStr":"05/15/2000"}],"countryCode":"NL","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/12/1998","docListMsg":null,"docNum":{"country":"NL","docNumber":"1010536","format":null,"date":"11/12/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"20685698","appDate":913075200000,"pubList":[{"pubCountry":"US","pubNum":"6093011","pubDate":964483200000,"kindCode":"A","pubDateStr":"07/25/2000"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"12/08/1998","docListMsg":"org.springframework.web.client.HttpClientErrorException$NotFound:
        404 : \"{\"inputDocNum\":null,\"outputDocNum\":null,\"output-number-type\":null,\"output-number-format\":null,\"numberConversionError\":{\"appId\":null,\"message\":\"Unable
        to convert DocDb Number: US.20685698.A\"}}\"","docNum":{"country":"US","docNumber":"20685698","format":null,"date":"12/08/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"AT","pubNum":"232441","pubDate":1045267200000,"kindCode":"T","pubDateStr":"02/15/2003"}],"countryCode":"AT","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"AT","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"69905327","appDate":942019200000,"pubList":[{"pubCountry":"DE","pubNum":"69905327","pubDate":1048118400000,"kindCode":"D1","pubDateStr":"03/20/2003"}],"countryCode":"DE","docList":null,"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"DE","docNumber":"69905327","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"EP","pubNum":"1000000","pubDate":958521600000,"kindCode":"A1","pubDateStr":"05/17/2000"},{"pubCountry":"EP","pubNum":"1000000","pubDate":1045008000000,"kindCode":"B1","pubDateStr":"02/12/2003"}],"countryCode":"EP","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"EP","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '22140'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:57:04 GMT
      apigw-requestid:
      - fDZdjg5aIAMEMeQ=
    status:
      code: 200
      message: OK
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 2168779, "hl_snippets": "2", "op": "OR", "q": "\"1000000\".PN.",
      "queryName": "\"1000000\".PN.", "highlights": "1", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "USPAT", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "\"1000000\".PN."}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '712'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":2168779,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"USPAT","countryCodes":[]}],"q":"\"1000000\".PN.","queryName":"\"1000000\".PN.","userEnteredQuery":"\"1000000\".PN.","viewName":"tile","op":"OR","highlights":"1","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"\"1000000\".PN.","error":null,"terms":["\"1000000\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":30,"highlightingTime":0,"cursorMarker":"AoIH///+UuS8LAA2MzA2ODMyNyFBVS1VUy0wMTAwMDAwMA==","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-1000000-A","publicationReferenceDocumentNumber":"1000000","compositeId":"3068327!AU-US-01000000","publicationReferenceDocumentNumber1":"01000000","datePublishedKwicHits":null,"datePublished":"1911-08-08T00:00:00Z","inventionTitle":"TEXT
        NOT AVAILABLE","type":"USPAT","mainClassificationCode":"152/315","applicantName":null,"assigneeName":null,"uspcFullClassificationFlattened":null,"ipcCodeFlattened":"B60C7/00","cpcInventiveFlattened":"G06F1/00;B60C7/10;B60C7/04","cpcAdditionalFlattened":"Y10T152/10423;H01L2224/05124;B60C2011/0313;G07D1/00","applicationFilingDate":null,"applicationFilingDateKwicHits":null,"relatedApplFilingDate":null,"primaryExaminer":null,"assistantExaminer":null,"applicationNumber":null,"frontPageStart":1,"frontPageEnd":2,"drawingsStart":1,"drawingsEnd":1,"specificationStart":2,"specificationEnd":3,"claimsStart":3,"claimsEnd":3,"abstractStart":2,"abstractEnd":2,"bibStart":2,"bibEnd":2,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":3,"pageCountDisplay":"3","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/01/000/000","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":null,"familyIdentifierCur":3068327,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":null,"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        1000000 A","derwentAccessionNumber":null,"documentSize":4351,"score":13.637712,"governmentInterest":null,"kindCode":["A"],"urpn":null,"urpnCode":null,"descriptionStart":2,"descriptionEnd":3,"publicationReferenceDocumentNumberOne":"01000000"}],"qtime":25}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:57:21 GMT
      Server-Timing:
      - intid;desc=1b22721a510308f1
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://ppubs.uspto.gov/dirsearch-public/patents/US-1000000-A/highlight?queryId=1&source=USPAT&includeSections=True
  response:
    body:
      string: '{"guid":"US-1000000-A","publicationReferenceDocumentNumber":"1000000","compositeId":"3068327!AU-US-01000000","publicationReferenceDocumentNumber1":"01000000","datePublishedKwicHits":null,"datePublished":"1911-08-08T00:00:00Z","inventionTitle":"TEXT
        NOT AVAILABLE","type":"USPAT","mainClassificationCode":"152/315","applicantName":null,"assigneeName":null,"uspcFullClassificationFlattened":null,"ipcCodeFlattened":"B60C7/00","cpcInventiveFlattened":"G06F1/00;B60C7/10;B60C7/04","cpcAdditionalFlattened":"Y10T152/10423;H01L2224/05124;B60C2011/0313;G07D1/00","applicationFilingDate":null,"applicationFilingDateKwicHits":null,"relatedApplFilingDate":null,"primaryExaminer":null,"assistantExaminer":null,"applicationNumber":null,"frontPageStart":1,"frontPageEnd":2,"drawingsStart":1,"drawingsEnd":1,"specificationStart":2,"specificationEnd":3,"claimsStart":3,"claimsEnd":3,"abstractStart":2,"abstractEnd":2,"bibStart":2,"bibEnd":2,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":3,"pageCountDisplay":"3","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/01/000/000","imageFileName":"00000001.tif","cpcCodes":null,"queryId":1,"tags":null,"inventorsShort":null,"familyIdentifierCur":3068327,"familyIdentifierCurStr":"3068327","languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":null,"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"<span
        term=\"us1000000a\" class=\"highlight18\">US 1000000 A</span>","derwentAccessionNumber":null,"documentSize":4351,"score":0.0,"governmentInterest":null,"kindCode":["A"],"urpn":null,"urpnCode":null,"abstractedPatentNumber":null,"assigneeCity":null,"assigneePostalCode":null,"assigneeState":null,"assigneeTypeCode":null,"curIntlPatentClassificationPrimary":null,"curIntlPatentClassificationPrimaryDateKwicHits":null,"designatedStates":null,"examinerGroup":null,"issuedUsCrossRefClassification":null,"jpoFtermCurrent":null,"languageOfSpecification":null,"chosenDrawingsReference":null,"derwentClass":null,"inventionTitleHighlights":null,"cpcOrigInventiveClassificationHighlights":null,"cpcInventiveDateKwicHits":null,"cpcOrigAdditionalClassification":null,"cpcAdditionalDateKwicHits":null,"curIntlPatentClssficationSecHighlights":null,"fieldOfSearchClassSubclassHighlights":null,"cpcCombinationSetsCurHighlights":null,"applicantCountry":null,"applicantCity":null,"applicantState":null,"applicantZipCode":null,"applicantAuthorityType":null,"applicantDescriptiveText":null,"applicationSerialNumber":null,"inventorCity":null,"inventorState":null,"inventorPostalCode":null,"standardTitleTermsHighlights":null,"primaryExaminerHighlights":null,"continuityData":null,"inventors":null,"uspcFullClassification":null,"uspcCodeFmtFlattened":null,"ipcCode":null,"applicationNumberHighlights":null,"dateProduced":"2012-09-18T00:00:00Z","auxFamilyMembersGroupTempPlaceHolder":null,"priorityCountryCode":null,"cpcCurAdditionalClassification":null,"internationalClassificationMain":null,"internationalClassificationSecondary":null,"internationalClassificationInformational":null,"europeanClassification":null,"europeanClassificationMain":null,"europeanClassificationSecondary":null,"lanuageIndicator":null,"intlPubClassificationPrimary":null,"intlPubClassificationPrimaryDateKwicHits":null,"intlPubClassificationSecondary":null,"intlPubClassificationSecondaryDateKwicHits":null,"publicationDate":null,"derwentWeekInt":0,"derwentWeek":null,"currentUsOriginalClassification":"152/315","currentUsCrossReferenceClassification":null,"locarnoClassification":null,"equivalentAbstractText":null,"hagueIntlRegistrationNumber":null,"hagueIntlFilingDate":null,"hagueIntlFilingDateKwicHits":null,"hagueIntlRegistrationDate":null,"hagueIntlRegistrationDateKwicHits":null,"hagueIntlRegistrationPubDate":null,"hagueIntlRegistrationPubDateKwicHits":null,"curIntlPatentClassificationNoninvention":["B60C7/00
        20060101","B60C7/10 20060101"],"curIntlPatentClassificationNoninventionDateKwicHits":null,"curIntlPatentClassificationSecondary":null,"curIntlPatentClassificationSecondaryDateKwicHits":null,"abstractHtml":null,"descriptionHtml":null,"claimsHtml":null,"briefHtml":null,"backgroundTextHtml":null,"subHeadingM0Html":null,"subHeadingM1Html":null,"subHeadingM2Html":null,"subHeadingM3Html":null,"subHeadingM4Html":null,"subHeadingM5Html":null,"subHeadingM6Html":null,"usClassIssued":null,"issuedUsDigestRefClassifi":null,"datePublYear":"1911","applicationYear":null,"pfDerwentWeekYear":null,"pfApplicationYear":null,"pfPublYear":null,"reissueApplNumber":null,"abstractHeader":null,"abstractedPublicationDerwent":null,"affidavit130BFlag":null,"affidavit130BText":null,"applicantGroup":null,"applicantHeader":null,"applicationFilingDateInt":0,"applicationFilingDateIntKwicHits":null,"applicationRefFilingType":null,"applicationReferenceGroup":null,"applicationSeriesAndNumber":null,"applicationSeriesCode":null,"assignee1":null,"assigneeDescriptiveText":null,"patentAssigneeTerms":null,"associateAttorneyName":null,"attorneyName":null,"biologicalDepositInformation":null,"applicationType":null,"unlinkedDerwentRegistryNumber":null,"unlinkedRingIndexNumbersRarerFragments":null,"claimStatement":null,"claimsTextAmended":null,"continuedProsecutionAppl":null,"cpcAdditionalLong":null,"cpcCisClassificationOrig":null,"cpcCombinationClassificationOrig":null,"cpcInventive":["G06F1/00
        20130101","B60C7/10 20130101","B60C7/04 20130101"],"cpcInventiveCurrentDateKwicHits":null,"cpcAdditional":["Y10T152/10423
        20150115","H01L2224/05124 20130101","B60C2011/0313 20130101","G07D1/00 20130101"],"cpcAdditionalCurrentDateKwicHits":null,"cpcOrigClassificationGroup":null,"curIntlPatentClassificationGroup":null,"curUsClassificationUsPrimaryClass":null,"curUsClassificationUsSecondaryClass":null,"customerNumber":null,"depositAccessionNumber":null,"depositDescription":null,"derwentClassAlpha":null,"designatedstatesRouteGroup":null,"docAccessionNumber":null,"drawingDescription":null,"editionField":null,"exchangeWeek":null,"exemplaryClaimNumber":null,"familyIdentifierOrig":null,"fieldOfSearchCpcClassification":null,"fieldOfSearchCpcMainClass":null,"fieldOfSearchIpcMainClass":null,"fieldOfSearchIpcMainClassSubclass":null,"fieldOfSearchSubclasses":null,"foreignRefGroup":null,"foreignRefPubDate":null,"foreignRefPubDateKwicHits":null,"foreignRefCitationClassification":null,"foreignRefPatentNumber":null,"foreignRefCitationCpc":null,"foreignRefCountryCode":null,"iceXmlIndicator":null,"internationalClassificationHeader":null,"internationalClassificationInformationalGroup":null,"intlPubClassificationGroup":null,"intlPubClassificationNonInvention":null,"inventorCitizenship":null,"inventorCorrection":null,"inventorDeceased":null,"inventorStreetAddress":null,"inventorText":null,"jpoFiClassification":null,"legalRepresentativeCity":null,"legalRepresentativeCountry":"[]","legalRepresentativeName":null,"legalRepresentativePostcode":null,"legalRepresentativeState":null,"legalRepresentativeStreetAddress":null,"legalRepresentativeText":null,"messengerDocsFlag":null,"newRecordPatentDerwent":null,"numberOfClaims":null,"numberOfDrawingSheets":null,"numberOfFigures":null,"numberOfPagesInSpecification":null,"numberOfPagesOfSpecification":null,"objectContents":null,"objectDescription":null,"parentDocCountry":null,"parentGrantDocCountry":null,"patentBibliographicHeader":null,"pctOrRegionalPublishingSerial":null,"pfDerwentWeekNum":null,"principalAttorneyName":null,"priorityApplicationCountry":null,"priorityClaimsCountry":null,"priorityNumberDerived":null,"publicationIssueNumber":null,"refCitedPatentDocNumber":null,"refCitedPatentDocDate":null,"refCitedPatentDocKindCode":null,"referenceCitedCode":null,"referenceCitedGroup":null,"referenceCitedSearchPhase":null,"referenceCitedText":null,"registrationNumber":null,"reissueApplCountry":null,"reissueParentKind":null,"reissueParentNumber":null,"reissueParentPubCountry":null,"reissuePatentGroup":null,"reissuePatentParentStatus":null,"reissuedPatentApplCountry":null,"reissuedPatentApplKind":null,"reissuedPatentApplNumber":null,"relatedApplChildPatentCountry":null,"relatedApplChildPatentName":null,"relatedApplChildPatentNumber":null,"relatedApplCountryCode":null,"relatedApplParentGrantPatentKind":null,"relatedApplParentGrantPatentName":null,"relatedApplParentPatentKind":null,"relatedApplParentPatentName":null,"relatedApplParentPctDoc":null,"relatedApplParentStatusCode":null,"relatedApplPatentNumber":null,"relatedApplRelatedPub":null,"relatedApplTypeOfCorrection":null,"rule47Flag":null,"selectedDrawingCharacter":null,"selectedDrawingFigure":null,"statutoryInventionText":null,"termOfExtension":null,"termOfPatentGrant":null,"titleTermsData":null,"additionalIndexingTerm":null,"applicationYearSearch":null,"pfApplicationYearSearch":null,"assigneeCountry":null,"certOfCorrectionFlag":null,"citedPatentLiteratureAddressInformation":null,"citedPatentLiteratureClassificationIpc":null,"citedPatentLiteratureOrganizationName":null,"citedPatentLiteratureRefNumber":null,"crossReferenceNumber":null,"country":"US","cpiManualCodes":null,"cpiSecondaryAccessionNumber":null,"curIntlPatentAllClassificationLong":null,"currentUsOriginalClassificationLong":null,"datePublSearch":"1911-08-08T00:00:00.000+00:00","datePublYearSearch":"1911","epiManualCodes":null,"fieldOfSearchMainClassNational":null,"inventorCountry":null,"ipcAllMainClassification":["B60C"],"issuedUsClassificationFull":null,"issuedUsDigestRefClassification":null,"jpoFiCurrentAdditionalClassification":null,"jpoFiCurrentInventiveClassification":null,"legalFirmName":null,"locarnoMainClassification":null,"nonCpiSecondaryAccessionNumber":null,"objectId":null,"otherRefPub":null,"pageNumber":null,"patentAssigneeCode":null,"patentAssigneeNameTotal":null,"patentFamilyDate":null,"patentFamilyDocNumber":null,"patentFamilyKind":null,"patentFamilyKindCode":null,"patentFamilyLanguage":null,"patentFamilyName":null,"patentNumberOfLocalApplication":null,"pct102eDate":null,"pct371c124Date":null,"pct371c124DateKwicHits":null,"pctFilingDate":null,"pctFilingDateKwicHits":null,"pctFilingDocCountryCode":null,"pctFilingKind":null,"pctFilingNumber":null,"pctName":null,"pctOrRegionalPublishingCountry":null,"pctOrRegionalPublishingKind":null,"pctOrRegionalPublishingName":null,"pctOrRegionalPublishingText":null,"pctPubDate":null,"pctPubDateKwicHits":null,"pctPubDocIdentifier":null,"pctPubNumber":null,"pfApplicationDateSearch":null,"pfApplicationType":null,"pfDerwentWeekDate":null,"pfPublDateSearch":null,"pfPublDateSearchKwicHits":null,"pfPublYearSearch":null,"polymerIndexingCodes":null,"polymerMultipunchCodeRecordNumber":null,"polymerMultipunchCodes":null,"priorPublishedDocCountryCode":null,"priorPublishedDocDate":null,"priorPublishedDocDateKwicHits":null,"priorPublishedDocIdentifier":null,"priorPublishedDocKindCode":null,"priorPublishedDocNumber":null,"priorityApplYear":null,"priorityApplicationDate":null,"priorityClaimsDateSearch":null,"priorityClaimsDocNumber":null,"priorityPatentDid":null,"priorityPatentNumber":null,"ptabCertFlag":null,"pubRefCountryCode":null,"pubRefDocNumber":"1000000","pubRefDocNumber1":"01000000","publicationData":null,"recordPatentNumber":null,"reexaminationFlag":null,"refCitedOthers":null,"refCitedPatentDocCountryCode":null,"refCitedPatentDocName":null,"refCitedPatentRelevantPassage":null,"reissueParentIssueDate":null,"reissuedPatentApplFilingDate":null,"relatedAccessionNumbers":null,"relatedApplChildPatentDate":null,"relatedApplFilingDateKwicHits":null,"relatedApplNumber":null,"relatedApplPatentIssueDate":null,"relatedApplPatentIssueDateKwicHits":null,"relatedDocumentKindCode":null,"securityLegend":null,"sequenceCwu":null,"sequenceListNewRules":null,"sequenceListOldRules":null,"sequencesListText":null,"standardTitleTerms":null,"supplementalExaminationFlag":null,"usBotanicLatinName":null,"usBotanicVariety":null,"usRefClassification":null,"usRefCpcClassification":null,"usRefGroup":null,"usRefIssueDate":null,"usRefIssueDateKwicHits":null,"usRefPatenteeName":null,"volumeNumber":null,"correspondenceNameAddress":null,"correspondenceAddressCustomerNumber":null,"ibmtdbAccessionNumber":null,"inventorsName":null,"applicationKindCode":"A","inventorNameDerived":null,"intlPubClassificationClass":null,"issuedUsOrigClassification":null,"curCpcSubclassFull":["G07D","H01L","Y10T","B60C","G06F","B60C"],"cpcCurAdditionalClass":["Y10T","H01L","B60C","G07D"],"cpcCurInventiveClass":["G06F","B60C","B60C"],"cpcCurClassificationGroup":["Y
        Y10T Y10T152/10423 20150115 L A B C 20150115 US","H H01L H01L2224/05124 20130101
        L A B H 20191011 US","B B60C B60C2011/0313 20130101 L A B H 20191011 US","G
        G06F G06F1/00 20130101 L I B H 20210915 EP","B B60C B60C7/10 20130101 F I
        B H 20190822 US","B B60C B60C7/04 20130101 L I B H 20191011 US","G G07D G07D1/00
        20130101 L A B H 20191011 US"],"curCpcClassificationFull":["B60C2011/0313
        20130101","B60C7/04 20130101","B60C7/10 20130101","H01L2224/05124 20130101","G07D1/00
        20130101","Y10T152/10423 20150115","G06F1/00 20130101"],"cpcCombinationClassificationCur":null,"cpcCombinationTallyCur":null,"intlFurtherClassification":null,"currentUsPatentClass":["152"],"idWithoutSolrPartition":"AU-US-01000000","curIntlPatentClassifictionPrimaryDateKwicHits":null,"curIntlPatentClssifSecHlights":null,"internationalClassificationInfom":null,"cpcOrigInventvClssifHlghts":null,"descriptionEnd":3,"publicationReferenceDocumentNumberOne":"01000000","descriptionStart":2}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 17:57:21 GMT
      Server-Timing:
      - intid;desc=f72dd6bc1573f7b7
    status:
      code: 200
      message: ''
- request:
    body: '{"start": 0, "pageCount": 500, "sort": "date_publ desc", "docFamilyFiltering":
      "familyIdFiltering", "searchType": 1, "familyIdEnglishOnly": true, "familyIdFirstPreferred":
      "US-PGPUB", "familyIdSecondPreferred": "USPAT", "familyIdThirdPreferred": "FPRS",
      "showDocPerFamilyPref": "showEnglish", "queryId": 0, "tagDocSearch": false,
      "query": {"caseId": 2168779, "hl_snippets": "2", "op": "OR", "q": "\"1000000\".PN.",
      "queryName": "\"1000000\".PN.", "highlights": "1", "qt": "brs", "spellCheck":
      false, "viewName": "tile", "plurals": true, "britishEquivalents": true, "databaseFilters":
      [{"databaseName": "USPAT", "countryCodes": []}], "searchType": 1, "ignorePersist":
      true, "userEnteredQuery": "\"1000000\".PN."}}'
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '712'
      Content-Type:
      - application/json
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: POST
    uri: https://ppubs.uspto.gov/dirsearch-public/searches/searchWithBeFamily
  response:
    body:
      string: '{"numFound":1,"perPage":500,"page":0,"totalPages":0,"hlSnippets":0,"sort":null,"query":{"id":null,"caseId":2168779,"numResults":1,"ignorePersist":true,"fq":null,"databaseFilters":[{"databaseName":"USPAT","countryCodes":[]}],"q":"\"1000000\".PN.","queryName":"\"1000000\".PN.","userEnteredQuery":"\"1000000\".PN.","viewName":"tile","op":"OR","highlights":"1","plurals":true,"britishEquivalents":true,"searchType":1,"excludeResultsAfter":null,"dateCreated":null,"deleteIn":false,"expand":true,"expandSort":"group_sort_date
        asc, id desc ","expandRows":"100","expandTrackDocScores":true,"expandTrackMaxScore":true,"termGraph":null,"hl":false,"fl":null,"originalQuery":"\"1000000\".PN.","error":null,"terms":["\"1000000\""],"facets":[],"pNumber":null,"hl_fl":null},"duration":24,"highlightingTime":0,"cursorMarker":"AoIH///+UuS8LAA2MzA2ODMyNyFBVS1VUy0wMTAwMDAwMA==","totalResults":1,"numberOfFamilies":1,"error":null,"patents":[{"guid":"US-1000000-A","publicationReferenceDocumentNumber":"1000000","compositeId":"3068327!AU-US-01000000","publicationReferenceDocumentNumber1":"01000000","datePublishedKwicHits":null,"datePublished":"1911-08-08T00:00:00Z","inventionTitle":"TEXT
        NOT AVAILABLE","type":"USPAT","mainClassificationCode":"152/315","applicantName":null,"assigneeName":null,"uspcFullClassificationFlattened":null,"ipcCodeFlattened":"B60C7/00","cpcInventiveFlattened":"G06F1/00;B60C7/10;B60C7/04","cpcAdditionalFlattened":"Y10T152/10423;H01L2224/05124;B60C2011/0313;G07D1/00","applicationFilingDate":null,"applicationFilingDateKwicHits":null,"relatedApplFilingDate":null,"primaryExaminer":null,"assistantExaminer":null,"applicationNumber":null,"frontPageStart":1,"frontPageEnd":2,"drawingsStart":1,"drawingsEnd":1,"specificationStart":2,"specificationEnd":3,"claimsStart":3,"claimsEnd":3,"abstractStart":2,"abstractEnd":2,"bibStart":2,"bibEnd":2,"certCorrectionStart":0,"certCorrectionEnd":0,"certReexaminationStart":0,"certReexaminationEnd":0,"supplementalStart":0,"supplementalEnd":0,"ptabStart":0,"ptabEnd":0,"amendStart":0,"amendEnd":0,"searchReportStart":0,"searchReportEnd":0,"pageCount":3,"pageCountDisplay":"3","previouslyViewed":false,"unused":false,"imageLocation":"uspat/US/01/000/000","imageFileName":"00000001.tif","cpcCodes":null,"queryId":null,"tags":null,"inventorsShort":null,"familyIdentifierCur":3068327,"familyIdentifierCurStr":null,"languageIndicator":"EN","databaseName":"USPT","dwImageDoctypeList":null,"dwImageLocList":null,"dwPageCountList":null,"dwImageDocidList":null,"patentFamilyMembers":null,"patentFamilyCountry":null,"patentFamilySerialNumber":null,"documentIdWithDashesDw":null,"pfPublDate":null,"pfPublDateKwicHits":null,"priorityClaimsDate":null,"priorityClaimsDateKwicHits":null,"pfApplicationSerialNumber":null,"pfApplicationDescriptor":null,"pfLanguage":null,"pfApplicationDate":null,"pfApplicationDateKwicHits":null,"clippedUri":null,"source":null,"documentId":"US
        1000000 A","derwentAccessionNumber":null,"documentSize":4351,"score":13.637712,"governmentInterest":null,"kindCode":["A"],"urpn":null,"urpnCode":null,"descriptionEnd":3,"publicationReferenceDocumentNumberOne":"01000000","descriptionStart":2}],"qtime":21}'
    headers:
      Connection:
      - keep-alive
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 18:04:09 GMT
      Server-Timing:
      - intid;desc=22c00bb9698796ed
      Transfer-Encoding:
      - chunked
    status:
      code: 200
      message: ''
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: OPTIONS
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: ''
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '0'
      Date:
      - Fri, 20 Jan 2023 18:04:55 GMT
      apigw-requestid:
      - fDanKiWHoAMEPCA=
    status:
      code: 204
      message: No Content
- request:
    body: null
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Authorization:
      - REDACTED
      Connection:
      - keep-alive
      User-Agent:
      - Python Patent Clientbot/3.2.0 (parkerhancock@users.noreply.github.com)
    method: GET
    uri: https://gd-api2.uspto.gov/patent-family/svc/family/publication/EP/1000000
  response:
    body:
      string: '{"country":"EP","internal":"false","corrAppNum":"1000000","id":"1000000","type":"publication","list":[{"appNum":"1010536","appDate":910828800000,"pubList":[{"pubCountry":"NL","pubNum":"1010536","pubDate":958348800000,"kindCode":"C2","pubDateStr":"05/15/2000"}],"countryCode":"NL","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/12/1998","docListMsg":null,"docNum":{"country":"NL","docNumber":"1010536","format":null,"date":"11/12/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"20685698","appDate":913075200000,"pubList":[{"pubCountry":"US","pubNum":"6093011","pubDate":964483200000,"kindCode":"A","pubDateStr":"07/25/2000"}],"countryCode":"US","docList":null,"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"12/08/1998","docListMsg":"org.springframework.web.client.HttpClientErrorException$NotFound:
        404 : \"{\"inputDocNum\":null,\"outputDocNum\":null,\"output-number-type\":null,\"output-number-format\":null,\"numberConversionError\":{\"appId\":null,\"message\":\"Unable
        to convert DocDb Number: US.20685698.A\"}}\"","docNum":{"country":"US","docNumber":"20685698","format":null,"date":"12/08/1998","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"AT","pubNum":"232441","pubDate":1045267200000,"kindCode":"T","pubDateStr":"02/15/2003"}],"countryCode":"AT","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"AT","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"69905327","appDate":942019200000,"pubList":[{"pubCountry":"DE","pubNum":"69905327","pubDate":1048118400000,"kindCode":"D1","pubDateStr":"03/20/2003"}],"countryCode":"DE","docList":null,"kindCode":"T","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"DE","docNumber":"69905327","format":null,"date":"11/08/1999","kindCode":"T"},"title":null,"applicantNames":null,"ip5":false},{"appNum":"99203729","appDate":942019200000,"pubList":[{"pubCountry":"EP","pubNum":"1000000","pubDate":958521600000,"kindCode":"A1","pubDateStr":"05/17/2000"},{"pubCountry":"EP","pubNum":"1000000","pubDate":1045008000000,"kindCode":"B1","pubDateStr":"02/12/2003"}],"countryCode":"EP","docList":{"title":"Apparatus
        for manufacturing green bricks for the brick manufacturing industry","docs":[{"docCode":"1001","docDesc":"Request
        for grant of a European patent","docId":"EA2JRLI1DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"1002","docDesc":"Designation
        of inventor","docId":"EA2JRLJNDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SRCH","docDesc":"Letter
        concerning search matters","docId":"EA2JRLJ0DHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"11/08/1999","shareable":false},{"docCode":"SPEC","docDesc":"Specification
        filed by fax and/or in non-official language","docId":"EA2JRLKBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DESC","docDesc":"Description","docId":"EA2JRLKODHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EA2JRLKZDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EA2JRLLBDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"11/08/1999","shareable":false},{"docCode":"DRAW","docDesc":"Drawings","docId":"EA2JRLLRDHXBS08","legalDate":942019200000,"docFormat":"PDF","numberOfPages":6,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"11/08/1999","shareable":false},{"docCode":"APPL","docDesc":"Matter
        concerning the application","docId":"EA4ZMR1TDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"12/01/1999","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EA4ZMR2ADHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIODOC","docDesc":"Priority
        document","docId":"EA4ZMR24DHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":17,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"PRIOTRAN","docDesc":"Translation
        of priority document","docId":"EA4ZMR3HDHXBS04","legalDate":944006400000,"docFormat":"PDF","numberOfPages":10,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"12/01/1999","shareable":false},{"docCode":"ABST","docDesc":"Abstract","docId":"EBHJF58CDHXBS05","legalDate":951696000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Application
        Documents","docGroupCode":"1","legalDateStr":"02/28/2000","shareable":false},{"docCode":"1507","docDesc":"Communication
        regarding the transmission of the European search report","docId":"EBH7YL38INPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"03/09/2000","shareable":false},{"docCode":"P0459","docDesc":"Annex
        to European Search report","docId":"EBH7YMIVINPPHID","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Documents
        including citations","docGroupCode":"101","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1503","docDesc":"European
        search report","docId":"EBH7YMLSINPPHIB","legalDate":952560000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,101,102,5","legalDateStr":"03/09/2000","shareable":false},{"docCode":"1133","docDesc":"Notification
        of forthcoming publication","docId":"EBL2KXTXINPPHIB","legalDate":954892800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/05/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBNZTATODHXBS04","legalDate":955411200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/11/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"EBOJJXREDHXBS13","legalDate":955324800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"04/10/2000","shareable":false},{"docCode":"1081","docDesc":"Reminder
        period for payment of examination fee/designation fee and correction of deficiencies
        in Written Opinion/amendment","docId":"EBSXC9T8INPPHIB","legalDate":959040000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"05/23/2000","shareable":false},{"docCode":"FREP","docDesc":"Document
        concerning representation","docId":"ECGJQI6ADHXBS16","legalDate":972604800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"No
        document group defined for this document","docGroupCode":"unknown","legalDateStr":"10/27/2000","shareable":false},{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2056","docDesc":"Bibliographic
        data of the European patent application","docId":"EEJVG2ABMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2,102","legalDateStr":"04/23/2002","shareable":false},{"docCode":"DREX","docDesc":"Text
        intended for grant","docId":"EEJVG2CDMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":14,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"AGRA31","docDesc":"Approval
        to announcement of intention to grant a European patent","docId":"EEZUDRHUDHXBS02","legalDate":1027900800000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"07/29/2002","shareable":false},{"docCode":"2005","docDesc":"Invitation
        to pay the fees for intended grant","docId":"EEZ471NBDHEPLEI","legalDate":1028678400000,"docFormat":"PDF","numberOfPages":3,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"2","legalDateStr":"08/07/2002","shareable":false},{"docCode":"IGRA7","docDesc":"Filing
        of the translations of the claims","docId":"EE8ZV40RDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Written
        Arguments, Opinions, Incoming amendments, Papers received from applicants","docGroupCode":"4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40WDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"CLMS","docDesc":"Claims","docId":"EE8ZV40ZDHXBS03","legalDate":1033430400000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Application
        Documents","docGroupCode":"1,4","legalDateStr":"10/01/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}],"country":"EP","docNumber":"99203729.A","message":null,"applicantNames":["Beheermaatschappij
        De Boer Nijmegen B.V., "],"oaIndCount":3,"officeActionDocs":[{"docCode":"2004","docDesc":"Communication
        about intention to grant a European patent","docId":"EEJVG18WMH21264","legalDate":1019520000000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"04/23/2002","shareable":false},{"docCode":"2006","docDesc":"Decision
        to grant a European patent","docId":"EFK9EIOKDHEPLEI","legalDate":1041897600000,"docFormat":"PDF","numberOfPages":2,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"01/07/2003","shareable":false},{"docCode":"2057","docDesc":"Communication
        regarding the expiry of opposition period","docId":"EGY4W09VDHEPLEI","legalDate":1071619200000,"docFormat":"PDF","numberOfPages":1,"docCodeDesc":"Office
        Actions Communications","docGroupCode":"23,2","legalDateStr":"12/17/2003","shareable":false}]},"kindCode":"A","priorityClaimList":[{"country":"NL","docNumber":"1010536","kindCode":"A"}],"appDateStr":"11/08/1999","docListMsg":null,"docNum":{"country":"EP","docNumber":"99203729","format":null,"date":"11/08/1999","kindCode":"A"},"title":null,"applicantNames":null,"ip5":true}]}'
    headers:
      Connection:
      - keep-alive
      Content-Length:
      - '22140'
      Content-Type:
      - application/json
      Date:
      - Fri, 20 Jan 2023 18:04:55 GMT
      apigw-requestid:
      - fDanKgrboAMEP-w=
    status:
      code: 200
      message: OK
version: 1
